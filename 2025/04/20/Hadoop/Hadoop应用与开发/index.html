<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Hadoop应用与开发1 | Blog</title><meta name="author" content="tong"><meta name="copyright" content="tong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Hadoop应用与开发1.Hadoop实验：基于MapReduce实现单词计数 目的1.学会基于MapReduce思想编写WordCount程序。 要求1.理解MapReduce编程思想；2.学会编写MapReduce版本WordCount；3.会执行该程序；4.可以自行分析执行过程。 原理MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop应用与开发1">
<meta property="og:url" content="http://example.com/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="Hadoop应用与开发1.Hadoop实验：基于MapReduce实现单词计数 目的1.学会基于MapReduce思想编写WordCount程序。 要求1.理解MapReduce编程思想；2.学会编写MapReduce版本WordCount；3.会执行该程序；4.可以自行分析执行过程。 原理MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/touxiang.png">
<meta property="article:published_time" content="2025-04-20T04:00:00.000Z">
<meta property="article:modified_time" content="2025-10-31T08:02:00.300Z">
<meta property="article:author" content="tong">
<meta property="article:tag" content="hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/touxiang.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hadoop应用与开发1",
  "url": "http://example.com/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/",
  "image": "http://example.com/img/touxiang.png",
  "datePublished": "2025-04-20T04:00:00.000Z",
  "dateModified": "2025-10-31T08:02:00.300Z",
  "author": [
    {
      "@type": "Person",
      "name": "tong",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/./img/favicon1.ico"><link rel="canonical" href="http://example.com/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop应用与开发1',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(https://i.loli.net/2019/09/09/5oDRkWVKctx2b6A.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/./img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Hadoop应用与开发1</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Hadoop应用与开发1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-20T04:00:00.000Z" title="发表于 2025-04-20 12:00:00">2025-04-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T08:02:00.300Z" title="更新于 2025-10-31 16:02:00">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="Hadoop应用与开发"><a href="#Hadoop应用与开发" class="headerlink" title="Hadoop应用与开发"></a>Hadoop应用与开发</h1><h2 id="1-Hadoop实验：基于MapReduce实现单词计数"><a href="#1-Hadoop实验：基于MapReduce实现单词计数" class="headerlink" title="1.Hadoop实验：基于MapReduce实现单词计数"></a>1.Hadoop实验：基于MapReduce实现单词计数</h2><blockquote>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>1.学会基于MapReduce思想编写WordCount程序。</p>
<h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><p>1.理解MapReduce编程思想；<br>2.学会编写MapReduce版本WordCount；<br>3.会执行该程序；<br>4.可以自行分析执行过程。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。</p>
<p>适用范围：数据量大，但是数据种类小可以放入内存。<br>基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。<br>理解MapReduce和Yarn：在新版Hadoop中，Yarn作为一个资源管理调度框架，是Hadoop下MapReduce程序运行的生存环境。其实MapRuduce除了可以运行Yarn框架下，也可以运行在诸如Mesos，Corona之类的调度框架上，使用不同的调度框架，需要针对Hadoop做不同的适配。</p>
<p>一个完成的MapReduce程序在Yarn中执行过程如下：</p>
<p>ResourcManager JobClient向ResourcManager提交一个job。<br>ResourcManager向Scheduler请求一个供MRAppMaster运行的container，然后启动它。<br>MRAppMaster启动起来后向ResourcManager注册。<br>ResourcManagerJobClient向ResourcManager获取到MRAppMaster相关的信息，然后直接与MRAppMaster进行通信。<br>MRAppMaster算splits并为所有的map构造资源请求。<br>MRAppMaster做一些必要的MR OutputCommitter的准备工作。<br>MRAppMaster向RM(Scheduler)发起资源请求，得到一组供map&#x2F;reduce task运行的container，然后与NodeManager一起对每一个container执行一些必要的任务，包括资源本地化等。<br>MRAppMaster 监视运行着的task 直到完成，当task失败时，申请新的container运行失败的task。<br>当每个map&#x2F;reduce task完成后，MRAppMaster运行MR OutputCommitter的cleanup 代码，也就是进行一些收尾工作。<br>当所有的map&#x2F;reduce完成后，MRAppMaster运行OutputCommitter的必要的job commit或者abort APIs。<br>MRAppMaster退出。</p>
<h2 id="3-1-MapReduce编程"><a href="#3-1-MapReduce编程" class="headerlink" title="3.1 MapReduce编程"></a>3.1 MapReduce编程</h2><p>编写在Hadoop中依赖Yarn框架执行的MapReduce程序，并不需要自己开发MRAppMaster和YARNRunner，因为Hadoop已经默认提供通用的YARNRunner和MRAppMaster程序， 大部分情况下只需要编写相应的Map处理和Reduce处理过程的业务程序即可。<br>编写一个MapReduce程序并不复杂，关键点在于掌握分布式的编程思想和方法，主要将计算过程分为以下五个步骤：<br>（1）迭代。遍历输入数据，并将之解析成key&#x2F;value对。<br>（2）将输入key&#x2F;value对映射(map)成另外一些key&#x2F;value对。<br>（3）依据key对中间数据进行分组(grouping)。<br>（4）以组为单位对数据进行归约(reduce)。<br>（5）迭代。将最终产生的key&#x2F;value对保存到输出文件中。</p>
<h2 id="3-2-Java-API解析"><a href="#3-2-Java-API解析" class="headerlink" title="3.2 Java API解析"></a>3.2 Java API解析</h2><p>InputFormat：用于描述输入数据的格式，常用的为TextInputFormat提供如下两个功能：<br>数据切分： 按照某个策略将输入数据切分成若干个split，以便确定Map Task个数以及对应的split。<br>为Mapper提供数据：给定某个split，能将其解析成一个个key&#x2F;value对。<br>OutputFormat：用于描述输出数据的格式，它能够将用户提供的key&#x2F;value对写入特定格式的文件中。<br>Mapper&#x2F;Reducer: Mapper&#x2F;Reducer中封装了应用程序的数据处理逻辑。<br>Writable:Hadoop自定义的序列化接口。实现该类的接口可以用作MapReduce过程中的value数据使用。<br>WritableComparable：在Writable基础上继承了Comparable接口，实现该类的接口可以用作MapReduce过程中的key数据使用。（因为key包含了比较排序的操作）。</p>
</blockquote>
<h3 id="4-1实验准备"><a href="#4-1实验准备" class="headerlink" title="4.1实验准备"></a>4.1实验准备</h3><p>4.1.1 上传数据文件至hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# hadoop fs -put /root/data/hadoop/wordcount/word  /</span><br></pre></td></tr></table></figure>

<p>4.1.2 使用idea开发工具<br>现在下载需要用到的jars，我们选择资料工具 -&gt; 软件下载 -&gt; Spark -&gt; 2.2.2<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277982237.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277988807.png" alt="img"><br>单击File -&gt; Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277996091.png" alt="img"><br>选择 Libraries -&gt; ➕ -&gt;java<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278002552.png" alt="img"><br>选择 &#x2F;spark-2.2.2-bin-hadoop2.7&#x2F;jars 路径下的所有包：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278010156.png" alt="img"><br>这里就可以看到我们导入的jar包<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278016825.png" alt="img"></p>
<h3 id="4-2-编写MapReduce程序"><a href="#4-2-编写MapReduce程序" class="headerlink" title="4.2 编写MapReduce程序"></a>4.2 编写MapReduce程序</h3><p>首先先建一个Package包，起名为njupt，并在该包下建一个WordCount类</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278027307.png" alt="img"></p>
<p>主要编写Map和Reduce类，其中Map过程需要继承org.apache.hadoop.mapreduce包中Mapper类，并重写其map方法；Reduce过程需要继承org.apache.hadoop.mapreduce包中Reduce类，并重写其reduce方法。</p>
<p>代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TokenizerMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, IntWritable&gt; &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">word</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">        <span class="comment">//map方法，划分一行文本，读一个单词写出一个&lt;单词,1&gt;</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context context)</span><span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="type">StringTokenizer</span> <span class="variable">itr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringTokenizer</span>(value.toString());</span><br><span class="line">            <span class="keyword">while</span> (itr.hasMoreTokens()) &#123;</span><br><span class="line">                word.set(itr.nextToken());</span><br><span class="line">                context.write(word, one);<span class="comment">//写出&lt;单词,1&gt;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//定义reduce类，对相同的单词，把它们中的VList值全部相加</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">IntSumReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, IntWritable, Text, IntWritable&gt; &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">result</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span></span><br><span class="line">                <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (IntWritable val : values) &#123;</span><br><span class="line">                sum += val.get();<span class="comment">//相当于&lt;Hello,1&gt;&lt;Hello,1&gt;，将两个1相加</span></span><br><span class="line">            &#125;</span><br><span class="line">            result.set(sum);</span><br><span class="line">            context.write(key, result);<span class="comment">//写出这个单词，和这个单词出现次数&lt;单词，单词出现次数&gt;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;<span class="comment">//主方法，函数入口</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();           <span class="comment">//实例化配置文件类</span></span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Job</span>(conf, <span class="string">&#x27;WordCount&#x27;</span>);             <span class="comment">//实例化Job类</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);     <span class="comment">//指定使用默认输入格式类</span></span><br><span class="line">        TextInputFormat.setInputPaths(job, args[<span class="number">0</span>]);      <span class="comment">//设置待处理文件的位置</span></span><br><span class="line">        job.setJarByClass(WordCount.class);               <span class="comment">//设置主类名</span></span><br><span class="line">        job.setMapperClass(TokenizerMapper.class);        <span class="comment">//指定使用上述自定义Map类</span></span><br><span class="line">        job.setCombinerClass(IntSumReducer.class);        <span class="comment">//指定开启Combiner函数</span></span><br><span class="line">        job.setMapOutputKeyClass(Text.class);            <span class="comment">//指定Map类输出的，K类型</span></span><br><span class="line">        job.setMapOutputValueClass(IntWritable.class);     <span class="comment">//指定Map类输出的，V类型</span></span><br><span class="line">        job.setPartitionerClass(HashPartitioner.class);       <span class="comment">//指定使用默认的HashPartitioner类</span></span><br><span class="line">        job.setReducerClass(IntSumReducer.class);         <span class="comment">//指定使用上述自定义Reduce类</span></span><br><span class="line">        job.setNumReduceTasks(Integer.parseInt(args[<span class="number">2</span>]));  <span class="comment">//指定Reduce个数</span></span><br><span class="line">        job.setOutputKeyClass(Text.class);                <span class="comment">//指定Reduce类输出的,K类型</span></span><br><span class="line">        job.setOutputValueClass(Text.class);               <span class="comment">//指定Reduce类输出的,V类型</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);  <span class="comment">//指定使用默认输出格式类</span></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));    <span class="comment">//设置输出结果文件位置</span></span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);    <span class="comment">//提交任务并监控任务状态</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-3使用IDEA开发工具将该代码打包"><a href="#4-3使用IDEA开发工具将该代码打包" class="headerlink" title="4.3使用IDEA开发工具将该代码打包"></a>4.3使用IDEA开发工具将该代码打包</h3><p>步骤如下：<br>点击File中的Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278041028.png" alt="img"><br>选择Artifacts中的JAR，并选择From modules with dependencies<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278048922.png" alt="img"><br>直接点击OK<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278055675.png" alt="img"><br>选中所有依赖将依赖删除<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278062165.png" alt="img"><br>选择JAR包生成目录<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278069119.png" alt="img"><br>在顶部菜单栏点击Build中的Build Artifacts<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278077764.png" alt="img"><br>最后点击build打包，如下图<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676278084865.png" alt="img"></p>
<p>假定打包后的文件名为hdpAction.jar，主类WordCount位于包njupt下，将jar包移至平台，则可使用如下命令向YARN集群提交本应用。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# yarn  jar  hdpAction.jar  njupt.WordCount  /word  /wordcount 1</span><br></pre></td></tr></table></figure>

<p>其中“yarn”为命令，“jar”为命令参数，后面紧跟打包后的代码地址，“njupt”为包名，“WordCount”为主类名，“&#x2F;word”为输入文件在HDFS中的位置，&#x2F;wordcount为输出文件在HDFS中的位置。</p>
<h2 id="2-Hadoop实验：HDFS原理与操作"><a href="#2-Hadoop实验：HDFS原理与操作" class="headerlink" title="2.Hadoop实验：HDFS原理与操作"></a>2.Hadoop实验：HDFS原理与操作</h2><blockquote>
<h3 id="目的-1"><a href="#目的-1" class="headerlink" title="目的"></a>目的</h3><p>1.理解HDFS的读写操作原理<br>2.掌握HDFS的一些常用命令<br>3.理解实际操作HDFS读写时的流程</p>
<h3 id="要求-1"><a href="#要求-1" class="headerlink" title="要求"></a>要求</h3><p>1.要求实验结束后，可以掌握HDFS的读写操作<br>2.要求实验结束后，可以熟练使用HDFS的基础操作</p>
<h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1-HDFS-读操作"><a href="#3-1-HDFS-读操作" class="headerlink" title="3.1 HDFS 读操作"></a>3.1 HDFS 读操作</h2><p>（如图 3-1 所示）</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700191357.png" alt="img"></p>
<p>图3-1</p>
<p>客户端通过调用 FileSystem 对象的 open() 方法来打开希望读取的文件，对于 HDFS 来说，这个对象是分布式文件系统的一个实例；</p>
<p>DistributedFileSystem 通过使用 RPC 来调用 NameNode 以确定文件起始块的位置，同一 Block 按照重复数会返回多个位置，这些位置按照 Hadoop 集群拓扑结构排序，距离客户端近的排在前面；</p>
<p>前两步会返回一个 FSDataInputStream 对象，该对象会被封装成 DFSInputStream 对象，DFSInputStream 可以方便的管理 datanode 和 namenode 数据流，客户端对这个输入流调用 read() 方法；</p>
<p>存储着文件起始块的 DataNode 地址的 DFSInputStream 随即连接距离最近的 DataNode，通过对数据流反复调用 read() 方法，可以将数据从 DataNode 传输到客户端；</p>
<p>到达块的末端时，DFSInputStream 会关闭与该 DataNode 的连接，然后寻找下一个块的最佳 DataNode，这些操作对客户端来说是透明的，从客户端的角度来看只是读一个持续不断的流；</p>
<p>一旦客户端完成读取，就对 FSDataInputStream 调用 close() 方法关闭文件读取。</p>
<h2 id="3-2-HDFS-写操作"><a href="#3-2-HDFS-写操作" class="headerlink" title="3.2 HDFS 写操作"></a>3.2 HDFS 写操作</h2><p>（如图 3-2 所示）</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700201068.png" alt="img"><br>图3-2</p>
<p>客户端通过调用 DistributedFileSystem 的 create() 方法创建新文件；</p>
<p>DistributedFileSystem 通过 RPC 调用 NameNode 去创建一个没有 Blocks 关联的新文件，创建前  NameNode 会做各种校验，比如文件是否存在、客户端有无权限去创建等。如果校验通过，NameNode  会为创建新文件记录一条记录，否则就会抛出 IO 异常；</p>
<p>前两步结束后会返回 FSDataOutputStream 的对象，和读文件的时候相似，FSDataOutputStream 被封装成  DFSOutputStream，DFSOutputStream 可以协调 NameNode 和 Datanode。客户端开始写数据到  DFSOutputStream，DFSOutputStream 会把数据切成一个个小的数据包，并写入内部队列称为“数据队列”(Data  Queue)；</p>
<p>DataStreamer 会去处理接受 Data Queue，它先问询 NameNode 这个新的 Block 最适合存储在哪几个  DataNode 里，比如重复数是 3，那么就找到 3 个最适合的 DataNode，把他们排成一个 pipeline。DataStreamer 把 Packet 按队列输出到管道的第一个 Datanode 中，第一个 DataNode 又把 Packet 输出到第二个 DataNode 中，以此类推；</p>
<p>DFSOutputStream 还有一个队列叫 Ack Quene，也是由 Packet 组成，等待 DataNode 的收到响应，当  Pipeline 中的所有 DataNode 都表示已经收到的时候，这时 Akc Quene 才会把对应的 Packet 包移除掉；</p>
<p>客户端完成写数据后调用 close() 方法关闭写入流；</p>
<p>DataStreamer 把剩余的包都刷到 Pipeline 里然后等待 Ack 信息，收到最后一个 Ack 后，通知 NameNode 把文件标示为已完成。</p>
</blockquote>
<h3 id="4-1-实验案例1"><a href="#4-1-实验案例1" class="headerlink" title="4.1 实验案例1"></a>4.1 实验案例1</h3><h3 id="4-1-1-程序代码"><a href="#4-1-1-程序代码" class="headerlink" title="4.1.1 程序代码"></a>4.1.1 程序代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.InputStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FileSystemCat</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">uri</span> <span class="operator">=</span> args[<span class="number">0</span>];</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem. get(URI.create (uri), conf);</span><br><span class="line">        <span class="type">InputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            in = fs.open( <span class="keyword">new</span> <span class="title class_">Path</span>(uri));</span><br><span class="line">            IOUtils.copyBytes(in, System.out, <span class="number">4096</span>, <span class="literal">false</span>);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            IOUtils.closeStream(in);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-案例1实现过程"><a href="#4-2-案例1实现过程" class="headerlink" title="4.2 案例1实现过程"></a>4.2 案例1实现过程</h3><h3 id="4-2-1-创建代码目录"><a href="#4-2-1-创建代码目录" class="headerlink" title="4.2.1 创建代码目录"></a>4.2.1 创建代码目录</h3><p>（1）创建环境并一键搭建<br>（2）使用如下命令启动 Hadoop：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/sbin</span><br><span class="line"># ./start-all.sh</span><br></pre></td></tr></table></figure>

<p>(一键搭建后实验环境已启动好无需再次启动)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># jps   // 查看启动的进程，确保 NameNode 和 DataNode 都有启动</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2-修改环境配置"><a href="#4-2-2-修改环境配置" class="headerlink" title="4.2.2 修改环境配置"></a>4.2.2 修改环境配置</h3><p>执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/profile</span><br></pre></td></tr></table></figure>

<p>在文件末尾添加</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">JAVA_HOME=/usr/local/jdk1.8.0_161/</span><br><span class="line">export HADOOP_HOME=/usr/cstor/hadoop</span><br><span class="line">export JRE_HOME=/usr/local/jdk1.8.0_161//jre</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/common/lib/*</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export HADOOP_OPTS=&#x27;-Djava.library.path=$HADOOP_HOME/lib:$HADOOP_HOME/lib/native&#x27;</span><br></pre></td></tr></table></figure>

<p>在 &#x2F;usr&#x2F;cstor 目录下使用如下命令建立 myclass 和 input 目录：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop</span><br><span class="line"># rm -rf myclass input </span><br><span class="line"># mkdir -p myclass input</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700220585.png" alt="img"></p>
<h3 id="4-2-3-建立例子文件上传到-HDFS-中"><a href="#4-2-3-建立例子文件上传到-HDFS-中" class="headerlink" title="4.2.3 建立例子文件上传到 HDFS 中"></a>4.2.3 建立例子文件上传到 HDFS 中</h3><p>进入 input 目录，在该目录中创建 quangle.txt 文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd input </span><br><span class="line"># touch quangle.txt </span><br><span class="line"># vim quangle.txt</span><br></pre></td></tr></table></figure>

<p>内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">On the top of the Crumpetty Tree </span><br><span class="line">The Quangle Wangle sat, </span><br><span class="line">But his face you could not see, </span><br><span class="line">On account of his Beaver Hat.</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700230425.png" alt="img"><br>使用如下命令在 hdfs 中建立目录 &#x2F;class4。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -mkdir /class4 </span><br><span class="line"># hadoop fs -ls /</span><br></pre></td></tr></table></figure>

<p>说明：如遇到报错没有 hadoop 命令，请重新执行 source hadoop-env.sh。后续的实验中同理。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700238936.png" alt="img"><br>把例子文件上传到 hdfs 的 &#x2F;class4 文件夹中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/input </span><br><span class="line"># hadoop fs -copyFromLocal quangle.txt /class4/quangle.txt </span><br><span class="line"># hadoop fs -ls /class4</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700246616.png" alt="img"></p>
<h3 id="4-2-4-配置本地环境"><a href="#4-2-4-配置本地环境" class="headerlink" title="4.2.4 配置本地环境"></a>4.2.4 配置本地环境</h3><p>对 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;etc&#x2F;hadoop 目录中的 hadoop-env.sh 进行配置，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/etc/hadoop</span><br><span class="line"># vim hadoop-env.sh</span><br></pre></td></tr></table></figure>

<p>加入 HADOOP_CLASSPATH 变量，值为 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;myclass，设置完毕后编译该配置文件，使配置生效。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_CLASSPATH=/usr/cstor/hadoop/myclass</span><br></pre></td></tr></table></figure>

<h3 id="4-2-5编写代码"><a href="#4-2-5编写代码" class="headerlink" title="4.2.5编写代码"></a>4.2.5编写代码</h3><p>进入 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;myclass 目录，在该目录中建立 FileSystemCat.java 代码文件，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/myclass</span><br><span class="line"># vim FileSystemCat.java</span><br></pre></td></tr></table></figure>

<p>输入代码内容：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700260457.png" alt="img"></p>
<h3 id="4-2-6编译代码"><a href="#4-2-6编译代码" class="headerlink" title="4.2.6编译代码"></a>4.2.6编译代码</h3><p>在 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;myclass 目录中，使用如下命令编译代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># javac -classpath /usr/cstor/mahout/lib/hadoop/hadoop-core-1.2.1.jar FileSystemCat.java</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700269795.png" alt="img"></p>
<h3 id="4-2-7-使用编译代码读取-HDFS-文件"><a href="#4-2-7-使用编译代码读取-HDFS-文件" class="headerlink" title="4.2.7 使用编译代码读取 HDFS 文件"></a>4.2.7 使用编译代码读取 HDFS 文件</h3><p>使用如下命令读取 HDFS 中 &#x2F;class4&#x2F;quangle.txt 内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop FileSystemCat /class4/quangle.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700277546.png" alt="img"></p>
<h3 id="4-3-实验案例2"><a href="#4-3-实验案例2" class="headerlink" title="4.3 实验案例2"></a>4.3 实验案例2</h3><p>在本地文件系统生成一个大约 100 字节的文本文件，写一段程序读入这个文件并将其第 101-120 字节的内容写入 HDFS 成为一个新文件。</p>
<h3 id="4-3-1-程序代码"><a href="#4-3-1-程序代码" class="headerlink" title="4.3.1 程序代码"></a>4.3.1 程序代码</h3><p>注意：在编译前请先删除中文注释！</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Progressable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">LocalFile2Hdfs</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取读取源文件和目标文件位置参数</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">local</span> <span class="operator">=</span> args[<span class="number">0</span>];</span><br><span class="line">        <span class="type">String</span> <span class="variable">uri</span> <span class="operator">=</span> args[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="type">FileInputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="type">OutputStream</span> <span class="variable">out</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 获取读入文件数据</span></span><br><span class="line">            in = <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(<span class="keyword">new</span> <span class="title class_">File</span>(local));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 获取目标文件信息</span></span><br><span class="line">            <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(URI.create(uri), conf);</span><br><span class="line">            out = fs.create(<span class="keyword">new</span> <span class="title class_">Path</span>(uri), <span class="keyword">new</span> <span class="title class_">Progressable</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">progress</span><span class="params">()</span> &#123;</span><br><span class="line">                    System.out.println(<span class="string">&#x27;*&#x27;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 跳过前100个字符</span></span><br><span class="line">            in.skip(<span class="number">100</span>);</span><br><span class="line">            <span class="type">byte</span>[] buffer = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">20</span>];</span><br><span class="line"></span><br><span class="line">            <span class="comment">// 从101的位置读取20个字符到buffer中</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">bytesRead</span> <span class="operator">=</span> in.read(buffer);</span><br><span class="line">            <span class="keyword">if</span> (bytesRead &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                out.write(buffer, <span class="number">0</span>, bytesRead);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            IOUtils.closeStream(in);</span><br><span class="line">            IOUtils.closeStream(out);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-4-案例2实现过程"><a href="#4-4-案例2实现过程" class="headerlink" title="4.4 案例2实现过程"></a>4.4 案例2实现过程</h3><h3 id="4-4-1-编写代码"><a href="#4-4-1-编写代码" class="headerlink" title="4.4.1 编写代码"></a>4.4.1 编写代码</h3><p>1.进入 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;myclass 目录，在该目录中建立 LocalFile2Hdfs.java代码文件，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/myclass</span><br><span class="line"># vim LocalFile2Hdfs.java</span><br></pre></td></tr></table></figure>

<p>输入代码内容：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700292536.png" alt="img"></p>
<h3 id="4-4-2-编译代码"><a href="#4-4-2-编译代码" class="headerlink" title="4.4.2 编译代码"></a>4.4.2 编译代码</h3><p>在 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;myclass 目录中，使用如下命令编译代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#  javac -classpath /usr/cstor/mahout/lib/hadoop/hadoop-core-1.2.1.jar LocalFile2Hdfs.java</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700311785.png" alt="img"></p>
<h3 id="4-4-3-建立测试文件"><a href="#4-4-3-建立测试文件" class="headerlink" title="4.4.3 建立测试文件"></a>4.4.3 建立测试文件</h3><p>进入 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;input 目录，在该目录中建立 local2hdfs.txt 文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/input/ </span><br><span class="line"># vim local2hdfs.txt</span><br></pre></td></tr></table></figure>

<p>内容为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Washington (CNN) -- Twitter is suing the U.S. government in an effort to loosen restrictions on what the social media giant can say publicly about the national security-related requests it receives for user data. The company filed a lawsuit against the Justice Department on Monday in a federal court in northern California, arguing that its First Amendment rights are being violated by restrictions that forbid the disclosure of how many national security letters and Foreign Intelligence Surveillance Act court orders it receives -- even if that number is zero. Twitter vice president Ben Lee wrote in a blog post that it&#x27;s suing in an effort to publish the full version of a &#x27;transparency report&#x27; prepared this year that includes those details. The San Francisco-based firm was unsatisfied with the Justice Department&#x27;s move in January to allow technological firms to disclose the number of national security-related requests they receive in broad ranges.</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700322386.png" alt="img"></p>
<h3 id="4-4-4-使用编译代码上传文件内容到-HDFS"><a href="#4-4-4-使用编译代码上传文件内容到-HDFS" class="headerlink" title="4.4.4 使用编译代码上传文件内容到 HDFS"></a>4.4.4 使用编译代码上传文件内容到 HDFS</h3><p>使用如下命令读取 local2hdfs 第 101-120 字节的内容写入 HDFS 成为一个新文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/input</span><br><span class="line"># hadoop LocalFile2Hdfs local2hdfs.txt /class4/local2hdfs_part.txt </span><br><span class="line"># hadoop fs -ls /class4</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700332694.png" alt="img"></p>
<h3 id="4-4-5-验证是否成功"><a href="#4-4-5-验证是否成功" class="headerlink" title="4.4.5 验证是否成功"></a>4.4.5 验证是否成功</h3><p>使用如下命令读取 local2hdfs_part.txt 内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -cat /class4/local2hdfs_part.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700339915.png" alt="img"></p>
<h3 id="4-5实验案例3"><a href="#4-5实验案例3" class="headerlink" title="4.5实验案例3"></a>4.5实验案例3</h3><p>实验案例 2 的反向操作，在 HDFS 中生成一个大约 100 字节的文本文件，写一段程序读入这个文件，并将其第 101-120 字节的内容写入本地文件系统成为一个新文件。</p>
<h3 id="4-5-1-程序代码"><a href="#4-5-1-程序代码" class="headerlink" title="4.5.1 程序代码"></a>4.5.1 程序代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Hdfs2LocalFile</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">String</span> <span class="variable">uri</span> <span class="operator">=</span> args[<span class="number">0</span>];</span><br><span class="line">        <span class="type">String</span> <span class="variable">local</span> <span class="operator">=</span> args[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="type">FSDataInputStream</span> <span class="variable">in</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="type">OutputStream</span> <span class="variable">out</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">FileSystem</span> <span class="variable">fs</span> <span class="operator">=</span> FileSystem.get(URI.create(uri), conf);</span><br><span class="line">            in = fs.open(<span class="keyword">new</span> <span class="title class_">Path</span>(uri));</span><br><span class="line">            out = <span class="keyword">new</span> <span class="title class_">FileOutputStream</span>(local);</span><br><span class="line"></span><br><span class="line">            <span class="type">byte</span>[] buffer = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">20</span>];</span><br><span class="line">            in.skip(<span class="number">100</span>);</span><br><span class="line">            <span class="type">int</span> <span class="variable">bytesRead</span> <span class="operator">=</span> in.read(buffer);</span><br><span class="line">            <span class="keyword">if</span> (bytesRead &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">                out.write(buffer, <span class="number">0</span>, bytesRead);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            IOUtils.closeStream(in);</span><br><span class="line">            IOUtils.closeStream(out);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-6-案例3实现过程"><a href="#4-6-案例3实现过程" class="headerlink" title="4.6 案例3实现过程"></a>4.6 案例3实现过程</h3><h3 id="4-6-1-编写代码"><a href="#4-6-1-编写代码" class="headerlink" title="4.6.1 编写代码"></a>4.6.1 编写代码</h3><p>进入 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;myclass 目录，在该目录中建立 Hdfs2LocalFile.java代码文件，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/myclass/ </span><br><span class="line"># vim Hdfs2LocalFile.java</span><br></pre></td></tr></table></figure>

<p>输入代码内容：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700356297.png" alt="img"></p>
<h3 id="4-6-2-编译代码"><a href="#4-6-2-编译代码" class="headerlink" title="4.6.2 编译代码"></a>4.6.2 编译代码</h3><p>在 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;myclass 目录中，使用如下命令编译代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># javac -classpath /usr/cstor/mahout/lib/hadoop/hadoop-core-1.2.1.jar Hdfs2LocalFile.java</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700365126.png" alt="img"></p>
<h3 id="4-6-3-建立测试文件"><a href="#4-6-3-建立测试文件" class="headerlink" title="4.6.3 建立测试文件"></a>4.6.3 建立测试文件</h3><p>进入 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;input 目录，在该目录中建立 hdfs2local.txt 文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/input/ </span><br><span class="line"># vim hdfs2local.txt</span><br></pre></td></tr></table></figure>

<p>内容为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The San Francisco-based firm was unsatisfied with the Justice Department&#x27;s move in January to allow technological firms to disclose the number of national security-related requests they receive in broad ranges. &#x27;It&#x27;s our belief that we are entitled under the First Amendment to respond to our users&#x27; concerns and to the statements of U.S. government officials by providing information about the scope of U.S. government surveillance -- including what types of legal process have not been received,&#x27; Lee wrote. &#x27;We should be free to do this in a meaningful way, rather than in broad, inexact ranges.&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700378316.png" alt="img">在 &#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;input 目录下把该文件上传到 hdfs 的 &#x2F;class4&#x2F; 文件夹中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -copyFromLocal hdfs2local.txt /class4/hdfs2local.txt </span><br><span class="line"># hadoop fs -ls /class4/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700399535.png" alt="img"></p>
<h3 id="4-6-4-使用编译代码把文件内容从-HDFS-输出到文件系统中"><a href="#4-6-4-使用编译代码把文件内容从-HDFS-输出到文件系统中" class="headerlink" title="4.6.4 使用编译代码把文件内容从 HDFS 输出到文件系统中"></a>4.6.4 使用编译代码把文件内容从 HDFS 输出到文件系统中</h3><p>使用如下命令读取 hdfs2local.txt 第 101-120 字节的内容写入本地文件系统成为一个新文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop Hdfs2LocalFile /class4/hdfs2local.txt hdfs2local_part.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700409635.png" alt="img"></p>
<h3 id="4-6-5-验证是否成功"><a href="#4-6-5-验证是否成功" class="headerlink" title="4.6.5 验证是否成功"></a>4.6.5 验证是否成功</h3><p>使用如下命令读取 hdfs2local_part.txt 内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># cat hdfs2local_part.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700419664.png" alt="img"></p>
<h3 id="4-7-eclipse上Hadoop插件的配置"><a href="#4-7-eclipse上Hadoop插件的配置" class="headerlink" title="4.7 eclipse上Hadoop插件的配置"></a>4.7 eclipse上Hadoop插件的配置</h3><h3 id="4-7-1-配置本机的java环境变量"><a href="#4-7-1-配置本机的java环境变量" class="headerlink" title="4.7.1 配置本机的java环境变量"></a>4.7.1 配置本机的java环境变量</h3><p>变量名：JAVA_HOME<br>变量值：电脑上JDK安装的绝对路径<br>变量名：CLASSPATH<br>变量值：.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;<br>变量名：Path<br>变量值：%JAVA_HOME%\bin<br>%JAVA_HOME%\jre\bin</p>
<h3 id="4-7-2配置Hadoop插件"><a href="#4-7-2配置Hadoop插件" class="headerlink" title="4.7.2配置Hadoop插件"></a>4.7.2配置Hadoop插件</h3><p>确定已关闭eclipse后，首先将hadoop-eclipse-plugin-2.7.1.jar文件拷贝至eclipse安装目录的plugins文件夹下。如图3-2（eclipse的插件存放目录）和图3-3（将Hadoop插件放入eclipse）所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700433686.png" alt="img"><br>3-2</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700438926.png" alt="img"><br>3-3</p>
<p>接下来，我们需要准备本地的Hadoop环境，用于加载hadoop目录中的jar包，只需解压hadoop-2.7.1.tar.gz文件，解压过程中可能会遇到如下错误，点击关闭忽略即可。如图3-4所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700446514.png" alt="img"><br>3-4</p>
<h3 id="4-7-3验证eclipse是否可以见Hadoop项目"><a href="#4-7-3验证eclipse是否可以见Hadoop项目" class="headerlink" title="4.7.3验证eclipse是否可以见Hadoop项目"></a>4.7.3验证eclipse是否可以见Hadoop项目</h3><p>现在，我们需要验证是否可以用Eclipse新建Hadoop（HDFS）项目。打开Eclipse软件，依次点击File-&gt;New-&gt;Other，查看是否已经有Map&#x2F;Reduce  Project的选项。第一次新建Map&#x2F;Reduce项目时，需要指定hadoop解压后的位置。如图3-5（Eclipse新建Map&#x2F;Reduce项目）、图3-6（设置Hadoop安装目录）和图3-7（指定Hadoop安装目录）所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700454926.png" alt="img"><br>3-5<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700460535.png" alt="img"><br>3-6<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700468135.png" alt="img"><br>3-7</p>
<h3 id="4-7-4使用eclipse导出jar包"><a href="#4-7-4使用eclipse导出jar包" class="headerlink" title="4.7.4使用eclipse导出jar包"></a>4.7.4使用eclipse导出jar包</h3><p>1在eclipse导航栏中选中该项目右击，点击export如图4-1 所示<br>2选择java中的Jar File 打Jar包，如图4-2所示<br>3选择jar包保存的地址，如图4-3所示<br>4Finish 结束<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700475524.png" alt="img"><br>4-1<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700481784.png" alt="img"><br>4-2<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1688700487805.png" alt="img"><br>4-3</p>
<h2 id="3-Hadoop实验：基于MapReduce实现Join操作"><a href="#3-Hadoop实验：基于MapReduce实现Join操作" class="headerlink" title="3.Hadoop实验：基于MapReduce实现Join操作"></a>3.Hadoop实验：基于MapReduce实现Join操作</h2><blockquote>
<h3 id="目的-2"><a href="#目的-2" class="headerlink" title="目的"></a>目的</h3><p>1.Hadoop实验：基于MapReduce实现Join操作</p>
<h3 id="要求-2"><a href="#要求-2" class="headerlink" title="要求"></a>要求</h3><p>1.掌握MapReduce编程思想<br>2.学会编写MapReduce版本Join程序，并能执行该程序和分析执行过程。</p>
<h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h2><p>对于RDBMS中的Join操作大伙一定非常熟悉，写SQL的时候要十分注意细节，稍有差池就会耗时巨久造成很大的性能瓶颈，而在Hadoop中使用MapReduce框架进行Join的操作时同样耗时，但是由于Hadoop的分布式设计理念的特殊性，因此对于这种Join操作同样也具备了一定的特殊性。</p>
<h2 id="3-2-原理"><a href="#3-2-原理" class="headerlink" title="3.2 原理"></a>3.2 原理</h2><p>使用MapReduce实现Join操作有多种实现方式：</p>
<p>1.在Reduce端连接为最为常见的模式：<br>Map端的主要工作：为来自不同表(文件)的key&#x2F;value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。<br>Reduce端的主要工作：在Reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行笛卡尔只就OK了。</p>
<p>2.在Map端进行连接<br>使用场景：一张表十分小、一张表很大。<br>用法:在提交作业的时候先将小表文件放到该作业的DistributedCache中，然后从DistributeCache中取出该小表进行Join key &#x2F; value解释分割放到内存中(可以放大Hash Map等等容器中)。然后扫描大表，看大表中的每条记录的Join key  &#x2F;value值是否能够在内存中找到相同Join key的记录，如果有则直接输出结果。</p>
<p>3.SemiJoin<br>SemiJoin就是所谓的半连接，其实仔细一看就是Reduce  Join的一个变种，就是在map端过滤掉一些数据，在网络中只传输参与连接的数据不参与连接的数据不必在网络中进行传输，从而减少了shuffle的网络传输量，使整体效率得到提高，其他思想和Reduce  Join是一模一样的。说得更加接地气一点就是将小表中参与Join的key单独抽出来通过DistributedCach分发到相关节点，然后将其取出放到内存中(可以放到HashSet中)，在map阶段扫描连接表，将Join  key不在内存HashSet中的记录过滤掉，让那些参与Join的记录通过shuffle传输到Reduce端进行Join操作，其他的和Reduce Join都是一样的</p>
</blockquote>
<h3 id="4-1-准备阶段"><a href="#4-1-准备阶段" class="headerlink" title="4.1 准备阶段"></a>4.1 准备阶段</h3><p>在这里我们介绍最为常见的在Reduce端连接的代码编写流程。<br>首先准备数据数据分为两个文件，分别为A表和B表数据。<br>使用vim命令编写数据文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim data.txt</span><br></pre></td></tr></table></figure>

<p>A表数据:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">201001 1003 abc</span><br><span class="line">201002 1005 def</span><br><span class="line">201003 1006 ghi</span><br><span class="line">201004 1003 jkl</span><br><span class="line">201005 1004 mno</span><br><span class="line">201006 1005 pqr</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018205535.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim info.txt</span><br></pre></td></tr></table></figure>

<p>B表数据:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1003 kaka</span><br><span class="line">1004 da</span><br><span class="line">1005 jue</span><br><span class="line">1006 zhao</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018214570.png" alt="img"><br>现在要通过程序得到A表第二个字段和B表第一个字段一致的数据的Join结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1003  201001 abc  kaka</span><br><span class="line">1003  201004 jkl  kaka</span><br><span class="line">1004  201005 mno da</span><br><span class="line">1005  201002 def  jue</span><br><span class="line">1005  201006 pqr jue</span><br><span class="line">1006  201003 ghi zhao</span><br></pre></td></tr></table></figure>

<h3 id="4-1-1-程序分析执行过程如下："><a href="#4-1-1-程序分析执行过程如下：" class="headerlink" title="4.1.1 程序分析执行过程如下："></a>4.1.1 程序分析执行过程如下：</h3><p>在map阶段，把所有记录标记成&lt;key,value&gt;的形式，其中key是1003&#x2F;1004&#x2F;1005&#x2F;1006的字段值，value则根据来源不同取不同的形式：来源于表A的记录，value的值为“201001 abc”等值；来源于表B的记录，value的值为”kaka“之类的值。</p>
<p>在Reduce阶段，先把每个key下的value列表拆分为分别来自表A和表B的两部分，分别放入两个向量中。然后遍历两个向量做笛卡尔积，形成一条条最终结果。</p>
<h3 id="4-2-编写程序"><a href="#4-2-编写程序" class="headerlink" title="4.2 编写程序"></a>4.2 编写程序</h3><p>打开 IDEA 开发工具，新建JAVA项目MRJoin 。</p>
<p>项目结构如图：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018225865.png" alt="img"></p>
<p>导入Hadoop相关外部依赖。</p>
<p>现在下载需要用到的jars，我们选择资料工具 -&gt; 软件下载 -&gt; Spark -&gt; 2.2.2</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018232936.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018238400.png" alt="img"></p>
<p>单击File -&gt; Project Structure</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018245634.png" alt="img"></p>
<p>选择 Libraries -&gt; ➕ -&gt;java </p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018251945.png" alt="img"></p>
<p>选择 &#x2F;spark-2.2.2-bin-hadoop2.7&#x2F;jars 路径下的所有包：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018259686.png" alt="img"></p>
<p>这里就可以看到我们导入的jar包</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018267092.png" alt="img"></p>
<p>新建类 MRJoin.java 。</p>
<h3 id="4-2-1-MRJoin-java的完整代码如下："><a href="#4-2-1-MRJoin-java的完整代码如下：" class="headerlink" title="4.2.1 MRJoin.java的完整代码如下："></a>4.2.1 MRJoin.java的完整代码如下：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MRJoin</span> &#123;</span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MR_Join_Mapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, TextPair, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span> </span><br><span class="line">                    <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="comment">// 获取输入文件的全路径和名称</span></span><br><span class="line">      <span class="type">String</span> <span class="variable">pathName</span> <span class="operator">=</span> ((FileSplit) context.getInputSplit()).getPath().toString();</span><br><span class="line">      <span class="keyword">if</span> (pathName.contains(<span class="string">&#x27;data.txt&#x27;</span>)) &#123;</span><br><span class="line">        String values[] = value.toString().split(<span class="string">&#x27; &#x27;</span>);</span><br><span class="line">        <span class="keyword">if</span> (values.length &lt; <span class="number">3</span>) &#123;</span><br><span class="line">          <span class="comment">// data数据格式不规范，字段小于3，抛弃数据</span></span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 数据格式规范，区分标识为1</span></span><br><span class="line">          <span class="type">TextPair</span> <span class="variable">tp</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TextPair</span>(<span class="keyword">new</span> <span class="title class_">Text</span>(values[<span class="number">1</span>]), <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&#x27;1&#x27;</span>));</span><br><span class="line">          context.write(tp, <span class="keyword">new</span> <span class="title class_">Text</span>(values[<span class="number">0</span>] + <span class="string">&#x27; &#x27;</span> + values[<span class="number">2</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (pathName.contains(<span class="string">&#x27;info.txt&#x27;</span>)) &#123;</span><br><span class="line">        String values[] = value.toString().split(<span class="string">&#x27; &#x27;</span>);</span><br><span class="line">        <span class="keyword">if</span> (values.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">          <span class="comment">// data数据格式不规范，字段小于2，抛弃数据</span></span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 数据格式规范，区分标识为0</span></span><br><span class="line">          <span class="type">TextPair</span> <span class="variable">tp</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TextPair</span>(<span class="keyword">new</span> <span class="title class_">Text</span>(values[<span class="number">0</span>]), <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&#x27;0&#x27;</span>));</span><br><span class="line">          context.write(tp, <span class="keyword">new</span> <span class="title class_">Text</span>(values[<span class="number">1</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MR_Join_Partitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;TextPair, Text&gt; &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(TextPair key, Text value, <span class="type">int</span> numParititon)</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> Math.abs(key.getFirst().hashCode() * <span class="number">127</span>) % numParititon;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MR_Join_Comparator</span> <span class="keyword">extends</span> <span class="title class_">WritableComparator</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">MR_Join_Comparator</span><span class="params">()</span> &#123;</span><br><span class="line">      <span class="built_in">super</span>(TextPair.class, <span class="literal">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> &#123;</span><br><span class="line">      <span class="type">TextPair</span> <span class="variable">t1</span> <span class="operator">=</span> (TextPair) a;</span><br><span class="line">      <span class="type">TextPair</span> <span class="variable">t2</span> <span class="operator">=</span> (TextPair) b;</span><br><span class="line">      <span class="keyword">return</span> t1.getFirst().compareTo(t2.getFirst());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">MR_Join_Reduce</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;TextPair, Text, Text, Text&gt; &#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(TextPair key, Iterable&lt;Text&gt; values, Context context)</span></span><br><span class="line">        <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">      <span class="type">Text</span> <span class="variable">pid</span> <span class="operator">=</span> key.getFirst();</span><br><span class="line">      <span class="type">String</span> <span class="variable">desc</span> <span class="operator">=</span> values.iterator().next().toString();</span><br><span class="line">      <span class="keyword">while</span> (values.iterator().hasNext()) &#123;</span><br><span class="line">        context.write(pid, <span class="keyword">new</span> <span class="title class_">Text</span>(values.iterator().next().toString() + <span class="string">&#x27; &#x27;</span> + desc));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String agrs[])</span> </span><br><span class="line">            <span class="keyword">throws</span> IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">    <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">    <span class="type">GenericOptionsParser</span> <span class="variable">parser</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">GenericOptionsParser</span>(conf, agrs);</span><br><span class="line">    String[] otherArgs = parser.getRemainingArgs();</span><br><span class="line">    <span class="keyword">if</span> (agrs.length &lt; <span class="number">3</span>) &#123;</span><br><span class="line">      System.err.println(<span class="string">&#x27;Usage: MRJoin &lt;in_path_one&gt; &lt;in_path_two&gt; &lt;output&gt;&#x27;</span>);</span><br><span class="line">      System.exit(<span class="number">2</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Job</span>(conf, <span class="string">&#x27;MRJoin&#x27;</span>);</span><br><span class="line">    <span class="comment">// 设置运行的job</span></span><br><span class="line">    job.setJarByClass(MRJoin.class);</span><br><span class="line">    <span class="comment">// 设置Map相关内容</span></span><br><span class="line">    job.setMapperClass(MR_Join_Mapper.class);</span><br><span class="line">    <span class="comment">// 设置Map的输出</span></span><br><span class="line">    job.setMapOutputKeyClass(TextPair.class);</span><br><span class="line">    job.setMapOutputValueClass(Text.class);</span><br><span class="line">    <span class="comment">// 设置partition</span></span><br><span class="line">    job.setPartitionerClass(MR_Join_Partitioner.class);</span><br><span class="line">    <span class="comment">// 在分区之后按照指定的条件分组</span></span><br><span class="line">    job.setGroupingComparatorClass(MR_Join_Comparator.class);</span><br><span class="line">    <span class="comment">// 设置Reduce</span></span><br><span class="line">    job.setReducerClass(MR_Join_Reduce.class);</span><br><span class="line">    <span class="comment">// 设置Reduce的输出</span></span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(Text.class);</span><br><span class="line">    <span class="comment">// 设置输入和输出的目录</span></span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">0</span>]));</span><br><span class="line">    FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">1</span>]));</span><br><span class="line">    FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">2</span>]));</span><br><span class="line">    <span class="comment">// 执行，直到结束就退出</span></span><br><span class="line">    System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TextPair</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span>&lt;TextPair&gt; &#123;</span><br><span class="line">  <span class="keyword">private</span> Text first;</span><br><span class="line">  <span class="keyword">private</span> Text second;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">TextPair</span><span class="params">()</span> &#123;</span><br><span class="line">    set(<span class="keyword">new</span> <span class="title class_">Text</span>(), <span class="keyword">new</span> <span class="title class_">Text</span>());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">TextPair</span><span class="params">(String first, String second)</span> &#123;</span><br><span class="line">    set(<span class="keyword">new</span> <span class="title class_">Text</span>(first), <span class="keyword">new</span> <span class="title class_">Text</span>(second));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="title function_">TextPair</span><span class="params">(Text first, Text second)</span> &#123;</span><br><span class="line">    set(first, second);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">set</span><span class="params">(Text first, Text second)</span> &#123;</span><br><span class="line">    <span class="built_in">this</span>.first = first;</span><br><span class="line">    <span class="built_in">this</span>.second = second;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> Text <span class="title function_">getFirst</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> first;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> Text <span class="title function_">getSecond</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> second;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    first.write(out);</span><br><span class="line">    second.write(out);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">    first.readFields(in);</span><br><span class="line">    second.readFields(in);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(TextPair tp)</span> &#123;</span><br><span class="line">    <span class="type">int</span> <span class="variable">cmp</span> <span class="operator">=</span> first.compareTo(tp.first);</span><br><span class="line">    <span class="keyword">if</span> (cmp != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> cmp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> second.compareTo(tp.second);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-3-打包并提交"><a href="#4-3-打包并提交" class="headerlink" title="4.3 打包并提交"></a>4.3 打包并提交</h3><p>第一步：选择File – Project Structure</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018284539.png" alt="img"></p>
<p>第二步：选择Artifacts，并选择+号，选择JAR，选择From modules with…</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018291223.png" alt="img"></p>
<p>第三步：选择Main Class和META-INF</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018297802.png" alt="img"></p>
<p>第四步：根据自己的需求修改JAR包名和路径，并删除依赖</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018304188.png" alt="img"></p>
<p>第五步：选择Build，再选择Build Artifacts</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018310142.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018317311.png" alt="img"></p>
<p>此时jar包就已经打包好了</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018323229.png" alt="img"></p>
<p>将MRJoin.jar 上传至服务器master节点的&#x2F;root&#x2F;下。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018330813.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018337153.png" alt="img"></p>
<p>这里我们需要创建输入路径和输出路径。并上传前面构建的双表文件。</p>
<p>以下是一些hdfs命令基础操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -mkdir /input/</span><br><span class="line"># hadoop fs -mkdir /input/MRJoin/</span><br><span class="line"></span><br><span class="line"># hadoop fs -put data.txt /input/MRJoin/</span><br><span class="line"></span><br><span class="line"># hadoop fs -put info.txt /input/MRJoin/</span><br><span class="line"></span><br><span class="line"># hadoop fs -ls /input/MRJoin/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018347299.png" alt="img"></p>
<p>接下来就是见证程序运行的时刻了！</p>
<h3 id="4-3-1-执行命令"><a href="#4-3-1-执行命令" class="headerlink" title="4.3.1 执行命令"></a>4.3.1 执行命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop  jar  MRJoin.jar  /input/MRJoin/data.txt  /input/MRJoin/info.txt  /output/MRJoin/</span><br></pre></td></tr></table></figure>

<h2 id="4-Hadoop实验：基于MapReduce实现计数器"><a href="#4-Hadoop实验：基于MapReduce实现计数器" class="headerlink" title="4.Hadoop实验：基于MapReduce实现计数器"></a>4.Hadoop实验：基于MapReduce实现计数器</h2><blockquote>
<h3 id="目的-3"><a href="#目的-3" class="headerlink" title="目的"></a>目的</h3><p>1.基于MapReduce思想编写计数器程序。</p>
<h3 id="要求-3"><a href="#要求-3" class="headerlink" title="要求"></a>要求</h3><p>1.理解MapReduce编程思想<br>2.学会编写MapReduce版本计数器程序，并能执行该程序和分析执行过程</p>
<h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1MapReduce计数器是什么？"><a href="#3-1MapReduce计数器是什么？" class="headerlink" title="3.1MapReduce计数器是什么？"></a>3.1MapReduce计数器是什么？</h2><p>计数器是用来记录job的执行进度和状态的。它的作用可以理解为日志。我们可以在程序的某个位置插入计数器，记录数据或者进度的变化情况。</p>
<h2 id="3-2MapReduce计数器能做什么？"><a href="#3-2MapReduce计数器能做什么？" class="headerlink" title="3.2MapReduce计数器能做什么？"></a>3.2MapReduce计数器能做什么？</h2><p>MapReduce 计数器（Counter）为我们提供一个窗口，用于观察 MapReduce Job 运行期的各种细节数据。对MapReduce性能调优很有帮助，MapReduce性能优化的评估大部分都是基于这些 Counter 的数值表现出来的。</p>
<p>在许多情况下，一个用户需要了解待分析的数据，尽管这并非所要执行的分析任务  的核心内容。以统计数据集中无效记录数目的任务为例，如果发现无效记录的比例  相当高，那么就需要认真思考为何存在如此多无效记录。是所采用的检测程序存在  缺陷，还是数据集质量确实很低，包含大量无效记录？如果确定是数据集的质量问  题，则可能需要扩大数据集的规模，以增大有效记录的比例，从而进行有意义的分析。</p>
<p>计数器是一种收集作业统计信息的有效手段，用于质量控制或应用级统计。计数器  还可辅助诊断系统故障。如果需要将日志信息传输到map或reduce任务，更好的  方法通常是尝试传输计数器值以监测某一特定事件是否发生。对于大型分布式作业  而言，使用计数器更为方便。首先，获取计数器值比输出日志更方便，其次，根据 计数器值统计特定事件的发生次数要比分析一堆日志文件容易得多。 </p>
<h2 id="3-3内置计数器"><a href="#3-3内置计数器" class="headerlink" title="3.3内置计数器"></a>3.3内置计数器</h2><p>MapReduce 自带了许多默认Counter，现在我们来分析这些默认 Counter 的含义，方便大家观察 Job  结果，如输入的字节数、输出的字节数、Map端输入&#x2F;输出的字节数和条数、Reduce端的输入&#x2F;输出的字节数和条数等。下面我们只需了解这些内置计数器，知道计数器组名称（groupName）和计数器名称（counterName），以后使用计数器会查找groupName和counterName即可。</p>
<h2 id="3-4自定义计数器"><a href="#3-4自定义计数器" class="headerlink" title="3.4自定义计数器"></a>3.4自定义计数器</h2><p>MapReduce允许用户编写程序来定义计数器，计数器的值可在mapper或reducer  中增加。多个计数器由一个Java枚举(enum)类型来定义，以便对计数器分组。一个作业可以定义的枚举类型数量不限，各个枚举类型所包含的字段数量也不限。枚  举类型的名称即为组的名称，枚举类型的字段就是计数器名称。计数器是全局的。换言之，MapReduce框架将跨所有map和reduce聚集这些计数器，并在作业结束 时产生一个最终结果。</p>
</blockquote>
<h3 id="4-1实验分析设计"><a href="#4-1实验分析设计" class="headerlink" title="4.1实验分析设计"></a>4.1实验分析设计</h3><p>该实验要求学生自己实现一个计数器，统计输入的无效数据。</p>
<p>创建数据文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim counter.txt</span><br></pre></td></tr></table></figure>

<p>规范的格式是3个字段，“\t”作为分隔符，其中有2条异常数据，一条数据是只有2个字段，一条数据是有4个字段。<br>其内容如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jim     1     28</span><br><span class="line">kate    0     26</span><br><span class="line">tom     1</span><br><span class="line">lily    0     29    22</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018739445.png" alt="img"><br>编写代码统计文档中字段不为3个的异常数据个数。如果字段超过3个视为过长字段，字段少于3个视为过短字段。</p>
<h3 id="4-2编写程序"><a href="#4-2编写程序" class="headerlink" title="4.2编写程序"></a>4.2编写程序</h3><p>打开IDEA开发工具，新建包mr 和类 Counters.java 。</p>
<p>项目结构如图：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018748176.png" alt="img"><br>导入Hadoop相关外部依赖。<br>步骤如下：<br>现在下载需要用到的jars，我们选择资料工具 -&gt; 软件下载 -&gt; Spark -&gt; 2.2.2<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018756037.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018761339.png" alt="img"><br>单击File -&gt; Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018768633.png" alt="img"><br>选择 Libraries -&gt; ➕ -&gt;java<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018778555.png" alt="img"><br>选择 &#x2F;spark-2.2.2-bin-hadoop2.7&#x2F;jars 路径下的所有包：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018786604.png" alt="img"><br>这里就可以看到我们导入的jar包<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018792629.png" alt="img"></p>
<p>Counters.java的完整代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">package mr ;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Counter;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line">import org.apache.hadoop.util.GenericOptionsParser;</span><br><span class="line"></span><br><span class="line">public class Counters &#123;</span><br><span class="line">    public static class MyCounterMap extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123;</span><br><span class="line">        public static Counter ct = null;</span><br><span class="line">        protected void map(LongWritable key, Text value,</span><br><span class="line">                org.apache.hadoop.mapreduce.Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span><br><span class="line">                throws java.io.IOException, InterruptedException &#123;</span><br><span class="line">            String arr_value[] = value.toString().split(&#x27;\t&#x27;);</span><br><span class="line">        if (arr_value.length &gt; 3) &#123;</span><br><span class="line">            ct = context.getCounter(&#x27;ErrorCounter&#x27;, &#x27;toolong&#x27;); // ErrorCounter为组名，toolong为组员名</span><br><span class="line">        ct.increment(1); // 计数器加一</span><br><span class="line">        &#125; else if (arr_value.length &lt; 3) &#123;</span><br><span class="line">            ct = context.getCounter(&#x27;ErrorCounter&#x27;, &#x27;tooshort&#x27;);</span><br><span class="line">            ct.increment(1);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">public static void main(String[] args) throws IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        Configuration conf = new Configuration();</span><br><span class="line">        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();</span><br><span class="line">        if (otherArgs.length != 2) &#123;</span><br><span class="line">            System.err.println(&#x27;Usage: Counters &lt;in&gt; &lt;out&gt;&#x27;);</span><br><span class="line">            System.exit(2);</span><br><span class="line">        &#125;</span><br><span class="line">        Job job = new Job(conf, &#x27;Counter&#x27;);</span><br><span class="line">        job.setJarByClass(Counters.class);</span><br><span class="line">    </span><br><span class="line">        job.setMapperClass(MyCounterMap.class);</span><br><span class="line">    </span><br><span class="line">        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));</span><br><span class="line">        System.exit(job.waitForCompletion(true) ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-3打包并提交"><a href="#4-3打包并提交" class="headerlink" title="4.3打包并提交"></a>4.3打包并提交</h3><p>步骤如下：<br>点击File中的Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018856816.png" alt="img"><br>选择Artifacts中的JAR，并选择From modules with dependencies<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018864643.png" alt="img"><br>直接点击OK<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018870513.png" alt="img"><br>选中所有依赖将依赖删除<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018879392.png" alt="img"><br>选择JAR包生成目录<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018885983.png" alt="img"><br>在顶部菜单栏点击Build中的Build Artifacts<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018892401.png" alt="img"><br>最后点击build打包，如下图<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018898392.png" alt="img"></p>
<p>包命名为Counters.jar，最后将jar包传至平台。<br>在hdfs创建input目录，并将之前创建的counter.txt文件上传至此目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir /input</span><br><span class="line"># hdfs dfs -put counter.txt /input</span><br></pre></td></tr></table></figure>

<p>接下来就是见证奇迹的时刻了！额，不好意思，是见证程序运行的时刻了！<br>执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar Counters.jar mr/Counters /input/counter.txt  /output/</span><br></pre></td></tr></table></figure>

<p>其中“hadoop”为命令，“jar”为命令参数，后面紧跟jar包全名。 “&#x2F;input&#x2F;counters.txt”为输入文件，“&#x2F;output&#x2F;”为输出文件（本实验不需要输出，所以输出文件可以为空，即大小为0）。</p>
<h2 id="5-Hadoop实验：基于MapReduce实现去重合并"><a href="#5-Hadoop实验：基于MapReduce实现去重合并" class="headerlink" title="5.Hadoop实验：基于MapReduce实现去重合并"></a>5.Hadoop实验：基于MapReduce实现去重合并</h2><blockquote>
<h3 id="目的-4"><a href="#目的-4" class="headerlink" title="目的"></a>目的</h3><p>1.对输入的多个文件进行合并，并剔除其中重复的内容，去重后的内容输出到一个文件中。<br>2.了解MapReduce最基本的用法</p>
<h3 id="要求-4"><a href="#要求-4" class="headerlink" title="要求"></a>要求</h3><p>1.本次试验后，要求学生能够使用MapReduce计算框架实现文件间的去重合并</p>
<h3 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h3><p>数据去重实例的最终目的是让原始数据中出现次数超过一次的数据在输出文件中只出现一次。我们自然而然会想到将同一个数据的所有记录都交给一台Reduce机器，无论这个数据出现多少次，只要在最终的结果中输出一次就行了。具体就是Reduce的输入应该以数据作为Key，而对value-list则没有要求。当Reduce接收到一个&lt;key,value-list&gt;时就直接将Key复制输出的Key中，并将value设置成空值。在MapReduce流程中，Map的输出&lt;Key,value&gt;经过shuffle过程聚集成&lt;key,value-list&gt;后会被交给Reduce。所以从设计好的Reduce输入可以反推出Map的输出的Key应为数据，而value为任意值。继续反推，Map输出的Key为数据。而在这个实例中每个数据代表输入文件中的一行内容，所以Map阶段要完成的任务就是在采用Haodoop默认的作业输入方式之后，将value设置成Key，并直接输出（输出中的value任意）。Map中的结果经过shuffle过程之后被交给reduce。在Reduce阶段不管每个Key有多少个value，都直接将输入的Key复制为输出的Key，并输出就可以了（输出中的value被设置成空）。</p>
<h2 id="3-1-数据准备"><a href="#3-1-数据准备" class="headerlink" title="3.1 数据准备"></a>3.1 数据准备</h2><p>创建三个文本文件内容如下<br>file1.txt中的内容:</p>
<p>20150101     x<br>20150102     y<br>20150103     x<br>20150104     y</p>
<p>file2.txt中的内容:<br>20150105     z<br>20150106     x<br>20150101     y<br>20150102     y</p>
<p>file3.txt中的内容:<br>20150103     x<br>20150104     z<br>20150105     y</p>
</blockquote>
<h3 id="4-1-编写程序"><a href="#4-1-编写程序" class="headerlink" title="4.1 编写程序"></a>4.1 编写程序</h3><p>现在下载需要用到的jars，我们选择资料工具 -&gt; 软件下载 -&gt; Spark -&gt; 2.2.2<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018398668.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018404156-1749813511809-174.png" alt="img"><br>单击File -&gt; Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018411202-1749813511809-176.png" alt="img"><br>选择 Libraries -&gt; ➕ -&gt;java<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018418071-1749813511809-178.png" alt="img"><br>选择 &#x2F;spark-2.2.2-bin-hadoop2.7&#x2F;jars 路径下的所有包：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018432690-1749813511809-180.png" alt="img"><br>这里就可以看到我们导入的jar包<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018439967-1749813511809-182.png" alt="img"></p>
<p>打开IDEA开发工具，新建名为cn.cstor.mr的Package包，并建一个FileMerge类。</p>
<p>项目结构如图：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018456401-1749813511809-184.png" alt="img"></p>
<h3 id="4-1-1-示例代码如下："><a href="#4-1-1-示例代码如下：" class="headerlink" title="4.1.1 示例代码如下："></a>4.1.1 示例代码如下：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.cstor.mr;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">FileMerge</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Map</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;Object, Text, Text, Text&gt; &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">Text</span> <span class="variable">text</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(Object key, Text value, Context content)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line"> </span><br><span class="line">            text = value;</span><br><span class="line">            content.write(text, <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&#x27;&#x27;</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Reduce</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;Text, Text, Text, Text&gt; &#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            context.write(key, <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&#x27;&#x27;</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line"> </span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        conf.set(<span class="string">&#x27;fs.defaultFS&#x27;</span>, <span class="string">&#x27;hdfs://master:8020&#x27;</span>);</span><br><span class="line">        String[] otherArgs = <span class="keyword">new</span> <span class="title class_">String</span>[]&#123;<span class="string">&#x27;input/f*.txt&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;output&#x27;</span>&#125;;</span><br><span class="line">        <span class="keyword">if</span> (otherArgs.length != <span class="number">2</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">&#x27;Usage:Merge and duplicate removal &lt;in&gt; &lt;out&gt;&#x27;</span>);</span><br><span class="line">            System.exit(<span class="number">2</span>);</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance();</span><br><span class="line">        job.setJarByClass(FileMerge.class);</span><br><span class="line">        job.setMapperClass(Map.class);</span><br><span class="line">        job.setReducerClass(Reduce.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"> </span><br><span class="line">        FileInputFormat.addInputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">0</span>]));</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(otherArgs[<span class="number">1</span>]));</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line"> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-导出Jar包"><a href="#4-2-导出Jar包" class="headerlink" title="4.2 导出Jar包"></a>4.2 导出Jar包</h3><p>步骤如下：<br>点击File中的Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018495325.png" alt="img"><br>选择Artifacts中的JAR，并选择From modules with dependencies<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018503403.png" alt="img"><br>直接点击OK<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018510417.png" alt="img"><br>选中所有依赖将依赖删除<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018598988.png" alt="img"><br>选择JAR包生成目录<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018607507.png" alt="img"><br>在顶部菜单栏点击Build中的Build Artifacts<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018622048.png" alt="img"><br>最后点击build打包，如下图<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018631230.png" alt="img"></p>
<p>Jar包名字为Merge.jar</p>
<p>并上传至平台&#x2F;usr&#x2F;cstor目录下<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018639522.png" alt="img"></p>
<p>在hdfs 上创建&#x2F;user&#x2F;root&#x2F;input目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir -p /user/root/input</span><br></pre></td></tr></table></figure>

<p>将之前创建的三个file文件上传至hdfs中&#x2F;user&#x2F;root&#x2F;input&#x2F;目录下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -put ./file* /user/root/input</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018682263.png" alt="img"></p>
<h3 id="4-3-运行Jar包"><a href="#4-3-运行Jar包" class="headerlink" title="4.3 运行Jar包"></a>4.3 运行Jar包</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop jar Merge.jar cn.cstor.mr/FileMerge</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018691829.png" alt="img"></p>
<h2 id="6-Hadoop实验：基于MapReduce实现倒排索引"><a href="#6-Hadoop实验：基于MapReduce实现倒排索引" class="headerlink" title="6.Hadoop实验：基于MapReduce实现倒排索引"></a>6.Hadoop实验：基于MapReduce实现倒排索引</h2><blockquote>
<h3 id="目的-5"><a href="#目的-5" class="headerlink" title="目的"></a>目的</h3><p>1.加强对MapReduce理解，开阔思路。<br>2.了解倒排索引，并作排序扩展。</p>
<h3 id="要求-5"><a href="#要求-5" class="headerlink" title="要求"></a>要求</h3><p>1.掌握使用MapReduce实现倒排索引并做排序处理。</p>
<h3 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h3><p>倒排索引源于实际应用中需要根据属性的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址。由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引(inverted index)。</p>
<p>本试验通过两次MapReduce对数据进行mr的操作，完成对不同文件的倒排索引，并且加以排序。</p>
</blockquote>
<h3 id="4-1编写程序"><a href="#4-1编写程序" class="headerlink" title="4.1编写程序"></a>4.1编写程序</h3><p>现在下载需要用到的jars，我们选择资料工具 -&gt; 软件下载 -&gt; Spark -&gt; 2.2.2<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010623133.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010629032.png" alt="img"><br>单击File -&gt; Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010635794.png" alt="img"><br>选择 Libraries -&gt; ➕ -&gt;java<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010641993.png" alt="img"><br>选择 &#x2F;spark-2.2.2-bin-hadoop2.7&#x2F;jars 路径下的所有包：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010648983.png" alt="img"><br>这里就可以看到我们导入的jar包<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010656167.png" alt="img"></p>
<p>打开 IDEA开发工具，新建包reverse，并在包下建两个java类，如图:<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010697081.png" alt="img"></p>
<p>ReverseIndexOne.java的完整代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> reverse;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ReverseIndexOne</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">SetOneMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span> &lt; LongWritable, Text, Text, IntWritable &gt;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">private</span> String filename;</span><br><span class="line">        <span class="keyword">private</span> String[] val;</span><br><span class="line">        <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">keyset</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">one</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>(<span class="number">1</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">setup</span><span class="params">(Mapper &lt; LongWritable, Text, Text, IntWritable &gt; .Context context)</span></span><br><span class="line">        <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//获取每个文件的名字</span></span><br><span class="line">            <span class="type">FileSplit</span> <span class="variable">inputSplit</span> <span class="operator">=</span> (FileSplit) context.getInputSplit();</span><br><span class="line">            filename = inputSplit.getPath().getName();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper &lt; LongWritable, Text, Text, IntWritable &gt; .Context context)</span></span><br><span class="line">        <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//文件按照空格拆分 并以单词+文件名作为key输出，1为value输出</span></span><br><span class="line">            val = value.toString().split(<span class="string">&#x27;\\s&#x27;</span>);</span><br><span class="line">            <span class="keyword">for</span>(String str: val)</span><br><span class="line">            &#123;</span><br><span class="line">                keyset.set(str + <span class="string">&#x27; &#x27;</span> + filename);</span><br><span class="line">                context.write(keyset, one);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">SetOneReduce</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span> &lt; Text, IntWritable, Text, IntWritable &gt;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="type">int</span> <span class="variable">sum</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">private</span> <span class="type">IntWritable</span> <span class="variable">val</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">IntWritable</span>();</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(Text key, Iterable &lt; IntWritable &gt; values, Reducer &lt; Text, IntWritable, Text, IntWritable &gt; .Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">        &#123;</span><br><span class="line">            sum = <span class="number">0</span>;</span><br><span class="line">            <span class="comment">//进行聚合累加算出每个单词在每个文件中的个数</span></span><br><span class="line">            <span class="keyword">for</span>(IntWritable intWritable: values)</span><br><span class="line">            &#123;</span><br><span class="line">                sum += intWritable.get();</span><br><span class="line">            &#125;</span><br><span class="line">            val.set(sum);</span><br><span class="line">            context.write(key, val);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//创建并且提交任务</span></span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(ReverseIndexOne.class);</span><br><span class="line">        job.setMapperClass(SetOneMapper.class);</span><br><span class="line">        job.setReducerClass(SetOneReduce.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(IntWritable.class);</span><br><span class="line">        <span class="comment">//文件加载的路径</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&#x27;/input/reverse/setOne&#x27;</span>));</span><br><span class="line">        <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&#x27;/output/reverse/setOne&#x27;</span>);</span><br><span class="line">        FileSystem.get(conf).delete(path, <span class="literal">true</span>);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, path);</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ReverseIndexTwo.java的完整代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> reverse;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ReverseIndexTwo</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//创建一个类做排序使用</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">ReverseBean</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span> &lt; ReverseBean &gt;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="type">int</span> num;</span><br><span class="line">        <span class="keyword">private</span> String filename;</span><br><span class="line">        <span class="keyword">private</span> String word;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getNum</span><span class="params">()</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> num;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setNum</span><span class="params">(<span class="type">int</span> num)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">this</span>.num = num;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> String <span class="title function_">getFilename</span><span class="params">()</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> filename;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setFilename</span><span class="params">(String filename)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">this</span>.filename = filename;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> String <span class="title function_">getWord</span><span class="params">()</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> word;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setWord</span><span class="params">(String word)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">this</span>.word = word;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException</span><br><span class="line">        &#123;</span><br><span class="line">            out.writeInt(num);</span><br><span class="line">            out.writeUTF(filename);</span><br><span class="line">            out.writeUTF(word);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in )</span> <span class="keyword">throws</span> IOException</span><br><span class="line">        &#123;</span><br><span class="line">            num = in .readInt();</span><br><span class="line">            filename = in .readUTF();</span><br><span class="line">            word = in .readUTF();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(ReverseBean o)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">check</span> <span class="operator">=</span> <span class="built_in">this</span>.getWord().compareTo(o.getWord());</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span>(check == <span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                check = o.getNum() - <span class="built_in">this</span>.getNum();</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> check;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">SetTwoMap</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span> &lt; LongWritable, Text, ReverseBean, NullWritable &gt;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">private</span> String[] val;</span><br><span class="line">        <span class="keyword">private</span> <span class="type">ReverseBean</span> <span class="variable">bean</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ReverseBean</span>();</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Mapper &lt; LongWritable, Text, ReverseBean, NullWritable &gt; .Context context)</span></span><br><span class="line">        <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//同样的按照空格拆分并组装到自定义的类中</span></span><br><span class="line">            val = value.toString().split(<span class="string">&#x27;\\s&#x27;</span>);</span><br><span class="line">            bean.setWord(val[<span class="number">0</span>]);</span><br><span class="line">            bean.setFilename(val[<span class="number">1</span>]);</span><br><span class="line">            bean.setNum(Integer.parseInt(val[<span class="number">2</span>]));</span><br><span class="line">            context.write(bean, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">ReverseComp</span> <span class="keyword">extends</span> <span class="title class_">WritableComparator</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="title function_">ReverseComp</span><span class="params">()</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">super</span>(ReverseBean.class, <span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">ReverseBean</span> <span class="variable">r1</span> <span class="operator">=</span> (ReverseBean) a;</span><br><span class="line">            <span class="type">ReverseBean</span> <span class="variable">r2</span> <span class="operator">=</span> (ReverseBean) b;</span><br><span class="line">            <span class="keyword">return</span> r1.getWord().compareTo(r2.getWord());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">SetTwoReduce</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span> &lt; ReverseBean, NullWritable, Text, NullWritable &gt;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">private</span> <span class="type">StringBuffer</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuffer</span>();</span><br><span class="line">        <span class="keyword">private</span> <span class="type">Text</span> <span class="variable">keyset</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Text</span>();</span><br><span class="line">        </span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(ReverseBean key, Iterable &lt; NullWritable &gt; values, Reducer &lt; ReverseBean, NullWritable, Text, NullWritable &gt; .Context context)</span></span><br><span class="line">        <span class="keyword">throws</span> IOException, InterruptedException</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// StringBuffer置为空</span></span><br><span class="line">            sb.setLength(<span class="number">0</span>);</span><br><span class="line">            <span class="comment">//进行组装</span></span><br><span class="line">            sb.append(key.getWord() + <span class="string">&#x27;\t&#x27;</span>);</span><br><span class="line">            <span class="keyword">for</span>(NullWritable nullWritable: values)</span><br><span class="line">            &#123;</span><br><span class="line">                sb.append(key.getFilename() + <span class="string">&#x27;--&amp;gt;&#x27;</span> + key.num + <span class="string">&#x27;,&#x27;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//删除掉最后多出的逗号</span></span><br><span class="line">            sb.deleteCharAt(sb.length() - <span class="number">1</span>);</span><br><span class="line">            keyset.set(sb.toString());</span><br><span class="line">            context.write(keyset, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(ReverseIndexTwo.class);</span><br><span class="line">        job.setMapperClass(SetTwoMap.class);</span><br><span class="line">        job.setReducerClass(SetTwoReduce.class);</span><br><span class="line">        job.setMapOutputKeyClass(ReverseBean.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setGroupingComparatorClass(ReverseComp.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&#x27;/output/reverse/setOne/&#x27;</span>));</span><br><span class="line">        <span class="type">Path</span> <span class="variable">path</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Path</span>(<span class="string">&#x27;/output/reverse/setTwo&#x27;</span>);</span><br><span class="line">        FileSystem.get(conf).delete(path, <span class="literal">true</span>);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, path);</span><br><span class="line">        System.exit(job.waitForCompletion(<span class="literal">true</span>) ? <span class="number">0</span> : <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-2打包并提交"><a href="#4-2打包并提交" class="headerlink" title="4.2打包并提交"></a>4.2打包并提交</h3><p>将该项目代码打成jar包，这里不用指定主类，我们下面采用命令参数的方式来控制主类。</p>
<p>点击File中的Project Structure</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010727901.png" alt="img"></p>
<p>选择Artifacts中的JAR，并选择From modules with dependencies</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010735538.png" alt="img"></p>
<p>直接点击OK</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010743021.png" alt="img"></p>
<p>选中所有依赖将依赖删除</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010749311.png" alt="img"></p>
<p>选择JAR包生成目录</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676010757315.png" alt="img"></p>
<p>在顶部菜单栏点击Build中的Build Artifacts</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676016104603.png" alt="img"></p>
<p>最后点击build打包，如下图</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676016115654.png" alt="img"></p>
<p>打包文件命名为ReverseIndex.jar</p>
<p>将ReverseIndex.jar 上传至服务器master节点的&#x2F;root&#x2F;下。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676016124090.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676016130030.png" alt="img"></p>
<p>通过命令创建输入文件夹：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir -p /input/reverse/setOne</span><br></pre></td></tr></table></figure>

<h3 id="4-3上传数据文件"><a href="#4-3上传数据文件" class="headerlink" title="4.3上传数据文件"></a>4.3上传数据文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -put /root/dataset/* /input/reverse/setOne</span><br></pre></td></tr></table></figure>

<p>接下来就是见证奇迹的时刻了！额，不好意思，是见证程序运行的时刻了！</p>
<p>执行命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop jar ReverseIndex.jar reverse/ReverseIndexOne</span><br></pre></td></tr></table></figure>

<p>其中“hadoop”为命令，“jar”为命令参数，后面紧跟jar包全名。 “reverse&#x2F;ReverseIndexOne”为指定运行的主类，即程序入口。</p>
<h2 id="7-Hadoop实验：HDFS基础与部署"><a href="#7-Hadoop实验：HDFS基础与部署" class="headerlink" title="7.Hadoop实验：HDFS基础与部署"></a>7.Hadoop实验：HDFS基础与部署</h2><blockquote>
<h3 id="目的-6"><a href="#目的-6" class="headerlink" title="目的"></a>目的</h3><p>1.理解大数据生态圈各组件特性<br>2.理解HDFS存在的优势<br>3.理解HDFS体系架构<br>4.学会在环境中部署HDFS学会HDFS基本命令</p>
<h3 id="要求-6"><a href="#要求-6" class="headerlink" title="要求"></a>要求</h3><p>1.要求实验结束时，能够构建出HDFS集群<br>2.要求能够在构建出的HDFS集群上使用基本的HDFS命令<br>3.要求能够大致了解Hadoop生态圈中各个组件</p>
<h3 id="原理-6"><a href="#原理-6" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1-Hadoop-生态："><a href="#3-1-Hadoop-生态：" class="headerlink" title="3.1 Hadoop 生态："></a>3.1 Hadoop 生态：</h2><p>Hadoop是一个提供高可靠，可扩展（横向）的分布式计算的开源软件平台。<br>1.Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构。<br>2.主要解决，海量数据的存储和海量数据的分析计算问题。<br>3.我们现在讲HADOOP 通常是指一个更广泛的概念——HADOOP 生态圈。<br>Hadoop的核心是HDFS和MapReduce、yarn，图3-1为hadoop生态系统：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676256145817.png" alt="img"><br>图3-1</p>
<p>HDFS（Hadoop分布式文件系统）<br>HDFS是Hadoop体系中数据存储管理的基础。它是一个高度容错的系统，能检测和应对硬件故障，用于在低成本的通用硬件上运行。HDFS简化了文件的一致性模型，通过流式数据访问，提供高吞吐量应用程序数据访问功能，适合带有大型数据集的应用程序。 </p>
<p>HDFS主要有以下几个部分组成：<br>Client：切分文件；访问HDFS；与NameNode交互，获取文件位置信息；与DataNode交互，读取和写入数据。<br>NameNode：Master节点，在hadoop1.X中只有一个，管理HDFS的名称空间和数据块映射信息，配置副本策略，处理客户端请求。<br>DataNode：Slave节点，存储实际的数据，汇报存储信息给NameNode。<br>Secondary NameNode：辅助NameNode，分担其工作量；定期合并fsimage和edits，推送给NameNode；</p>
<p>Mapreduce（分布式计算框架）<br>MapReduce是一种计算模型，用以进行大数据量的计算。其中Map对数据集上的独立元素进行指定的操作，生成键-值对形式中间结果。Reduce则对中间结果中相同“键”的所有“值”进行规约，以得到最终结果。MapReduce这样的功能划分，非常适合在大量计算机组成的分布式并行环境里进行数据处理。 </p>
<p>Yarn（资源管理框架）<br>YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的  Hadoop  资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。<br>主要工作过程如下：<br>ResourceManager主要负责所有的应用程序的资源分配，<br>ApplicationMaster主要负责每个作业的任务调度，也就是说每一个作业对应一个ApplicationMaster。<br>Nodemanager是接收Resourcemanager 和ApplicationMaster的命令来实现资源的分配执行体。<br>在YARN中，资源管理由ResourceManager和NodeManager共同完成，其中，ResourceManager中的调度器负责资源的分配，而NodeManager则负责资源的供给和隔离。ResourceManager将某个NodeManager上资源分配给任务（这就是所谓的“资源调度”）后，NodeManager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础的保证，这就是所谓的资源隔离。<br>在Yarn平台上可以运行多个计算框架，如：MR，Tez，Storm，Spark等计算，框架。</p>
<p>Sqoop（数据同步工具）<br>Sqoop是SQL-to-Hadoop的缩写，主要用于传统数据库和Hadoop之间传输数据。数据的导入和导出本质上是Mapreduce程序，充分利用了MR的并行化和容错性。其中主要利用的是MP中的Map任务来实现并行导入，导出。</p>
<p>Hbase（分布式列存数据库）<br>HBase是一个针对结构化数据的可伸缩、高可靠、高性能、分布式和面向列的动态模式数据库。和传统关系数据库不同，HBase采用了BigTable的数据模型：增强的稀疏排序映射表（Key&#x2F;Value），其中，键由行关键字、列关键字和时间戳构成。HBase提供了对大规模数据的随机、实时读写访问，同时，HBase中保存的数据可以使用MapReduce来处理，它将数据存储和并行计算完美地结合在一起。<br>Zookeeper（分布式协作服务）<br>主要解决分布式环境下的数据管理问题：统一命名，状态同步，集群管理，配置同步等。</p>
<p>Hive（基于Hadoop的数据仓库）<br>Hive定义了一种类似SQL的查询语言(HQL),将SQL转化为MapReduce任务在Hadoop上执行。通常用于离线分析。</p>
<p>Flume（日志收集工具）<br>它将数据从产生、传输、处理并最终写入目标的路径的过程抽象为数据流，在具体的数据流中，数据源支持在Flume中定制数据发送方，从而支持收集各种不同协议数据。同时，Flume数据流提供对日志数据进行简单处理的能力，如过滤、格式转换等。此外，Flume还具有能够将日志写往各种数据目标（可定制）的能力。总的来说，Flume是一个可扩展、适合复杂环境的海量日志收集系统。</p>
<p>Spark<br>Spark 是专为大规模数据处理而设计的快速通用的计算引擎。Spark拥有Hadoop  MapReduce所具有的优点；但不同于MapReduce的是——Job中间输出结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。</p>
<p>Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。<br>尽管创建 Spark 是为了支持分布式数据集上的迭代作业，但是实际上它是对 Hadoop 的补充，可以在 Hadoop 文件系统中并行运行。通过名为 Mesos 的第三方集群框架可以支持此行为。Spark可用来构建大型的、低延迟的数据分析应用程序。</p>
<p>Storm<br>Storm为分布式实时计算提供了一组通用原语，可被用于“流处理”之中，实时处理消息并更新数据库。<br>Storm可以方便地在一个计算机集群中编写与扩展复杂的实时计算，Storm用于实时处理，就好比 Hadoop 用于批处理。Storm保证每个消息都会得到处理，而且它很快，在一个小集群中，每秒可以处理数以百万计的消息。</p>
<h2 id="3-2-HDFS优点："><a href="#3-2-HDFS优点：" class="headerlink" title="3.2 HDFS优点："></a>3.2 HDFS优点：</h2><p>1.存储并管理PB级数据<br>2.处理非结构化数据<br>3.注重数据处理的吞吐量（latency）<br>4.应用模式为：write-once-read-many 存取模式<br>5.完美解决数据传输和读写的性能瓶颈<br>6.过去所拥有的技能可以平稳过渡。比如SQL,R<br>7.极大的降低转移平台的成本<br>8.大幅度减少系统维护工具</p>
</blockquote>
<h3 id="4-1-设置-Host-映射文件"><a href="#4-1-设置-Host-映射文件" class="headerlink" title="4.1 设置 Host 映射文件"></a>4.1 设置 Host 映射文件</h3><p>1.设置 IP 地址与机器名的映射，设置信息如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/hosts   //配置主机名对应的IP地址  设置：&lt;IP 地址&gt; &lt;主机名&gt;</span><br></pre></td></tr></table></figure>

<p>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.30.3.114     master</span><br><span class="line">10.30.0.105     slave1</span><br><span class="line">10.30.3.115     slave2</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736459749.png" alt="img"><br>注意：就是在打开的 &#x2F;etc&#x2F;hosts 文件的最后一行加上 ip+master、ip+slave1、ip+slave2，记得使用的是 tab 键而不是空格。<br>2.使用 ping 命令验证设置是否成功。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ping slave1</span><br><span class="line"># ping slave2</span><br></pre></td></tr></table></figure>

<p><img src="http://10.131.2.101/static/upload/resource/exp/ins/edc9e36227434cabae9533e9b3c3dc14/image/image.1696736504878.png" alt="img"></p>
<h3 id="4-2-设置集群节点免密登录"><a href="#4-2-设置集群节点免密登录" class="headerlink" title="4.2 设置集群节点免密登录"></a>4.2 设置集群节点免密登录</h3><p>1.生成master服务器密钥<br>执行命令ssh-keygen，生成master服务器密钥。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ssh-keygen</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736605630.png" alt="img"><br>在master上执行“ssh-keygen”命令生成公私钥。</p>
<p>第一个提示是询问将公私钥文件存放在哪，直接回车，选择默认位置。<br>第二个提示是请求用户输入密钥，既然操作的目的就是实现SSH无密钥登录，故此处必须使用空密钥，所谓的空密钥指的是直接回车，不是空格，更不是其他字符。此处请读者务必直接回车，使用空密钥。<br>第三个提示是要求用户确认刚才输入的密钥，既然刚才是空密钥（直接回车即空），那现在也应为空，直接回车即可。</p>
<p>最后，可通过命令“ls -all &#x2F;root&#x2F;.ssh”查看到，SSH密钥文件夹.ssh目录下的确生成了两个文件id_rsa和id_rsa_pub，这两个文件都有用，其中公钥用于加密，私钥用于解密。中间的rsa表示算法为RSA算法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ls -all /root/.ssh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736621652.png" alt="img"><br>2.拷贝master服务器公钥至本机<br>执行命令ssh-copy-id master，将master服务器公钥拷贝至master服务器本身。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ssh-copy-id master</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736634637.png" alt="img"><br>&lt;1&gt;第一次连接 master 时，需要输入用户密码来完成公钥文件传输。<br>&lt;2&gt;验证 master 服务器 ssh 免密登录 master 本身<br>&lt;3&gt;公钥拷贝完成后，可以在 master 服务器上直接执行命令 ssh master ，查看是否可以免密登录 master 服务器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ssh master       #登录本机网络地址</span><br><span class="line"># exit    #退出本次登录</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736667698.png" alt="img"><br>&lt;4&gt;拷贝 master 服务器公钥至其余服务器<br>&lt;5&gt;执行命令 ssh-copy-id slave1，将 master 服务器公钥拷贝至 slave1 服务器。<br>&lt;6&gt;第一次连接 slave1 时，需要输入用户密码来完成公钥文件传输。<br>&lt;7&gt;依照同样的方式将公钥拷贝至 slave2 服务器。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736797757.png" alt="img"><br>公钥拷贝完成后，可以在 master 服务器上直接执行命令 ssh master ，查看是否可以免密登录 slave1 ~ 2 服务器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ssh  slave1    //用户从master远程登录slave1</span><br><span class="line"># ssh  slave2    //用户从master远程登录slave2</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736826005.png" alt="img"><br>注意：我们搭建的是3节点完全分布式的Hadoop集群，所以3个节点的 &#x2F;etc&#x2F;hosts 文件要保持一致。用 scp 命令分发文件到每个节点覆盖原有配置。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># scp /etc/hosts slave1:/etc/hosts</span><br><span class="line"># scp /etc/hosts slave2:/etc/hosts</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736840267.png" alt="img"></p>
<h3 id="4-3-设置操作系统环境"><a href="#4-3-设置操作系统环境" class="headerlink" title="4.3 设置操作系统环境"></a>4.3 设置操作系统环境</h3><p>确认一下当前的Hadoop版本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop version</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736908192.png" alt="img"></p>
<h3 id="4-3-1设置JDK安装目录"><a href="#4-3-1设置JDK安装目录" class="headerlink" title="4.3.1设置JDK安装目录"></a>4.3.1设置JDK安装目录</h3><p>进入conf目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>

<p>编辑文件“hadoop-env.sh”</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim hadoop-env.sh</span><br></pre></td></tr></table></figure>

<p>找到如下一行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=$&#123;JAVA_HOME&#125;</span><br></pre></td></tr></table></figure>

<p>将这行内容修改为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_161</span><br></pre></td></tr></table></figure>

<p>这里的“&#x2F;usr&#x2F;local&#x2F;jdk1.8.0_161”就是JDK安装位置，如果不同，请根据实际情况更改。</p>
<h3 id="4-3-2-配置HDFS"><a href="#4-3-2-配置HDFS" class="headerlink" title="4.3.2 配置HDFS"></a>4.3.2 配置HDFS</h3><p>编辑文件“core-site.xml”，将如下内容嵌入此文件里最后两行的标签之间：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim core-site.xml</span><br></pre></td></tr></table></figure>

<p>内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/usr/cstor/hadoop/cloud&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://master:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736939570.png" alt="img"><br>配置“workers”文件，将localhost修改为slave1~2：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim workers</span><br></pre></td></tr></table></figure>

<p>输入内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1  </span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>

<p>core-site.xml属性说明<br>1.hadoop.tmp.dir 设置hadoop运行时产生文件的存储目录<br>2.fs.defalutFS 设置hadoop文件系统，由URL指定8020位namenode和datanode的通信端口</p>
<h3 id="4-3-3-HDFS配置文件关键属性介绍"><a href="#4-3-3-HDFS配置文件关键属性介绍" class="headerlink" title="4.3.3 HDFS配置文件关键属性介绍"></a>4.3.3 HDFS配置文件关键属性介绍</h3><p>1.io.file.buffer.size 设置缓冲区大小，默认4kb（64kb 128kb）<br>2.fs.trash.interval  设置回收站中的文件保留多久后删除，以分钟为单位，默认值是0，表示回收特性无效。该回收功能是用户级特性，启用后，每个用户都有自己独立的回收站目录，即home目录下的.trash目录，恢复时只要从该目录找到被删除的文件，将其移除就可以了。hdfs会自动删除回收站中的文件，其它文件系统不具备这个功能，需要使用下列命令自行删除 hadoop fs -expunge<br>3.dfs.block.size 设置hdfs块大小，默认64mb （128mb 256mb）<br>4.dfs.balance.bandwidthPerSec 设置均衡器在不同节点之间复制数据的带宽<br>5.dfs.datanode.du.reserved 设置保留空间的大小，以供其它程序使用,以字节为单位<br>6.fs.checkpoint.period 设置辅助namenode每隔多久创建检查点，以秒为单位<br>7.fs.checkpoint.size 设置当编辑日志（edits）大大小达到多少mb时，创建检查点，系统每5分钟检查一次编辑日志大小<br>8.dfs.datanode.numblocks 设置datanode一个目录存放多少个块时，就重新创建一个子目录<br>9.dfs.datanode.scan.period.hours 设置datanode块扫描的周期，默认三周（504小时）扫描一次</p>
<h3 id="4-3-4-拷贝集群配置至其它服务器"><a href="#4-3-4-拷贝集群配置至其它服务器" class="headerlink" title="4.3.4 拷贝集群配置至其它服务器"></a>4.3.4 拷贝集群配置至其它服务器</h3><p>在master机上执行下列命令，将配置好的hadoop从master节点拷贝至slave节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor</span><br><span class="line"># scp -r hadoop/ slave1:/usr/cstor/ </span><br><span class="line"># scp -r hadoop/ slave2:/usr/cstor/</span><br></pre></td></tr></table></figure>

<p>Hadoop 环境搭建<br>在master服务器上格式化主节点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop/bin/hadoop namenode -format</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696736997381.png" alt="img"></p>
<h3 id="4-4-启动Hadoop"><a href="#4-4-启动Hadoop" class="headerlink" title="4.4 启动Hadoop"></a>4.4 启动Hadoop</h3><p>启动前需注意我们需要在环境变量添加如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>

<p>启动Hadoop的命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/sbin</span><br><span class="line"># ./start-dfs.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676268536706.png" alt="img"></p>
<h3 id="4-5-通过查看进程的方式验证"><a href="#4-5-通过查看进程的方式验证" class="headerlink" title="4.5 通过查看进程的方式验证"></a>4.5 通过查看进程的方式验证</h3><p>master节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696737055731.png" alt="img"><br>slave节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ssh slave1</span><br><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696737067256.png" alt="img"></p>
<h3 id="4-6-HDFS基本命令格式如下"><a href="#4-6-HDFS基本命令格式如下" class="headerlink" title="4.6 HDFS基本命令格式如下"></a>4.6 HDFS基本命令格式如下</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -cmd args</span><br></pre></td></tr></table></figure>

<p>其中，cmd为具体的操作，args为参数。<br>部分HDFS命令示例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -help ls           //查看ls命令的帮助文档</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696742394147.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -mkdir -p /user/test        //建立目录/user/test</span><br><span class="line"># hadoop fs -ls /user                //查看/user目录下的目录和文件</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696742403895.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># vim a.txt                      //创建,编辑a.txt文件</span><br><span class="line">hello world!!!</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696742497111.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -put a.txt /user/test    //上传a.txt文件至/user/test</span><br><span class="line"># hadoop fs -cat /user/test/a.txt    //查看/user/test/a.txt文件内容</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1696742576071.png" alt="img"></p>
<h2 id="8-Hadoop实验：YARN集群部署与介绍"><a href="#8-Hadoop实验：YARN集群部署与介绍" class="headerlink" title="8.Hadoop实验：YARN集群部署与介绍"></a>8.Hadoop实验：YARN集群部署与介绍</h2><blockquote>
<h3 id="目的-7"><a href="#目的-7" class="headerlink" title="目的"></a>目的</h3><p>1.了解什么是MapReduce框架<br>2.了解什么是YARN框架<br>3.学会搭建YARN分布式集群<br>4.学会使用YARN集群提交一些简单的任务<br>5.理解YARN作为Hadoop生态中的资源管理器的意义。</p>
<h3 id="要求-7"><a href="#要求-7" class="headerlink" title="要求"></a>要求</h3><p>1.学会如何搭建YARN集群<br>2.实验结束后可以用YARN集群来提交任务<br>3.观察任务提交之后YARN的执行过程</p>
<h3 id="原理-7"><a href="#原理-7" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1-MapReduce-概述"><a href="#3-1-MapReduce-概述" class="headerlink" title="3.1 MapReduce 概述"></a>3.1 MapReduce 概述</h2><h3 id="3-1-1简介"><a href="#3-1-1简介" class="headerlink" title="3.1.1简介"></a>3.1.1简介</h3><p>MapReduce是一个用于处理海量数据的分布式计算框架。这个框架解决了数据分布式存储（数据都存取在HDFS上，计算的目标数据就是来自于HDFS）、作业调度（一个Hadoop集群上可以跑很多个MapReduce，不可能某一个MapReduce占了所有资源，资源是共享的）、容错（非个人因素导致的问题比如网络堵塞、机器间通信等复杂问题，会自动切换到其他节点上）。</p>
<h3 id="3-1-2MapReduce分而治之思想"><a href="#3-1-2MapReduce分而治之思想" class="headerlink" title="3.1.2MapReduce分而治之思想"></a>3.1.2MapReduce分而治之思想</h3><p>1.数钱实例：一堆钞票，各种面值分别多少<br>-单点策略(一个人数所有的钞票，数出各种面值有多少张)<br>-分治策略(每个人分得一堆钞票，数出各种面值有多少种; 汇总，每个人负责统计一种面值)<br>2.解决数据可以切割进行计算的应用<br>分治思想（分解、求解、合并）<br>MapReduce映射（分：map（把复杂的问题分解为若干“简单的任务”）合：reduce）</p>
<h3 id="3-1-3MapReduce计算框架·执行流程"><a href="#3-1-3MapReduce计算框架·执行流程" class="headerlink" title="3.1.3MapReduce计算框架·执行流程"></a>3.1.3MapReduce计算框架·执行流程</h3><p>Map部分：<br>一个map实际上对应一个split分片，首先map读取split，因为map是一个程序，作为系统里面的一个进程，自己维护着一个进程空间。把split数据读进来之后直接存到了自己的内存上（buffer in  memory），然后开始往内存写。内存默认大小为100M，但是100M很容易写满，当它写到80M的时候会锁住内存区，然后把这80%的数据转储到磁盘上，然后清理内存。转储的过程中会作排序（sort），图中partitions中三个部分相当于前面数钱的例子中面值的分类。三个小的数据部分再归并排序成大的数据（merge on disk）。途中只展示出一个Map的执行流程，还有other maps。</p>
<p>Reduce部分<br>还是数钱的例子，假设图中Reduce部分是负责处理一百元面值的，把每个Map上属于一百元区域的数据通过fetch红线全部归纳到Reduce机器上（相当于拷贝）。然后把从每个从Map拷贝过来的数据两两合并，再统一交给Reduce处理，最后输出。</p>
<h3 id="3-1-4两个重要的进程"><a href="#3-1-4两个重要的进程" class="headerlink" title="3.1.4两个重要的进程"></a>3.1.4两个重要的进程</h3><p>&lt;1&gt;JobTracker主进程，负责接收客户作业提交，调度任务到作节点上运行，并提供诸如监控工作节点状态及任务进度等管理功能，一个MapReduce集群有一个JobTracker，一般运行在可靠的硬件上。<br>TaskTracker是通过周期性的心跳来通知JobTracker其当前的健康状态，每一次心跳包含了可用的Map和Reduce任务数目、占用的数目以及运行中的任务详细信息。JobTracker利用一个线程池来同时处理心跳和客户请求。</p>
<p>&lt;2&gt;TaskTracker由JobTracker指派任务，实例化用户程序，在本地执行任务并周期性地向JobTracker汇报状态。在每一个工作节点上永远只会有一个TaskTracker。<br>-JobTracker一直在等待用户提交作业<br>-TaskTracker每隔3秒向JobTracker发送心跳询问有没有任务可做，如果有，让其派发任务给它执行.</p>
<h3 id="3-1-5MapReduce配置文件属性介绍"><a href="#3-1-5MapReduce配置文件属性介绍" class="headerlink" title="3.1.5MapReduce配置文件属性介绍"></a>3.1.5MapReduce配置文件属性介绍</h3><p>mapred-site.xml（MapReduce守护进程的配置项，包括作业历史服务器）<br>    mapreduce.frame.work(告诉mapreduce运行在什么之上)<br>    yarn.app.mapreduce.am.resource.mb(mapreduce作业的缺省配置)<br>    mapreduce.map.memory.mb(每个map task需要的内存量)<br>    mapreduce.ruduce.memory.mb(每个map task 需要的内存量)</p>
<h2 id="3-2-YARN概述"><a href="#3-2-YARN概述" class="headerlink" title="3.2 YARN概述"></a>3.2 YARN概述</h2><p>YARN是一个资源管理、任务调度的框架，采用master&#x2F;slave架构，主要包含三大模块：ResourceManager（RM）、NodeManager（NM）、ApplicationMaster（AM）。</p>
<p>其中，ResourceManager负责所有资源的监控、分配和管理，运行在主节点；  NodeManager负责每一个节点的维护，运行在从节点；ApplicationMaster负责每一个具体应用程序的调度和协调，只有在有任务正在执行时存在。对于所有的applications，RM拥有绝对的控制权和对资源的分配权。而每个AM则会和RM协商资源，同时和NodeManager通信来执行和监控task。几个模块之间的关系如图3-1所示：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277051834.png" alt="img"><br>图3-1</p>
<h3 id="3-2-1-YARN运行流程"><a href="#3-2-1-YARN运行流程" class="headerlink" title="3.2.1 YARN运行流程"></a>3.2.1 YARN运行流程</h3><p>YARN运行流程如图 3-2 所示：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277057903.png" alt="img"><br>图3-2</p>
<p>master向RM提交应用程序，其中包括启动该应用的ApplicationMaster的必须信息，例如ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。</p>
<p>ResourceManager启动一个container用于运行ApplicationMaster。<br>启动中的ApplicationMaster向ResourceManager注册自己，启动成功后与RM保持心跳。<br>ApplicationMaster向ResourceManager发送请求，申请相应数目的container。<br>ResourceManager返回ApplicationMaster的申请的containers信息。申请成功的container，由ApplicationMaster进行初始化。container的启动信息初始化后，AM与对应的NodeManager通信，要求NM启动container。AM与NM保持心跳，从而对NM上运行的任务进行监控和管理。</p>
<p>container运行期间，ApplicationMaster对container进行监控。container通过RPC协议向对应的AM汇报自己的进度和状态等信息。<br>应用运行期间，master直接与AM通信获取应用的状态、进度更新等信息。<br>应用运行结束后，ApplicationMaster向ResourceManager注销自己，并允许属于它的container被收回。</p>
<h3 id="3-2-2-Yarn配置文件属性介绍"><a href="#3-2-2-Yarn配置文件属性介绍" class="headerlink" title="3.2.2 Yarn配置文件属性介绍"></a>3.2.2 Yarn配置文件属性介绍</h3><p>yarn-site.xml（YARN守护进程的配置项，包括资源管理器、web应用代理服务器和节点管理器）</p>
<p>yarn.resourcemanager.hostname(运行资源管理器的机器主机名，默认值为0.0.0.0。)</p>
<p>yarn.resourcemanager.address(运行资源管理器的PRC服务器的主机名和端口。)</p>
<p>yarn.nodemanager.local-dirs(逗号分隔的目录名称，是YARN容器本地临时存储空间。)</p>
<p>yarn.nodemanager.aux-services	 (逗号分隔的服务名称，是节点管理器运行的附加服务列表。每项服务由属性yarn.nodemanager.auxservices.servicename.class所定义的类实现。默认情况下，不指定附加服务)</p>
<p>yarn.nodemanager.resource.memorymb(节点管理器运行的容器可以分配到的物理内存容量（单位是MB）。默认值是8192。)</p>
<p>yarn.nodemanager.vmem-pmem-ratio(容器所占的虚拟内存和物理内存之比。该值指示了虚拟内存的使用可以超过所分配内存的量。默认值是2.1)</p>
<p>yarn.nodemanager.resource.cpuvcores(节点管理器运行的容器可以分配到的CPU核数目)</p>
</blockquote>
<h3 id="4-1-在master机上配置YARN"><a href="#4-1-在master机上配置YARN" class="headerlink" title="4.1 在master机上配置YARN"></a>4.1 在master机上配置YARN</h3><p>操作之前请确认HDFS已经启动，具体操作参考之前的实验内容。<br>指定YARN主节点，编辑文件“&#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;yarn-site.xml”，将如下内容嵌入此文件里configuration标签间：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277881816.png" alt="img"><br>yarn-site.xml是YARN守护进程的配置文件。第一句配置了ResourceManager的主机名，第二句配置了节点管理器运行的附加服务为mapreduce_shuffle，只有这样才可以运行MapReduce程序。<br>在master机上操作：将配置好的YARN配置文件拷贝至slaveX。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# scp -r /usr/cstor/hadoop/etc/hadoop/yarn-site.xml slave1:/usr/cstor/hadoop/etc/hadoop/</span><br><span class="line">[root@master ~]# scp -r /usr/cstor/hadoop/etc/hadoop/yarn-site.xml slave2:/usr/cstor/hadoop/etc/hadoop/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277890425.png" alt="img"></p>
<h3 id="4-2-统一启动YARN"><a href="#4-2-统一启动YARN" class="headerlink" title="4.2 统一启动YARN"></a>4.2 统一启动YARN</h3><p>确认已配置slaves文件，在master机器上查看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# cat  /usr/cstor/hadoop/etc/hadoop/slaves</span><br></pre></td></tr></table></figure>

<p>内容如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277900136.png" alt="img"><br>环境变量中应有如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure>

<p>YARN配置无误，统一启动YARN：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# /usr/cstor/hadoop/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h3 id="4-3-验证YARN启动成功"><a href="#4-3-验证YARN启动成功" class="headerlink" title="4.3 验证YARN启动成功"></a>4.3 验证YARN启动成功</h3><p>读者可分别在三台机器上执行如下命令，查看YARN服务是否已启动。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# jps      //jps查看java进程</span><br></pre></td></tr></table></figure>

<p>你会在master上看到类似的如下信息：<br>2347  ResourceManager</p>
<p>这表明在master节点成功启动ResourceManager，它负责整个集群的资源管理分配，是一个全局的资源管理系统。</p>
<p>而在slave1、slave2上看到类似的如下信息：<br>4021  NodeManager</p>
<p>NodeManager是每个节点上的资源和任务管理器，它是管理这台机器的代理，负责该节点程序的运行，以及该节点资源的管理和监控。YARN集群每个节点都运行一个NodeManager。</p>
<p>在当前的Windows机器上打开浏览器，地址栏输入master的IP和端口号8088（例：10.1.1.7:8088），即可在Web界面看到YARN相关信息。</p>
<h3 id="4-4-在master机上提交DistributedShell任务"><a href="#4-4-在master机上提交DistributedShell任务" class="headerlink" title="4.4 在master机上提交DistributedShell任务"></a>4.4 在master机上提交DistributedShell任务</h3><p>distributedshell，他可以看做YARN编程中的“hello  world”，它的主要功能是并行执行用户提供的shell命令或者shell脚本。-jar指定了包含ApplicationMaster的jar文件，-shell_command指定了需要被ApplicationMaster执行的Shell命令。<br>在xshell上再打开一个master的连接，执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# /usr/cstor/hadoop/bin/yarn \</span><br><span class="line">org.apache.hadoop.yarn.applications.distributedshell.Client  -jar \</span><br><span class="line">/usr/cstor/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar \</span><br><span class="line">-shell_command  uptime</span><br></pre></td></tr></table></figure>

<h3 id="4-5-在master机上提交MapReduce型任务"><a href="#4-5-在master机上提交MapReduce型任务" class="headerlink" title="4.5 在master机上提交MapReduce型任务"></a>4.5 在master机上提交MapReduce型任务</h3><h3 id="4-5-1-指定在YARN上运行MapReduce任务"><a href="#4-5-1-指定在YARN上运行MapReduce任务" class="headerlink" title="4.5.1 指定在YARN上运行MapReduce任务"></a>4.5.1 指定在YARN上运行MapReduce任务</h3><p>首先，在master机上，将文件“&#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml. template”重命名为“&#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml”。<br>接着，编辑此文件并将如下内容嵌入此文件的configuration标签间：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277917656.png" alt="img"><br>最后，将master机的“&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;mapred-site.xml”文件拷贝到slaveX，重新启动集群。</p>
<h3 id="4-5-2-在master端提交PI-Estimator任务"><a href="#4-5-2-在master端提交PI-Estimator任务" class="headerlink" title="4.5.2 在master端提交PI Estimator任务"></a>4.5.2 在master端提交PI Estimator任务</h3><p>首先进入Hadoop安装目录：&#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;，然后提交PI Estimator任务。<br>命令最后两个两个参数的含义：第一个参数是指要运行map的次数，这里是2次；第二个参数是指每个map任务，取样的个数；而两数相乘即为总的取样数。Pi Estimator使用Monte Carlo方法计算Pi值的，Monte Carlo方法自行百度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi 2 10</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676277926562.png" alt="img"></p>
<h2 id="9-Hadoop实验：基于MapReduce实现二次排序"><a href="#9-Hadoop实验：基于MapReduce实现二次排序" class="headerlink" title="9.Hadoop实验：基于MapReduce实现二次排序"></a>9.Hadoop实验：基于MapReduce实现二次排序</h2><blockquote>
<h3 id="目的-8"><a href="#目的-8" class="headerlink" title="目的"></a>目的</h3><p>1.基于MapReduce思想编写SecondarySort程序。</p>
<h3 id="要求-8"><a href="#要求-8" class="headerlink" title="要求"></a>要求</h3><p>1.理解MapReduce编程思想<br>2.学会编写MapReduce版本二次排序程序，将其执行并分析执行过程。</p>
<h3 id="原理-8"><a href="#原理-8" class="headerlink" title="原理"></a>原理</h3><p>MR默认会对键进行排序，然而有的时候我们也有对值进行排序的需求。满足这种需求一是可以在reduce阶段排序收集过来的values，但是，如果有数量巨大的values可能就会导致内存溢出等问题，这就是二次排序应用的场景——将对值的排序也安排到MR计算过程之中，而不是单独来做。</p>
<p>二次排序就是首先按照第一字段排序，然后再对第一字段相同的行按照第二字段排序，注意不能破坏第一次排序的结果。</p>
</blockquote>
<h3 id="4-1准备数据"><a href="#4-1准备数据" class="headerlink" title="4.1准备数据"></a>4.1准备数据</h3><p>输入数据如下：secsortdata.txt (‘\t’分割)（数据放在&#x2F;root&#x2F;dataset目录下,如果没有请自行创建）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">7	444</span><br><span class="line">3	9999</span><br><span class="line">7	333</span><br><span class="line">4	22</span><br><span class="line">3	7777</span><br><span class="line">7	555</span><br><span class="line">3	6666</span><br><span class="line">6	0</span><br><span class="line">3	8888</span><br><span class="line">4	11</span><br></pre></td></tr></table></figure>

<p>准备好数据后，我们在将数据上传至HDFS的input目录之中（如果HDFS中没有input、output目录请自行创建）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /input</span><br><span class="line">hdfs dfs -mkdir /output     创建目录命令</span><br><span class="line"></span><br><span class="line">hdfs dfs -put secsortdata.txt /input</span><br><span class="line">hdfs dfs -ls /input</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676017923553.png" alt="img"></p>
<h3 id="4-2编写程序-1"><a href="#4-2编写程序-1" class="headerlink" title="4.2编写程序"></a>4.2编写程序</h3><p>项目架构：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676017932011.png" alt="img"></p>
<p>我们首先导入Hadoop相关外部依赖。</p>
<p>现在下载需要用到的jars，我们选择资料工具 -&gt; 软件下载 -&gt; Spark -&gt; 2.2.2</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676017987300.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676017993190.png" alt="img"><br>单击File -&gt; Project Structure</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018002155.png" alt="img"></p>
<p>选择 Libraries -&gt; ➕ -&gt;java </p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018009918.png" alt="img"></p>
<p>选择 &#x2F;spark-2.2.2-bin-hadoop2.7&#x2F;jars 路径下的所有包：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018016333.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018032446.png" alt="img"></p>
<p>程序主要难点在于排序和聚合。</p>
<p>对于排序我们需要定义一个IntPair类用于数据的存储，并在IntPair类内部自定义Comparator类以实现第一字段和第二字段的比较。</p>
<p>对于聚合我们需要定义一个FirstPartitioner类，在FirstPartitioner类内部指定聚合规则为第一字段。</p>
<p>此外，我们还需要开启MapReduce框架自定义Partitioner 功能和GroupingComparator功能。</p>
<p>IntPair 类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">IntPair</span> <span class="keyword">implements</span> <span class="title class_">WritableComparable</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> IntWritable first;</span><br><span class="line">    <span class="keyword">private</span> IntWritable second;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">set</span><span class="params">(IntWritable first, IntWritable second)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.first = first;</span><br><span class="line">        <span class="built_in">this</span>.second = second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//注意：需要添加无参的构造方法，否则反射时会报错。</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">IntPair</span><span class="params">()</span> &#123;</span><br><span class="line">        set(<span class="keyword">new</span> <span class="title class_">IntWritable</span>(), <span class="keyword">new</span> <span class="title class_">IntWritable</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">IntPair</span><span class="params">(<span class="type">int</span> first, <span class="type">int</span> second)</span> &#123;</span><br><span class="line">        set(<span class="keyword">new</span> <span class="title class_">IntWritable</span>(first), <span class="keyword">new</span> <span class="title class_">IntWritable</span>(second));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">IntPair</span><span class="params">(IntWritable first, IntWritable second)</span> &#123;</span><br><span class="line">        set(first, second);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> IntWritable <span class="title function_">getFirst</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> first;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setFirst</span><span class="params">(IntWritable first)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.first = first;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> IntWritable <span class="title function_">getSecond</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setSecond</span><span class="params">(IntWritable second)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.second = second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">write</span><span class="params">(DataOutput out)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        first.write(out);</span><br><span class="line">        second.write(out);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">readFields</span><span class="params">(DataInput in)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        first.readFields(in);</span><br><span class="line">        second.readFields(in);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> first.hashCode() * <span class="number">163</span> + second.hashCode();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (o <span class="keyword">instanceof</span> IntPair) &#123;</span><br><span class="line">            <span class="type">IntPair</span> <span class="variable">tp</span> <span class="operator">=</span> (IntPair) o;</span><br><span class="line">            <span class="keyword">return</span> first.equals(tp.first) &amp;&amp; second.equals(tp.second);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> first + <span class="string">&quot;\t&quot;</span> + second;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(Object o)</span> &#123;</span><br><span class="line">        IntPair tp=(IntPair) o;</span><br><span class="line">        <span class="type">int</span> <span class="variable">cmp</span> <span class="operator">=</span> first.compareTo(tp.first);</span><br><span class="line">        <span class="keyword">if</span> (cmp != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> cmp;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> second.compareTo(tp.second);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>SecondarySort类：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparator;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SecondarySort</span> &#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TheMapper</span> <span class="keyword">extends</span> <span class="title class_">Mapper</span>&lt;LongWritable, Text, IntPair, NullWritable&gt; &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span><br><span class="line">                <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            String[] fields = value.toString().split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line">            <span class="type">int</span> <span class="variable">field1</span> <span class="operator">=</span> Integer.parseInt(fields[<span class="number">0</span>]);</span><br><span class="line">            <span class="type">int</span> <span class="variable">field2</span> <span class="operator">=</span> Integer.parseInt(fields[<span class="number">1</span>]);</span><br><span class="line">            context.write(<span class="keyword">new</span> <span class="title class_">IntPair</span>(field1,field2), NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">TheReducer</span> <span class="keyword">extends</span> <span class="title class_">Reducer</span>&lt;IntPair, NullWritable,IntPair, NullWritable&gt; &#123;</span><br><span class="line">        <span class="comment">//private static final Text SEPARATOR = new Text(&quot;------------------------------------------------&quot;);</span></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="keyword">void</span> <span class="title function_">reduce</span><span class="params">(IntPair key, Iterable&lt;NullWritable&gt; values, Context context)</span></span><br><span class="line">                <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">            context.write(key, NullWritable.get());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">FirstPartitioner</span> <span class="keyword">extends</span> <span class="title class_">Partitioner</span>&lt;IntPair, NullWritable&gt; &#123;</span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">getPartition</span><span class="params">(IntPair key, NullWritable value,</span></span><br><span class="line"><span class="params">                <span class="type">int</span> numPartitions)</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> Math.abs(key.getFirst().get()) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//如果不添加这个类，默认第一列和第二列都是升序排序的。</span></span><br><span class="line"><span class="comment">//这个类的作用是使第一列升序排序，第二列降序排序</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">KeyComparator</span> <span class="keyword">extends</span> <span class="title class_">WritableComparator</span> &#123;</span><br><span class="line">        <span class="comment">//无参构造器必须加上，否则报错。</span></span><br><span class="line">        <span class="keyword">protected</span> <span class="title function_">KeyComparator</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="built_in">super</span>(IntPair.class, <span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(WritableComparable a, WritableComparable b)</span> &#123;</span><br><span class="line">            <span class="type">IntPair</span> <span class="variable">ip1</span> <span class="operator">=</span> (IntPair) a;</span><br><span class="line">            <span class="type">IntPair</span> <span class="variable">ip2</span> <span class="operator">=</span> (IntPair) b;</span><br><span class="line">            <span class="comment">//第一列按升序排序</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">cmp</span> <span class="operator">=</span> ip1.getFirst().compareTo(ip2.getFirst());</span><br><span class="line">            <span class="keyword">if</span> (cmp != <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> cmp;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//在第一列相等的情况下，第二列按倒序排序</span></span><br><span class="line">            <span class="keyword">return</span> -ip1.getSecond().compareTo(ip2.getSecond());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//入口程序</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">Configuration</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Configuration</span>();</span><br><span class="line">        <span class="type">Job</span> <span class="variable">job</span> <span class="operator">=</span> Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(SecondarySort.class);</span><br><span class="line">        <span class="comment">//设置Mapper的相关属性</span></span><br><span class="line">        job.setMapperClass(TheMapper.class);</span><br><span class="line">        <span class="comment">//当Mapper中的输出的key和value的类型和Reduce输出</span></span><br><span class="line"><span class="comment">//的key和value的类型相同时，以下两句可以省略。</span></span><br><span class="line">        <span class="comment">//job.setMapOutputKeyClass(IntPair.class);</span></span><br><span class="line">        <span class="comment">//job.setMapOutputValueClass(NullWritable.class);</span></span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">0</span>]));</span><br><span class="line">        <span class="comment">//设置分区的相关属性</span></span><br><span class="line">        job.setPartitionerClass(FirstPartitioner.class);</span><br><span class="line">        <span class="comment">//在map中对key进行排序</span></span><br><span class="line">        job.setSortComparatorClass(KeyComparator.class);</span><br><span class="line">        <span class="comment">//job.setGroupingComparatorClass(GroupComparator.class);</span></span><br><span class="line">        <span class="comment">//设置Reducer的相关属性</span></span><br><span class="line">        job.setReducerClass(TheReducer.class);</span><br><span class="line">        job.setOutputKeyClass(IntPair.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> <span class="title class_">Path</span>(args[<span class="number">1</span>]));</span><br><span class="line">        <span class="comment">//设置Reducer数量</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">reduceNum</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(args.length &gt;= <span class="number">3</span> &amp;&amp; args[<span class="number">2</span>] != <span class="literal">null</span>)&#123;</span><br><span class="line">            reduceNum = Integer.parseInt(args[<span class="number">2</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        job.setNumReduceTasks(reduceNum);</span><br><span class="line">        job.waitForCompletion(<span class="literal">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-3打包提交"><a href="#4-3打包提交" class="headerlink" title="4.3打包提交"></a>4.3打包提交</h3><p>使用 IEDA 开发工具将该代码打包，选择主类为 SecondarySort 。如果没有指定主类，那么在执行时就要指定须执行的类。</p>
<p>选择File – Project Structure</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018062283.png" alt="img"></p>
<p>选择Artifacts，并选择+号，选择JAR，选择From modules with…</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018069868.png" alt="img"></p>
<p>选择Main Class和META-INF</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018076448.png" alt="img"></p>
<p>根据自己的需求修改JAR包名和路径，并删除依赖</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018082508.png" alt="img"></p>
<p>选择Build，再选择Build Artifacts</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018090643.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018097532.png" alt="img"></p>
<p>此时jar包就已经打包好了</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018103635.png" alt="img"></p>
<p>我们再使用WinSCP将jar包上传至我们的集群中去。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018110050.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/cstor/hadoop/</span><br><span class="line">bin/hadoop jar /root/MR.jar /input/secsortdata.txt /output/secsort</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676018116884.png" alt="img"></p>
<p>其中“hadoop”为命令，“jar”为命令参数，后面紧跟打的包，&#x2F;input&#x2F;secsortdata.txt”为输入文件在HDFS中的位置，“&#x2F;output&#x2F;”为输出文件在HDFS中的位置。</p>
<h2 id="10-Hive实验：Hive组件介绍与部署"><a href="#10-Hive实验：Hive组件介绍与部署" class="headerlink" title="10.Hive实验：Hive组件介绍与部署"></a>10.Hive实验：Hive组件介绍与部署</h2><blockquote>
<h3 id="目的-9"><a href="#目的-9" class="headerlink" title="目的"></a>目的</h3><p>1.本实验我们将学习Hive的工作原理接学会部署Hive环境。</p>
<h3 id="要求-9"><a href="#要求-9" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能：<br>1.了解HIve与关系数据库的区别；<br>2.了解Hive 的优点和缺点；<br>3.学会部署Hive环境并启动；</p>
<h3 id="原理-9"><a href="#原理-9" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1Hive介绍"><a href="#3-1Hive介绍" class="headerlink" title="3.1Hive介绍"></a>3.1Hive介绍</h2><p>Hive：由Facebook开源，用于解决海量结构化日志的数据统计。<br>hive是基于Hadoop的一个数据仓库工具，用来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。hive数据仓库工具能将结构化的数据文件映射为一张数据库表，并提供SQL查询功能，能将SQL语句转变成MapReduce任务来执行。</p>
<h2 id="3-2Hive与关系数据库的区别"><a href="#3-2Hive与关系数据库的区别" class="headerlink" title="3.2Hive与关系数据库的区别"></a>3.2Hive与关系数据库的区别</h2><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803107354.png" alt="img"><br>使用 Hive 的命令行接口很像操作关系数据库，但是 Hive 和关系数据库还是有很大的不同， Hive 与关系数据库的区别具体如下：<br>Hive 和关系数据库存储文件的系统不同，Hive 使用的是 Hadoop 的 HDFS（Hadoop 的分布式文件系统），关系数据库则是服务器本地的文件系统。<br>Hive 使用的计算模型是 MapReduce，而关系数据库则是自身的计算模型。<br>关系数据库都是为实时查询的业务进行设计的，而 Hive 则是为海量数据做数据挖掘设计的，实时性很差；实时性的区别导致 Hive 的应用场景和关系数据库有很大的不同。<br>Hive 很容易扩展自己的存储能力和计算能力，这个是继承 Hadoop 的，而关系数据库在这个方面相比起来要差很多。</p>
<h2 id="3-3Hive的优缺点"><a href="#3-3Hive的优缺点" class="headerlink" title="3.3Hive的优缺点"></a>3.3Hive的优缺点</h2><h3 id="3-3-1优点"><a href="#3-3-1优点" class="headerlink" title="3.3.1优点"></a>3.3.1优点</h3><p>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。<br>避免了去写MapReduce，减少开发人员的学习成本。<br>Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</p>
<h3 id="3-3-2缺点"><a href="#3-3-2缺点" class="headerlink" title="3.3.2缺点"></a>3.3.2缺点</h3><p>Hive 不支持记录级别的增删改操作<br>Hive 的查询延时很严重<br>Hive 不支持事务</p>
<h2 id="3-4Hive-架构"><a href="#3-4Hive-架构" class="headerlink" title="3.4Hive 架构"></a>3.4Hive 架构</h2><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803114049.png" alt="img"><br>由上图可知，Hadoop 的 MapReduce 是 Hive 架构的根基。Hive 架构包括如下组件：CLI（command line  interface）、JDBC&#x2F;ODBC、Thrift Server、WEB GUI、Metastore 和  Driver(Complier、Optimizer 和 Executor)，这些组件分为两大类：服务端组件和客户端组件。</p>
<h3 id="3-4-1服务端组件"><a href="#3-4-1服务端组件" class="headerlink" title="3.4.1服务端组件"></a>3.4.1服务端组件</h3><p>Driver 组件：该组件包括 Complier、Optimizer 和 Executor，它的作用是将 HiveQL（类 SQL）语句进行解析、编译优化，生成执行计划，然后调用底层的 MapReduce 计算框架。</p>
<p>Metastore 组件：元数据服务组件，这个组件存储 Hive 的元数据，Hive 的元数据存储在关系数据库里，Hive  支持的关系数据库有 derby 和 mysql。元数据对于 Hive 十分重要，因此 Hive 支持把 metastore  服务独立出来，安装到远程的服务器集群里，从而解耦 Hive 服务和 metastore 服务，保证 Hive 运行的健壮性。<br>Thrift 服务：Thrift 是 facebook 开发的一个软件框架，它用来进行可扩展且跨语言的服务的开发，Hive 集成了该服务，能让不同的编程语言调用 hive 的接口。</p>
<h3 id="3-4-2客户端组件"><a href="#3-4-2客户端组件" class="headerlink" title="3.4.2客户端组件"></a>3.4.2客户端组件</h3><p> CLI：command line interface，命令行接口。<br>Thrift 客户端：上面的架构图里没有写上 Thrift 客户端，但是 Hive 架构的许多客户端接口是建立在 Thrift 客户端之上，包括 JDBC 和 ODBC 接口。<br>WEBGUI：Hive 客户端提供了一种通过网页的方式访问 hive 所提供的服务。这个接口对应 Hive 的 hwi 组件（hive web interface），使用前要启动 hwi 服务。</p>
</blockquote>
<h3 id="4-1设置环境变量"><a href="#4-1设置环境变量" class="headerlink" title="4.1设置环境变量"></a>4.1设置环境变量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/profile</span><br><span class="line"></span><br><span class="line">export HIVE_HOME=/usr/cstor/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803123370.png" alt="img"></p>
<p>编辑完成后保存并退出，使用 source 命令来激活以上环境变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># source /etc/profile</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803129529.png" alt="img"></p>
<h3 id="4-2配置hive文件"><a href="#4-2配置hive文件" class="headerlink" title="4.2配置hive文件"></a>4.2配置hive文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hive/conf/</span><br><span class="line"># ls</span><br><span class="line"># cp hive-default.xml.template hive-site.xml</span><br><span class="line"># ls</span><br><span class="line"># vim hive-site.xml</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803137654.png" alt="img"></p>
<p>以下的配置项你可以通过 Vim 编辑器的搜索功能进行快速的定位（命令模式下按 &#x2F; 键可以进行查找），也可以参考行数进行寻找。<br>在第  396 行，找到名称为 javax.jdo.option.ConnectionURL 的配置项，将其配置值修改为  jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;hive?createDatabaseIfNotExist&#x3D;true。这是在设置元数据库的 JDBC 连接信息。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803145238.png" alt="img"></p>
<p>在第 790 行，找到名称为 javax.jdo.option.ConnectionDriverName 的配置项，将其配置值修改为 com.mysql.jdbc.Driver。这是在设置数据库连接的驱动程序名称。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803151719.png" alt="img"></p>
<p>在第 815 行，找到名称为 javax.jdo.option.ConnectionUserName 的配置项，将其配置值修改为 root。这是在设置连接数据库所使用的用户名</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803158022.png" alt="img"></p>
<p>在第 381 行，找到名称为 javax.jdo.option.ConnectionPassword 的配置项，将其配置值修改为 123456。这是在设置连接数据库所使用的密码。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803165153.png" alt="img"></p>
<p>在第 47 行，找到名称为 hive.exec.scratchdir 的配置项，将其配置值修改为 &#x2F;user&#x2F;hive&#x2F;tmp。这是在设置 HDFS 上临时文件的目录路径。</p>
<p>在第 52 行，找到名称为 hive.exec.local.scratchdir 的配置项，将其配置值修改为 &#x2F;tmp&#x2F;hive。这是在设置本地文件系统中临时文件的目录路径。注意该路径不是 HDFS 上的路径，而是本地文件系统的路径。</p>
<p>在第 57 行，找到名称为 hive.downloaded.resources.dir 的配置项，将其配置值修改为 &#x2F;tmp&#x2F;hive 。这是在设置下载资源的存放位置。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803172949.png" alt="img"></p>
<p>在第 1320 行，找到名称为 hive.querylog.location 的配置项，将其配置值修改为 &#x2F;home&#x2F;hadoop&#x2F;hive&#x2F;log 。这是在设置查询日志的存放位置。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803178795.png" alt="img"></p>
<p>在第3215行将&amp;#8三个字符删除</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803186596.png" alt="img"></p>
<h3 id="4-3元数据库配置"><a href="#4-3元数据库配置" class="headerlink" title="4.3元数据库配置"></a>4.3元数据库配置</h3><p>在 MySQL 数据库中创建上述配置项所用到的用户及密码,进入到 MySQL 的命令行之后，使用如下语句来创建名为root的数据库用户，密码也设置为 123456 。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># mysqladmin -u root password &#x27;123456&#x27;</span><br><span class="line"># mysql -u root -p123456</span><br><span class="line">none&gt; use mysql;</span><br><span class="line">mysql&gt; update user set password=&#x27;*6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9&#x27; where</span><br><span class="line">host=&#x27;master&#x27; and user=&#x27;root&#x27;;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803195089.png" alt="img"></p>
<p>最后需要将 MySQL 的 JDBC 驱动复制到 Hive 的库目录中，以便于 Hive 连接到 MySQL。</p>
<p>请在终端中输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># cp /root/resource/mysql/mysql-connector-Java/5.1.47/* /usr/cstor/hive/lib/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803200797.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803205439.png" alt="img"></p>
<h3 id="4-4设置-Hive-体系参数"><a href="#4-4设置-Hive-体系参数" class="headerlink" title="4.4设置 Hive 体系参数"></a>4.4设置 Hive 体系参数</h3><p>元数据库配置好之后，还需要设置 Hive 内部用于识别 Hadoop 位置、内部配置文件路径等方面的配置项。首先是使用 cp 命令将设置模版复制一份，使其生效。</p>
<p>请在终端中输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hive/conf</span><br><span class="line"># cp hive-env.sh.template hive-env.sh</span><br></pre></td></tr></table></figure>

<p>随后用 Vim 编辑 器打开 hive-env.sh 文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim hive-env.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803214854.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">在第 48 行，设置 Hadoop 的安装路径：</span><br><span class="line"></span><br><span class="line">HADOOP_HOME=/usr/cstor/hadoop</span><br><span class="line"></span><br><span class="line">在第 51 行，设置 Hive 的配置文件目录路径：</span><br><span class="line"></span><br><span class="line">export HIVE_CONF_DIR=/usr/cstor/hive/conf</span><br><span class="line"></span><br><span class="line">在第 54 行，设置 Hive 的外部插件库目录路径：</span><br><span class="line"></span><br><span class="line">export HIVE_AUX_JARS_PATH=/usr/cstor/hive/lib</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803222171.png" alt="img"></p>
<p>对以上配置项编辑完成后，保存并退出编辑器。</p>
<h3 id="4-5初始化元数据库"><a href="#4-5初始化元数据库" class="headerlink" title="4.5初始化元数据库"></a>4.5初始化元数据库</h3><p>所有的配置工作完成后，就可以开始初始化元数据库了。由于稍后用到的数据会存储在 HDFS 上，因此需要提前启动好 HDFS 。(本次我们使用的环境已经启动了HDFS，所以不用启动了，如果没有启动HDFS请输入一下命令)</p>
<p>启动 HDFS ：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd/usr/cstor/hadoop</span><br><span class="line"># sbin/start-dfs.sh</span><br><span class="line"># sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p>如果是首次使用 Hadoop ，则需要通过 hdfs namenode -format 对其进行初始化。此处将会使用 Hive 自带的 schematool 工具来完成初始化工作。请在终端中输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># schematool -initSchema -dbType mysql</span><br></pre></td></tr></table></figure>

<p><img src="http://10.131.2.101/static/upload/resource/exp/ins/1e481a76f9e6400da7aed47f9ef79355/image/image.1681803230943.png" alt="img"></p>
<h3 id="4-6启动Hive"><a href="#4-6启动Hive" class="headerlink" title="4.6启动Hive"></a>4.6启动Hive</h3><p>当提示信息中表示已经完成初始化之后，就可以通过 hive 命令进入到其命令行。请在终端中输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hive</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803238012.png" alt="img"></p>
<h2 id="11-Hive实验：Hive基础操作"><a href="#11-Hive实验：Hive基础操作" class="headerlink" title="11.Hive实验：Hive基础操作"></a>11.Hive实验：Hive基础操作</h2><blockquote>
<h3 id="目的-10"><a href="#目的-10" class="headerlink" title="目的"></a>目的</h3><p>1.了解和学习hive的基本操作。</p>
<h3 id="要求-10"><a href="#要求-10" class="headerlink" title="要求"></a>要求</h3><p>本实验要求学生能：<br>1.hive中创建各类表；<br>2.修改hive表中的结构；<br>3.运用常用语法对表中数据进行查询；<br>4.运用排序语句对表中数据进行排序整理。</p>
<h3 id="原理-10"><a href="#原理-10" class="headerlink" title="原理"></a>原理</h3><p>hive 表的介绍包括：普通表，外部表，分区表，Bucket 表，表的命名，表增加、删除分区增加、更新列修改列的名字、类型、位置、注释，增加表的元数据信息等。</p>
<p>Hive没有专门的数据存储格式，也没有为数据建立索引，用户可以非常自由的组织 Hive中的表，只需要在创建表的时候告诉Hive数据中的列分隔符和行分隔符，Hive就可以解析数据。</p>
<p>Hive中所有的数据都存储在HDFS中，Hive中包含以下数据模型：表(Table)，外部表(External Table)，分区(Partition)，桶(Bucket)。</p>
<p>Hive中Table和数据库中 Table在概念上是类似的，每一个Table在Hive中都有一个相应的目录存储数据。例如，一个表  pvs，它在 HDFS 中的路径为：&#x2F;wh&#x2F;pvs，其中，wh  是在hive-site.xml中由${hive.metastore.warehouse.dir}指定的数据仓库的目录，所有的 Table  数据（不包括 External Table）都保存在这个目录中。</p>
<p>分区(Partition) 对应于数据库中的 分区(Partition) 列的密集索引，但是 Hive 中 分区(Partition)  的组织方式和数据库中的很不相同。在 Hive 中，表中的一个分区(Partition) 对应于表下的一个目录，所有的分区(Partition)  的数据都存储在对应的目录中。例如：pvs 表中包含 ds 和 ctry 两个分区(Partition)，则对应于 ds &#x3D; 20090801,  ctry &#x3D; US 的 HDFS 子目录为：&#x2F;wh&#x2F;pvs&#x2F;ds&#x3D;20090801&#x2F;ctry&#x3D;US；对应于 ds &#x3D; 20090801,  ctry &#x3D; CA 的 HDFS 子目录为；&#x2F;wh&#x2F;pvs&#x2F;ds&#x3D;20090801&#x2F;ctry&#x3D;CA。<br>外部表(External Table) 指向已经在 HDFS 中存在的数据，可以创建分区(Partition)。它和 Table 在元数据的组织上是相同的，而实际数据的存储则有较大的差异。</p>
<p>Table 的创建过程和数据加载过程（这两个过程可以在同一个语句中完成），在加载数据的过程中，实际数据会被移动到数据仓库目录中；之后对数据的访问将会直接在数据仓库目录中完成。</p>
</blockquote>
<h3 id="4-1启动hive"><a href="#4-1启动hive" class="headerlink" title="4.1启动hive"></a>4.1启动hive</h3><p>需要重新启动 hdfs，以及启动 mysql。<br>然后启动 Hadoop 集群：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/sbin/</span><br><span class="line"># hdfs namenode -format </span><br><span class="line"># ./start-all.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803260291.png" alt="img"></p>
<p><img src="http://10.131.2.101/static/upload/resource/exp/ins/17bea027cb244429a6701ac60804d065/image/image.1681803265305.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hive/bin/</span><br><span class="line"># hive</span><br></pre></td></tr></table></figure>

<p><img src="http://10.131.2.101/static/upload/resource/exp/ins/17bea027cb244429a6701ac60804d065/image/image.1681803270812.png" alt="img"></p>
<h3 id="4-2建表（CREATE）的语法如下"><a href="#4-2建表（CREATE）的语法如下" class="headerlink" title="4.2建表（CREATE）的语法如下"></a>4.2建表（CREATE）的语法如下</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name</span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[COMMENT table_comment]</span><br><span class="line">[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">[CLUSTERED BY (col_name, col_name, ...)</span><br><span class="line">[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">[ROW FORMAT row_format]</span><br><span class="line">[STORED AS file_format]</span><br><span class="line">[LOCATION hdfs_path]</span><br></pre></td></tr></table></figure>

<p>上面的一些关键字解释：</p>
<p>CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXIST 选项来忽略这个异常<br>EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）<br>LIKE 允许用户复制现有的表结构，但是不复制数据<br>COMMENT 可以为表与字段增加描述<br>ROW FORMAT 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW  FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的  SerDe，Hive 通过 SerDe 确定表的具体的列的数据。<br>STORED AS 如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCE。</p>
<h3 id="4-3建表（CREATE）"><a href="#4-3建表（CREATE）" class="headerlink" title="4.3建表（CREATE）"></a>4.3建表（CREATE）</h3><h3 id="4-3-1创建普通表"><a href="#4-3-1创建普通表" class="headerlink" title="4.3.1创建普通表"></a>4.3.1创建普通表</h3><p>首先创建people表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table if not exists people(id string,name string,sex string)</span><br><span class="line">&gt; row format delimited </span><br><span class="line">&gt;fields terminated by &#x27;,&#x27;</span><br><span class="line">&gt;stored as textfile;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803280182.png" alt="img"><br>构造测试数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat s.txt</span><br></pre></td></tr></table></figure>

<p>（可以根据图片自行建立一个简单数据。）<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803287346.png" alt="img"><br>加载数据到people表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &#x27;/root/s.txt&#x27; into table people;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803293794.png" alt="img"></p>
<p>查询people表数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from people;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803303325.png" alt="img"></p>
<p>查询指定列</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select name, sex from people;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803309438.png" alt="img"><br>别名<br>可以通过给查询列起别名的方式来更改原有列的名称：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select name n, sex s from people;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803315345.png" alt="img"></p>
<h3 id="4-3-2创建外部表"><a href="#4-3-2创建外部表" class="headerlink" title="4.3.2创建外部表"></a>4.3.2创建外部表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE EXTERNAL TABLE cstor1(</span><br><span class="line">        id             INT,</span><br><span class="line">        email          STRING,</span><br><span class="line">        name           STRING</span><br><span class="line">        )</span><br><span class="line">        LOCATION &#x27;/home/hive/external&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803331804.png" alt="img"><br>和简单表相比较，可以发现外部表多了 external 的关键字说明以及 LOCATION 指定外部表存放的路径（如果没有 LOCATION，Hive 将在 HDFS  上的&#x2F;user&#x2F;hive&#x2F;warehouse 文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里)。</p>
<h3 id="4-3-3-Hive分区表"><a href="#4-3-3-Hive分区表" class="headerlink" title="4.3.3 Hive分区表"></a>4.3.3 Hive分区表</h3><p>创建分区表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table parthive (createdate string, value string) partitioned by (year string) row format delimited fields terminated by &#x27;\t&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803339986.png" alt="img"><br>查看新建的表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803347272.png" alt="img"><br>给parthive表创建两个分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; alter table parthive add partition(year=&#x27;2014&#x27;);</span><br><span class="line">hive&gt; alter table parthive add partition(year=&#x27;2015&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803353627.png" alt="img"><br>查看parthive的表结构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; describe parthive;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803360678.png" alt="img"><br>向year&#x3D;2015分区导入本地数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &#x27;/root/data/12/parthive.txt&#x27; into table parthive partition(year=&#x27;2015&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803369409.png" alt="img"><br>根据条件查询year&#x3D;2015的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from parthive t where t.year=&#x27;2015&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803402429.png" alt="img"><br>根据条件统计year&#x3D;2015的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from parthive where year=&#x27;2015&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803389682.png" alt="img"></p>
<p>在hive命令行中创建parthive表，并创建2014和2015两个分区，查看其表结构<br>用命令查看HDFS文件，Hive中parthive表在HDFS文件中的存储目录结构如下图</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803426801.png" alt="img"></p>
<h3 id="4-3-4-创建-Bucket-表"><a href="#4-3-4-创建-Bucket-表" class="headerlink" title="4.3.4 创建 Bucket 表"></a>4.3.4 创建 Bucket 表</h3><p>分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive可以进一步组织成桶，也就是更为细粒度的数据范围划分。分桶是将数据集分解成更容易管理的若干部分的另一个技术。分区针对的是数据的存储路径；分桶针对的是数据文件。下面来看一个分桶表的例子。<br>首先来构造测试数据，数据分为两列，一列是id，一列是name（根据图片自行创建）。如下所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803437945.png" alt="img"><br>创建分桶表，这里需要注意的是，在创建分桶表之前，需要设置强制分桶属性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set hive.enforce.bucketing=true;</span><br></pre></td></tr></table></figure>

<p>在建表之前需要注意的是，因为分桶表加载数据必须得走MapReduce，所以使用load的方式加载数据是不会进行分桶的，故得需要使用insert的方式加载其他表中的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table stu(id int, name string)</span><br><span class="line">    &gt; row format delimited</span><br><span class="line">    &gt; fields terminated by &#x27;,&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803449671.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table students(id int,name string)</span><br><span class="line">    &gt; clustered by(id) into 4 buckets</span><br><span class="line">    &gt; row format delimited</span><br><span class="line">    &gt; fields terminated by &#x27;,&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803456933.png" alt="img"><br>使用load的方式将构造的数据加载到stu普通表中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath &#x27;/root/student.txt&#x27; into table stu;</span><br></pre></td></tr></table></figure>

<p>使用insert方式将stu中的数据加载到students中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert into table students select id,name from stu;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803464480.png" alt="img"><br>打开浏览器，在地址栏中输入，这里的ip地址根据实际环境编写<a target="_blank" rel="noopener" href="http://10.30.92.3:50070/%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%88%90%E5%90%8E%E6%9F%A5%E7%9C%8B%E7%9B%91%E6%8E%A7%E7%95%8C%E9%9D%A2%EF%BC%8C%E5%8F%91%E7%8E%B0%E5%B7%B2%E7%BB%8F%E5%B0%86%E6%95%B0%E6%8D%AE%E5%88%86%E6%88%90%E4%BA%86%E5%9B%9B%E4%B8%AA%E6%A1%B6">http://10.30.92.3:50070/加载数据完成后查看监控界面，发现已经将数据分成了四个桶</a></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803469924.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803474695.png" alt="img"></p>
<p>对于大数据量的数据集，有时候我们需要对其进行抽样查询，此时可以使用桶表来完成这个需求操作。其语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TABLESAMPLE(BUCKET x OUT OF y)</span><br></pre></td></tr></table></figure>

<p>其中，y必须是table总bucket数的倍数或者因子。Hive根据y的大小，决定抽样的比例。x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of  2)，表示总共抽取（4&#x2F;2&#x3D;）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。这里必须注意的是x的值必须小于等于y的值。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from students tablesample(bucket 1 out of 4 on id);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803482164.png" alt="img"></p>
<h3 id="4-4修改表结构"><a href="#4-4修改表结构" class="headerlink" title="4.4修改表结构"></a>4.4修改表结构</h3><p>（1）重命名表<br>（2）增加、删除分区<br>（3）增加、更新列<br>（4）修改列的名字、类型、位置、注释<br>（5）增加表的元数据信息<br>复制一个空表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE empty LIKE cstor2;</span><br></pre></td></tr></table></figure>

<p>重命名表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP TABLE [IF EXISTS] table_name [RESTRICT|CASCAD];</span><br></pre></td></tr></table></figure>

<p>增加分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION &#x27;location1&#x27; ] partition_spec [ LOCATION &#x27;location2&#x27; ] ...</span><br><span class="line">partition_spec:</span><br><span class="line">: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)</span><br></pre></td></tr></table></figure>

<p>删除</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name DROP</span><br><span class="line">partition_spec, partition_spec,...</span><br></pre></td></tr></table></figure>

<p>增加、更新列<br>ADD 是代表新增一字段，字段位置在所有列后面(partition列前)<br>REPLACE 则是表示替换表中所有字段。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)</span><br></pre></td></tr></table></figure>

<p>修改列的名字、类型、位置、注释<br>这个命令可以允许改变列名、数据类型、注释、列位置或者它们的任意组合</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name CHANGE [COLUMN]</span><br><span class="line">col_old_name col_new_name column_type</span><br><span class="line">[COMMENT col_comment]</span><br><span class="line">[FIRST|AFTER column_name]</span><br></pre></td></tr></table></figure>

<p>增加表的元数据信息<br>用户可以用这个命令向表中增加元数据信息 metadata</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name SET TBLPROPERTIES table_properties table_properties:</span><br><span class="line">: (property_name = property_value, ...)</span><br></pre></td></tr></table></figure>

<p>改变文件格式和组织</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE table_name SET FILEFORMAT file_format</span><br><span class="line">ALTER TABLE table_name CLUSTERED BY(col_name, col_name, ...)</span><br><span class="line">[SORTED BY(col_name, ...)] INTO num_buckets BUCKETS</span><br></pre></td></tr></table></figure>

<p>创建视图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CREATE VIEW [IF NOT EXISTS] view_name [ (column_name [COMMENT column_comment], ...) ][COMMENT view_comment][TBLPROPERTIES (property_name = property_value, ...)]</span><br><span class="line">AS SELECT ...</span><br></pre></td></tr></table></figure>

<p>删除视图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP VIEW view_name</span><br></pre></td></tr></table></figure>

<p>创建、删除函数<br>创建函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TEMPORARY FUNCTION function_name AS class_name</span><br></pre></td></tr></table></figure>

<p>删除函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP TEMPORARY FUNCTION function_name</span><br></pre></td></tr></table></figure>

<p>展示、描述语句<br>显示 表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show tables;</span><br></pre></td></tr></table></figure>

<p>显示 数据库</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show databases;</span><br></pre></td></tr></table></figure>

<p>显示 函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show functions;</span><br></pre></td></tr></table></figure>

<p>描述 表&#x2F;列</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe [EXTENDED] table_name[DOT col_name]</span><br></pre></td></tr></table></figure>

<p>删除数据库<br>删除为空的数据库</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database db_hive;</span><br></pre></td></tr></table></figure>

<p>如果删除的数据库不存在，最好采用if exists 判断数据库是否存在</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database if exists db_hive;</span><br></pre></td></tr></table></figure>

<p>如果数据库中有表存在，这里需要使用cascade强制删除数据库</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; drop database if exists db_hive cascade;</span><br></pre></td></tr></table></figure>

<h3 id="4-5算术运算符"><a href="#4-5算术运算符" class="headerlink" title="4.5算术运算符"></a>4.5算术运算符</h3><p>可以通过算术运算符对查询的内容进行基础的操作，常用的算术运算符如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">运算符						描述</span><br><span class="line">A+B						A和B相加</span><br><span class="line">A-B						A减去B</span><br><span class="line">A*B						A和B相乘</span><br><span class="line">A/B						A除以B</span><br><span class="line">A%B						A对B取余</span><br><span class="line">A&amp;B						A和B按位取与</span><br><span class="line">A|B						A和B按位取或</span><br><span class="line">A^B						A和B按位取异或</span><br><span class="line">~A						A按位取反</span><br></pre></td></tr></table></figure>

<h3 id="4-6常用的基础函数"><a href="#4-6常用的基础函数" class="headerlink" title="4.6常用的基础函数"></a>4.6常用的基础函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">函数						描述</span><br><span class="line">count()						求记录数</span><br><span class="line">max()						求最大值</span><br><span class="line">min()						求最小值</span><br><span class="line">avg()						求平均值</span><br><span class="line">sum()						求和</span><br></pre></td></tr></table></figure>

<h3 id="4-7常用语法"><a href="#4-7常用语法" class="headerlink" title="4.7常用语法"></a>4.7常用语法</h3><h3 id="4-7-1LIMIT语法"><a href="#4-7-1LIMIT语法" class="headerlink" title="4.7.1LIMIT语法"></a>4.7.1LIMIT语法</h3><p>可以使用LIMIT语句限制查询的条数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select name n, sex s from people limit 3;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803504627.png" alt="img"></p>
<h3 id="4-7-2WHERE语句"><a href="#4-7-2WHERE语句" class="headerlink" title="4.7.2WHERE语句"></a>4.7.2WHERE语句</h3><p>使用WHERE语句可以过滤不满足条件的数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select name n, sex s from people where sex = &#x27;m&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803518238.png" alt="img"></p>
<h3 id="4-7-3GROUP-BY-语句"><a href="#4-7-3GROUP-BY-语句" class="headerlink" title="4.7.3GROUP BY 语句"></a>4.7.3GROUP BY 语句</h3><p> GROUP BY语句通常会跟聚合函数一起使用，下面的例子统计了people表中所有不同性别的总人数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select sex,count(*)from people group by sex;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803524176.png" alt="img"></p>
<h3 id="4-7-4HANING子句"><a href="#4-7-4HANING子句" class="headerlink" title="4.7.4HANING子句"></a>4.7.4HANING子句</h3><p>HAVING子句只能用于GROUP BY子句之后，且与WHERE不同的是，HAVING后面可以使用分组函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sex,count(*) sl from people group by sex having sex = &#x27;m&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803529960.png" alt="img"></p>
<h3 id="4-7-5JOIN语句"><a href="#4-7-5JOIN语句" class="headerlink" title="4.7.5JOIN语句"></a>4.7.5JOIN语句</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table if not exists favorite(id string,name string,fav string)</span><br><span class="line">      row format delimited </span><br><span class="line">      fields terminated by &#x27;,&#x27;</span><br><span class="line">      stored as textfile;</span><br><span class="line"></span><br><span class="line">hive&gt; load data local inpath ‘/root/fav.txt’ into table favorite;</span><br><span class="line"></span><br><span class="line">hive&gt; select * from favorite;</span><br></pre></td></tr></table></figure>

<p>JOIN语句代表多表关联，这里需要注意的是它只支持等值连接，不支持非等值连接。那我们再建一张favorite表的，表示用户的兴趣爱好。favorite表有三个字段，分别是id、姓名和兴趣。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803552777.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803546730.png" alt="img"><br>下面是一个JOIN的例子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select p.name,p.sex,fav from people p join favorite f on p.name = f.name;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803568725.png" alt="img"><br>上例就是一个内连接的例子，内连接是指只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。左外连接（LEFT JOIN）是指JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。右外连接（RIGHT  JOIN）是指JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。满外连接（FULL  JOIN)将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。Hive会对每对JOIN连接对象启动一个MapReduce任务。</p>
<h3 id="4-8排序语句"><a href="#4-8排序语句" class="headerlink" title="4.8排序语句"></a>4.8排序语句</h3><h3 id="4-8-1Order-by语句"><a href="#4-8-1Order-by语句" class="headerlink" title="4.8.1Order by语句"></a>4.8.1Order by语句</h3><p> 使用Order by进行全局排序，它只有一个Reducer。默认是升序排序，即ASC（ascend）。也可以使用DESC（descend）指定为降序排列。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from people order by sex desc;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803582378.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from people order by sex,id desc;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803589754.png" alt="img"></p>
<h3 id="4-8-2-Distribute-By语句"><a href="#4-8-2-Distribute-By语句" class="headerlink" title="4.8.2 Distribute By语句"></a>4.8.2 Distribute By语句</h3><p> 在有些情况下，我们需要控制某个特定行应该到哪个reducer，通常是为了进行后续的聚集操作。分区排序（Distribute  By）可以用来处理这种情况。分区排序类似MapReduce中的自定义分区，它结合sort by使用。对于distribute  by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。distribute  by的分区规则是根据分区字段的hash码与reduce的个数进行模除后，余数相同的分到一个区。来看个例子：先按照性别分区，然后按照id降序输出。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; set mapreduce.job.reduces=3;</span><br><span class="line">hive&gt; select * from people distribute by sex sort by id desc;</span><br></pre></td></tr></table></figure>

<p><img src="http://10.131.2.101/static/upload/resource/exp/ins/17bea027cb244429a6701ac60804d065/image/image.1681803596673.png" alt="img"></p>
<h3 id="4-8-3-Cluster-By语句"><a href="#4-8-3-Cluster-By语句" class="headerlink" title="4.8.3 Cluster By语句"></a>4.8.3 Cluster By语句</h3><p> 当distribute by和sorts by字段相同时，可以使用cluster by方式。cluster  by除了具有distribute by的功能外还兼具sort  by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。例如，以下两种写法是等价的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;  select * from people cluster by sex;</span><br><span class="line">hive&gt; select * from people distribute by sex sort by sex;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803603851.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803615175.png" alt="img"></p>
<h2 id="12-Hive实验：beeline入门"><a href="#12-Hive实验：beeline入门" class="headerlink" title="12.Hive实验：beeline入门"></a>12.Hive实验：beeline入门</h2><blockquote>
<h3 id="目的-11"><a href="#目的-11" class="headerlink" title="目的"></a>目的</h3><p>1.学习beeline</p>
<h3 id="要求-11"><a href="#要求-11" class="headerlink" title="要求"></a>要求</h3><p>本实验要求学生：<br>了解beeline和HiveServer2</p>
<h3 id="原理-11"><a href="#原理-11" class="headerlink" title="原理"></a>原理</h3><p>Beeline，它其实是HiveServer2的JDBC客户端，基于SQLLine命令行接口。Beeline Shell可以工作在嵌入式模式和远程模式，在嵌入式模式中，它运行一个嵌入式的Hive（类似于Hive  CLI），在远程模式中，通过Thrift连接到一个单独的HiveServer2进程，从Hive  0.14开始，当Beeline和HiveServer2一起使用时，它会从HiveServer2打印执行查询的日志信息到STDERR。建议在生产环境使用远程HiveServer2模式，因为这样更安全，不需要为用户授予直接的HDFS&#x2F;Metastore访问权限。</p>
<p>HiveServer是一种可选服务，允许远程客户端可以使用各种编程语言向Hive提交请求并检索结果。HiveServer是建立在Apache ThriftTM（<a target="_blank" rel="noopener" href="http://thrift.apache.org/%EF%BC%89">http://thrift.apache.org/）</a> 之上的，因此有时会被称为Thrift Server，这可能会导致混乱，因为新服务HiveServer2也是建立在Thrift之上的．自从引入HiveServer2后，HiveServer也被称为HiveServer1。</p>
<p>HiveServer2(HS2)是一种能使客户端执行Hive查询的服务。  HiveServer2是HiveServer1的改进版，HiveServer1已经被废弃。HiveServer2可以支持多客户端并发和身份认证。旨在为开放API客户端（如JDBC和ODBC）提供更好的支持。<br>HiveServer2单进程运行，提供组合服务，包括基于Thrift的Hive服务（TCP或HTTP）和用于Web UI的Jetty Web服务器。</p>
<p>架构<br>基于Thrift的Hive服务是HiveServer2的核心，负责维护Hive查询（例如，从Beeline）。Thrift是构建跨平台服务的RPC框架。其堆栈由4层组成：server，Transport，Protocol和处理器。可以在 <a target="_blank" rel="noopener" href="https://thrift.apache.org/docs/concepts">https://thrift.apache.org/docs/concepts</a> 找到有关分层的更多详细信息。</p>
</blockquote>
<h3 id="4-1启动hive-1"><a href="#4-1启动hive-1" class="headerlink" title="4.1启动hive"></a>4.1启动hive</h3><p>需要重新启动 hdfs，以及启动 mysql。<br>启动之前先进&#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;etc&#x2F;hadoop&#x2F;配置root用户访问hadoop</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/etc/hadoop/</span><br><span class="line"># vim  core-site.xml</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804129639.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804136196.png" alt="img"><br>然后启动 Hadoop 集群：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/sbin/</span><br><span class="line"># ./start-all.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804143891.png" alt="img"><br>再进&#x2F;usr&#x2F;cstor&#x2F;hive&#x2F;conf配置hive.server2的端口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hive/conf</span><br><span class="line"># vim hive-site.xml</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804150053.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&#x27;1.0&#x27;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&#x27;text/xsl&#x27; href=&#x27;configuration.xsl&#x27;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;master&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804156191.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hive</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804163059.png" alt="img"></p>
<h3 id="4-2创建数据库"><a href="#4-2创建数据库" class="headerlink" title="4.2创建数据库"></a>4.2创建数据库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; create database db_hive;</span><br><span class="line">或者</span><br><span class="line">hive &gt; create database if not exists db_hive;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804170028.png" alt="img"><br>显示数据库</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show databases;</span><br><span class="line">hive&gt; show databases like &#x27;db_hive*&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804175406.png" alt="img"></p>
<p>查看数据库详情信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc database db_hive;</span><br><span class="line">或者</span><br><span class="line">hive&gt; desc database extended db_hive;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804180256.png" alt="img"><br>切换当前数据库</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive &gt; use db_hive;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804186819.png" alt="img"></p>
<h3 id="4-3创建表"><a href="#4-3创建表" class="headerlink" title="4.3创建表"></a>4.3创建表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE TABLE cstor ( id int, name String, age Int, address String)</span><br><span class="line">ROW FORMAT DELIMITED</span><br><span class="line">FIELDS TERMINATED BY ‘\t’</span><br><span class="line">LINES TERMINATED BY ‘\n’</span><br><span class="line">STORED AS TEXTFILE ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804193080.png" alt="img"></p>
<h3 id="4-3-1导入数据前的准备"><a href="#4-3-1导入数据前的准备" class="headerlink" title="4.3.1导入数据前的准备"></a>4.3.1导入数据前的准备</h3><p>在桌面上新建一个txt文件，通过winSCP上传至“&#x2F;usr&#x2F;cstor”目录下<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804201017.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804206809.png" alt="img"></p>
<h3 id="4-3-2导入数据"><a href="#4-3-2导入数据" class="headerlink" title="4.3.2导入数据"></a>4.3.2导入数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; load data local inpath ‘/usr/cstor/cstor.txt’ overwrite into table cstor;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804212798.png" alt="img"></p>
<h3 id="4-3-3查询数据"><a href="#4-3-3查询数据" class="headerlink" title="4.3.3查询数据"></a>4.3.3查询数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from cstor;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804218609.png" alt="img"></p>
<h3 id="4-4启动hiveserver2"><a href="#4-4启动hiveserver2" class="headerlink" title="4.4启动hiveserver2"></a>4.4启动hiveserver2</h3><p>启动hiveserver2,先将hive终止</p>
<p>beeline 初解<br>有计算机网络基础的同学或许会有些灵感，当我们需要远程访问一个数据库时，我们通常需要一个套接字以及相应的服务来连接两台机器。<br>对于 hive 而言，它本质是借助 hdfs 实现的一个数据仓库。我们在远端能通过 jdbc 协议（基于 TCP 协议）访问我们的 hdfs 集群，相应的套接字服务就是 hive 服务列表中的 hiveserver2，默认的端口是 10000。<br>所以，我们如果想使用 beeline 访问 hive 时，首先应该开启 hiveserver2 这个服务。</p>
<h3 id="4-4-1启动hiveserver2"><a href="#4-4-1启动hiveserver2" class="headerlink" title="4.4.1启动hiveserver2"></a>4.4.1启动hiveserver2</h3><p>开启 hiveserver2，并将其运行在后台：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804226263.png" alt="img"><br>我们 jps 一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># Jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804235367.png" alt="img"><br>如图所示，新增了一个 runjar 进程，说明我们的 hiveserver2 服务已经成功运行在后台。<br>现在我们查看一下 10000 端口：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># netstat -anop | grep 10000</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804241699.png" alt="img"></p>
<h3 id="4-4-2连接数据库"><a href="#4-4-2连接数据库" class="headerlink" title="4.4.2连接数据库"></a>4.4.2连接数据库</h3><p> 接下来，我们使用 beeline连接数据库。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># beeline</span><br><span class="line"># !connect jdbc:hive2://master:10000/db_hive</span><br></pre></td></tr></table></figure>

<p>提示我们输入用户，我们在这里输入root，输入密码就是默认的，直接回车。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804249963.png" alt="img"></p>
<h3 id="4-4-3查询数据库"><a href="#4-4-3查询数据库" class="headerlink" title="4.4.3查询数据库"></a>4.4.3查询数据库</h3><p> 用以下命令查看我们是否连接成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># show databases;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804258204.png" alt="img"></p>
<h3 id="4-5使用java-api查询数据"><a href="#4-5使用java-api查询数据" class="headerlink" title="4.5使用java api查询数据"></a>4.5使用java api查询数据</h3><p> 使用 Java 语言调用Hive 相关 API，获取集群中的 cstor表中数据。<br>启动 Eclipse，创建一个 java 项目，项目名为learnHive：<br>打开Eclipse,点击File-&gt;New-&gt;Project-&gt;Java Project</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804293389.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804298618.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804305029.png" alt="img"></p>
<p>在项目中创建一个 lib 文件夹，用于存储 hive 相关的 jar 包。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804313003.png" alt="img"></p>
<p>将&#x2F;usr&#x2F;cstor&#x2F;hive&#x2F;lib下面的 jar 包（扩展名为 jar  的文件）和&#x2F;usr&#x2F;cstor&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;common下面的hadoop-common-2.7.3.jar通过WinSCP传至windows桌面上，再拷贝到上一步创建的 lib 包中。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804318890.png" alt="img"></p>
<p>将 lib 包中的 jar 包添加到类路径下</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804359488.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804366021.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804372442.png" alt="img"></p>
<p>在 src 中新建 package，名为 cn.cstor.hive。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804386724.png" alt="img"><br>在 cn.cstor.hive 中新建 java 类：hive，在类的 main方法中编写如下代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.cstor.hive;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.Statement;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">hive</span> &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="type">String</span> <span class="variable">diverName</span> <span class="operator">=</span> <span class="string">&#x27;org.apache.hive.jdbc.HiveDriver&#x27;</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">Class.forName(diverName);</span><br><span class="line"><span class="type">Connection</span> <span class="variable">con</span> <span class="operator">=</span> DriverManager.getConnection(<span class="string">&#x27;jdbc:hive2://（主节点ip）:10000/db_hive&#x27;</span>,<span class="string">&#x27;root&#x27;</span>,<span class="string">&#x27;&#x27;</span>); </span><br><span class="line"><span class="type">Statement</span> <span class="variable">stmt</span> <span class="operator">=</span> con.createStatement();</span><br><span class="line"><span class="type">ResultSet</span> <span class="variable">rs</span> <span class="operator">=</span> stmt.executeQuery(<span class="string">&#x27;select * from cstor&#x27;</span>);</span><br><span class="line"><span class="keyword">while</span>(rs.next())&#123;</span><br><span class="line">System.out.print(rs.getInt(<span class="string">&#x27;id&#x27;</span>)+<span class="string">&#x27;  &#x27;</span>+rs.getString(<span class="string">&#x27;name&#x27;</span>)+<span class="string">&#x27;  &#x27;</span>+rs.getInt(<span class="string">&#x27;age&#x27;</span>)+<span class="string">&#x27;  &#x27;</span>+rs.getString(<span class="string">&#x27;address&#x27;</span>) );</span><br><span class="line"> System.out.println();</span><br><span class="line">&#125;</span><br><span class="line">System.out.println(<span class="string">&#x27;查询成功&#x27;</span>);</span><br><span class="line">con.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击运行查看结果：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804396200.png" alt="img"></p>
<p>退出当前连接的数据库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># !close</span><br></pre></td></tr></table></figure>

<p>如图所示，我们已经成功退出连接的 db_hive 数据库。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804404415.png" alt="img"></p>
<h2 id="13-Hive挑战：Hive实现单词统计"><a href="#13-Hive挑战：Hive实现单词统计" class="headerlink" title="13.Hive挑战：Hive实现单词统计"></a>13.Hive挑战：Hive实现单词统计</h2><blockquote>
<h3 id="目的-12"><a href="#目的-12" class="headerlink" title="目的"></a>目的</h3><p>1.学会使用split()、explode的一些基础用法，熟练使用HQL的基础语言</p>
<h3 id="要求-12"><a href="#要求-12" class="headerlink" title="要求"></a>要求</h3><p>1.要求实验结束后可以熟练使用HQL语句满足自身需求</p>
<h3 id="原理-12"><a href="#原理-12" class="headerlink" title="原理"></a>原理</h3><p>Hive中提供了类似于SQL语言的查询语言——HiveQL，可以通过 HiveQL语句快速实现简单的 MapReduce统计， Hive 自身可以将 HiveQL 语句快速转换成 MapReduce  任务进行运行，而不必开发专门的 MapReduce 应用程序。</p>
</blockquote>
<h3 id="4-1准备数据-1"><a href="#4-1准备数据-1" class="headerlink" title="4.1准备数据"></a>4.1准备数据</h3><p>首先，新打开一个终端，进入cstor目录<br>然后我们在这个目录下新建一个 word.txt 文档，作为我们的数据文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim /usr/cstor/word.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353373136.png" alt="img"></p>
<p>保存退出之后，我们重新回到 hive 命令行中，我们创建一个 text 表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table text（line string）;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353406033.png" alt="img"></p>
<p>将数据加载到该表中:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#x27;/usr/cstor/word.txt&#x27; into table text;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353426871.png" alt="img"></p>
<p>注意：使用 load 函数时，如果从本地加载到 hive 表中，需要使用关键字 local，它会将文件同时上传到 hdfs 系统中。<br>查看 text 表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from text;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353472143.png" alt="img"></p>
<h3 id="4-2单词统计"><a href="#4-2单词统计" class="headerlink" title="4.2单词统计"></a>4.2单词统计</h3><p>由于一行文本有多个单词，所以我们需要将每行的文本切割成单个的单词，这里我们需要使用 split 函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select split(line,&#x27; &#x27;) from text;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353543177.png" alt="img"></p>
<p>如图所示，每行文本已经被切割开来，得到的是数组类型，并不是 hive 能直接通过 group by 处理的形式，所以我们需要使用 hive 的另一个高级函数 explode。<br>explode 这个函数的功能是行转列（俗称炸裂），也就是说将上面我们得到的数组中的每个元素生成一行。<br>命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select explode(split(line,&#x27; &#x27;)) from text;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353574274.png" alt="img"></p>
<p>对于我们炸裂出来的数据，原来的列的名称已经不再适用，我们将其取别名为 word：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select explode(split(line,&#x27; &#x27;))as word from text;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353588473.png" alt="img"></p>
<p>接下来，我们就需要使用 group by 来对我们得到的炸裂开来的数据进行统计。<br>这里需要注意的是，我们需要将上面得到的结果作为另一张表 t(子查询)，然后对这张表进行统计，否则将会报错。<br>命令如下（注意：这个过程需要的时间会比较长，请耐心等待）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select t.word,count(*) from (select explode(split(line,&#x27; &#x27;))as word from text) as t group by t.word;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353613111.png" alt="img"></p>
<p>如图所示，我们已经将每个单词的个数统计出来，但是看起来总不怎么顺眼，接下来我们将所有单词按照降序排列，同时只打印前三个单词（注意：这个过程需要的时间会比较长，请耐心等待）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select t.word,count(*) c from (select explode(split(line,&#x27; &#x27;))as word from text) as t group by t.word order by c desc limit 3;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353632800.png" alt="img"></p>
<p>order by 默认升序，我们使用 desc 设置为降序，同时按 count（*） 出来的结果作为排序的策略。<br>在实际生产环境中，我们都需要将我们查询得到的结果，存入另一张表中，以供其他人使用。<br>所以本节最后一个操作就是将我们的结果存入另一张表中。<br>命令如下（注意：这个过程需要的时间会比较长，请耐心等待）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table wordcount as select t.word,count(*) c from (select explode(split(line,&#x27; &#x27;))as word from text) as t group by t.word order by c desc limit 3;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353684927.png" alt="img"></p>
<p>查看 wordcount 表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from wordcount;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353705461.png" alt="img"></p>
<h2 id="14-Hive实验：自定义函数和窗口函数"><a href="#14-Hive实验：自定义函数和窗口函数" class="headerlink" title="14.Hive实验：自定义函数和窗口函数"></a>14.Hive实验：自定义函数和窗口函数</h2><blockquote>
<h3 id="目的-13"><a href="#目的-13" class="headerlink" title="目的"></a>目的</h3><p>1.了解Hive中基础内置函数以及如何自定义函数<br>2.了解Hive中的窗口函数</p>
<h3 id="要求-13"><a href="#要求-13" class="headerlink" title="要求"></a>要求</h3><p>1.熟练使用Hive中内置函数及窗口寒素	<br>2.要求实验结束后，可以熟练的根据自身需求自定义函数</p>
<h3 id="原理-13"><a href="#原理-13" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1-内置函数"><a href="#3-1-内置函数" class="headerlink" title="3.1 内置函数"></a>3.1 内置函数</h2><p>Hive中的内置函数，比较简单，虽然有很多内置的函数，但是生产上肯定不够那么全面，所以用户需要自定义函数来满足自身的需求</p>
<h3 id="3-1-1自定义函数分类"><a href="#3-1-1自定义函数分类" class="headerlink" title="3.1.1自定义函数分类"></a>3.1.1自定义函数分类</h3><p>UDF(User-Defined-Function)用户自定义函数，输入一个数据然后产生一个数据；<br>UDAF(User-Defined Aggregation Function)用户自定义聚合函数，多个输入数据然后产生一个输出参数；<br>UDTF(User-Defined Table-generating Function)用户自定义表生成函数，输入一行数据生成N行数据</p>
<h3 id="3-1-2-自定义函数流程"><a href="#3-1-2-自定义函数流程" class="headerlink" title="3.1.2 自定义函数流程"></a>3.1.2 自定义函数流程</h3><p>要想在Hive中完成自定义函数的操作，要按照如下的流程进行操作：<br>1、自定义Java类并继承org.apache.hadoop.hive.ql.exec.UDF；<br>2、覆写evaluate函数，evaluate函数支持重载；<br>3、把程序打包放到hive所在服务器；<br>4、进入hive客户端，添加jar包；<br>5、创建关联到Java类的Hive函数；<br>6、Hive命令行中执行查询语句：select id, 方法名(name) from 表名——得出自定义函数输出的结果。</p>
<h2 id="3-2-窗口函数"><a href="#3-2-窗口函数" class="headerlink" title="3.2 窗口函数"></a>3.2 窗口函数</h2><p>窗口函数指定了函数工作的数据窗口大小(当前行的上下多少行)，这个数据窗口大小可能会随着行的变化而变化，相比较于聚合函数，窗口函数对于每个组返回多行，组内每一行对应返回一行值而聚合函数对于每个组只返回一行。在日常的开发中常用的窗口函数有sum() over() 、count() over()、排名函数</p>
<h3 id="3-2-1-Hive的窗口函数功能"><a href="#3-2-1-Hive的窗口函数功能" class="headerlink" title="3.2.1 Hive的窗口函数功能"></a>3.2.1 Hive的窗口函数功能</h3><p>sum(col) over() : 分组对col累计求和<br>count(col) over() : 分组对col累计<br>min(col) over() : 分组对col求最小值<br>max(col) over() : 分组求col的最大值<br>avg(col) over() : 分组求col列的平均值<br>first_value(col) over() : 某分组排序后的第一个col值<br>last_value(col) over() : 某分组排序后的最后一个col值<br>lag(col,n,DEFAULT) : 统计往前n行的col值，n可选，默认为1，DEFAULT当往上第n行为NULL时候，取默认值，如不指定，则为NULL<br>lead(col,n,DEFAULT) : 统计往后n行的col值，n可选，默认为1，DEFAULT当往下第n行为NULL时候，取默认值，如不指定，则为NULL<br>ntile(n) : 用于将分组数据按照顺序切分成n片，返回当前切片值。注意：n必须为int类型<br>row_number() over() : 排名函数，不会重复，适合于生成主键或者不并列排名<br>rank() over() : 排名函数，有并列名次，名次不连续。如:1，1，3<br>dense_rank() over() : 排名函数，有并列名次，名次连续。如：1，1，2</p>
</blockquote>
<h3 id="4-1Hive的内置函数"><a href="#4-1Hive的内置函数" class="headerlink" title="4.1Hive的内置函数"></a>4.1Hive的内置函数</h3><p>在Hive中给我们内置了很多函数官方地址<br>可以在启动hive后输入命令查看函数：<br>查看所有的内置函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show functions;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803841319.png" alt="img"><br>查看函数的具体语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; DESCRIBE FUNCTION case;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803847250.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; DESCRIBE FUNCTION EXTENDED case;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803853808.png" alt="img"></p>
<h3 id="4-2-Hive自定义函数"><a href="#4-2-Hive自定义函数" class="headerlink" title="4.2 Hive自定义函数"></a>4.2 Hive自定义函数</h3><h3 id="4-2-1使用eclipse打jar包"><a href="#4-2-1使用eclipse打jar包" class="headerlink" title="4.2.1使用eclipse打jar包"></a>4.2.1使用eclipse打jar包</h3><p>Hive中的内置函数，比较简单，用到的时候可以在官网进行查询，虽然有很多内置的函数，但是生产上肯定不够那么全面，所有，用户需要自定义函数来满足自身的求。<br>准备数据：<br>在hive中建一张名为temp的表，内容如下，具体建表步骤参考之前课程：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803863263.png" alt="img"></p>
<p>(1)使用eclipse创建一个项目</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803869673.png" alt="img"></p>
<p>(2)继承UDF类,重写evaluate方法</p>
<p>创建一个HelloUdf类继承UDF，并且重写evaluate方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.UTFDataFormatException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HelloUdf</span> <span class="keyword">extends</span> <span class="title class_">UDF</span> &#123;</span><br><span class="line"> <span class="keyword">public</span> Text <span class="title function_">evaluate</span> <span class="params">(<span class="keyword">final</span> Text t)</span> &#123;</span><br><span class="line">     <span class="keyword">if</span> (t == <span class="literal">null</span>) <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;Hello:&quot;</span> + t);</span><br><span class="line"> &#125;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String [] args)</span>&#123;</span><br><span class="line">       System.out.println(<span class="keyword">new</span> <span class="title class_">HelloUdf</span>().evaluate(<span class="keyword">new</span> <span class="title class_">Text</span>(<span class="string">&quot;hero&quot;</span>)));</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803876805.png" alt="img"><br>(3)打jar包<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803882008.png" alt="img"><br>通过WinSCP上传jar包至平台&#x2F;usr&#x2F;cstor&#x2F;目录下</p>
<h3 id="4-2-2创建自定义函数有两种格式临时函数，永久函数："><a href="#4-2-2创建自定义函数有两种格式临时函数，永久函数：" class="headerlink" title="4.2.2创建自定义函数有两种格式临时函数，永久函数："></a>4.2.2创建自定义函数有两种格式临时函数，永久函数：</h3><p>(1)进入hive</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add jar /usr/cstor/UDF.jar;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803888239.png" alt="img"></p>
<p>(2)临时自定义函数（只对当前session有效）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create temporary function say_hello as &#x27;DEMO.HelloUDF &#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803893546.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select name,say_hello(name) from temp;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803899904.png" alt="img"><br>查看jar包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list jar;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803906089.png" alt="img"><br>查看自定义函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show functions;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803911410.png" alt="img"><br>(3)永久函数<br>把jar包上传到hdfs上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -put /usr/cstor/UDF.jar /</span><br></pre></td></tr></table></figure>

<p>创建永久函数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create function say_hello1 as &#x27;DEMO.HelloUDF&#x27; using jar &#x27;hdfs:///UDF.jar&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803920398.png" alt="img"><br>测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select name,say_hello1(name) from temp;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803926826.png" alt="img"></p>
<h3 id="4-3Hive-窗口函数"><a href="#4-3Hive-窗口函数" class="headerlink" title="4.3Hive 窗口函数"></a>4.3Hive 窗口函数</h3><p>准备数据（创建测试表，存放当天每半小时的店铺销售数据）</p>
<h3 id="4-3-1-Hive中创建表"><a href="#4-3-1-Hive中创建表" class="headerlink" title="4.3.1.Hive中创建表"></a>4.3.1.Hive中创建表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE IF NOT EXISTS shop_data(</span><br><span class="line">    shop_id INT comment &#x27;店铺id&#x27;, </span><br><span class="line">    stat_date STRING comment &#x27;时间&#x27;, </span><br><span class="line">    ordamt DOUBLE comment &#x27;销售额&#x27;</span><br><span class="line">)</span><br><span class="line">ROW FORMAT DELIMITED </span><br><span class="line">FIELDS TERMINATED BY &#x27;\t&#x27; </span><br><span class="line">LINES TERMINATED BY &#x27;\n&#x27;</span><br><span class="line">STORED AS TEXTFILE;</span><br></pre></td></tr></table></figure>

<p>插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">insert into shop_data values </span><br><span class="line">(10026,&#x27;201901230030&#x27;,5170),</span><br><span class="line">(10026,&#x27;201901230100&#x27;,5669),</span><br><span class="line">(10026,&#x27;201901230130&#x27;,2396),</span><br><span class="line">(10026,&#x27;201901230200&#x27;,1498),</span><br><span class="line">(10026,&#x27;201901230230&#x27;,1997),</span><br><span class="line">(10026,&#x27;201901230300&#x27;,1188),</span><br><span class="line">(10026,&#x27;201901230330&#x27;,598),</span><br><span class="line">(10026,&#x27;201901230400&#x27;,479),</span><br><span class="line">(10026,&#x27;201901230430&#x27;,1587),</span><br><span class="line">(10026,&#x27;201901230530&#x27;,799),</span><br><span class="line">(10027,&#x27;201901230030&#x27;,2170),</span><br><span class="line">(10027,&#x27;201901230100&#x27;,1623),</span><br><span class="line">(10027,&#x27;201901230130&#x27;,3397),</span><br><span class="line">(10027,&#x27;201901230200&#x27;,1434),</span><br><span class="line">(10027,&#x27;201901230230&#x27;,1001),</span><br><span class="line">(10028,&#x27;201901230300&#x27;,1687),</span><br><span class="line">(10028,&#x27;201901230330&#x27;,1298),</span><br><span class="line">(10028,&#x27;201901230400&#x27;,149),</span><br><span class="line">(10029,&#x27;201901230430&#x27;,2587),</span><br><span class="line">(10029,&#x27;201901230530&#x27;,589);</span><br></pre></td></tr></table></figure>

<h3 id="4-3-2-窗口聚合函数"><a href="#4-3-2-窗口聚合函数" class="headerlink" title="4.3.2.窗口聚合函数"></a>4.3.2.窗口聚合函数</h3><h4 id="count开窗函数"><a href="#count开窗函数" class="headerlink" title="count开窗函数"></a>count开窗函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">select shop_id,stat_date,ordamt,</span><br><span class="line">-- 以符合条件的所有行作为窗口</span><br><span class="line">count(shop_id) over() as count1,</span><br><span class="line"> -- 以按shop_id分组的所有行作为窗口</span><br><span class="line">count(shop_id) over(partition by shop_id) as count2,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序的所有行作为窗口</span><br><span class="line">count(shop_id) over(partition by shop_id order by stat_date) as count3,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按当前行+往前1行+往后2行的行作为窗口</span><br><span class="line">count(ordamt) over(partition by shop_id order by stat_date rows between 1 preceding and 2 following) as count4,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从起点到末尾,默认从起点到末尾和count2结果相同</span><br><span class="line">count(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following) as count5,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从起点到当前行的前一行</span><br><span class="line">count(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and 1 preceding) as count6,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从起点到当前行</span><br><span class="line">count(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and current row) as count7,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从当前行到末尾</span><br><span class="line">count(ordamt) over(partition by shop_id order by stat_date rows between current row and unbounded following) as count8,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从当前行往后一行到末尾</span><br><span class="line">count(ordamt) over(partition by shop_id order by stat_date rows between 1 following and unbounded following) as count9,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从当前行往后一行到当前行往后2行</span><br><span class="line">count(ordamt) over(partition by shop_id order by stat_date rows between 1 following and 2 following) as count10</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803957867.png" alt="img"></p>
<h4 id="sum开窗函数"><a href="#sum开窗函数" class="headerlink" title="sum开窗函数"></a>sum开窗函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、统计每个商品截止到当前时间的销售总额，默认从起点到当前行</span><br><span class="line">sum(ordamt) over(partition by shop_id order by stat_date) as sum_amt1,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、统计每个商品前半小时到后一小时的销售额（按当前行+往前1行+往后2行的行作为窗口）</span><br><span class="line">sum(ordamt) over(partition by shop_id order by stat_date rows between 1 preceding and 2 following) as sum_amt2,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、统计每个商品的销售总额（从起点到末尾）</span><br><span class="line">sum(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following) as sum_amt3,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、统计截止到前半小时的销售总额（从起点到当前行的前一行）</span><br><span class="line">sum(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and 1 preceding) as sum_amt4,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、统计每个商品截止到当前时间的销售总额，默认从起点到当前行（从起点到当前行）</span><br><span class="line">sum(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and current row) as sum_amt5,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、统计当前时间及之后的销售总额（从当前行的末尾）</span><br><span class="line">sum(ordamt) over(partition by shop_id order by stat_date rows between current row and unbounded following) as sum_amt6,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、统计当前时间的后半小时及之后的销售额（当前行后一行到末尾）</span><br><span class="line">sum(ordamt) over(partition by shop_id order by stat_date rows between 1 following and unbounded following) as sum_amt7,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、统计当前时间后半小时到后一小时之间的销售额（按从当前行往后一行到当前行往后2行）</span><br><span class="line">sum(ordamt) over(partition by shop_id order by stat_date rows between 1 following and 2 following) as sum_amt8</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803965929.png" alt="img"></p>
<h4 id="avg开窗函数"><a href="#avg开窗函数" class="headerlink" title="avg开窗函数"></a>avg开窗函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、</span><br><span class="line">round(avg(ordamt) over(partition by shop_id order by stat_date),2) as avg_amt1,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、按当前行+往前1行+往后2行的行作为窗口的平均值</span><br><span class="line">round(avg(ordamt) over(partition by shop_id order by stat_date rows between 1 preceding and 2 following), 2) as avg_amt2,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到末尾的平均值</span><br><span class="line">round(avg(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following), 2) as avg_amt3,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的前一行的平均值</span><br><span class="line">round(avg(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and 1 preceding), 2) as avg_amt4,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的平均值</span><br><span class="line">round(avg(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and current row), 2) as avg_amt5,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从当前行的末尾的平均值</span><br><span class="line">round(avg(ordamt) over(partition by shop_id order by stat_date rows between current row and unbounded following), 2) as avg_amt6,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、当前行后一行到末尾的平均值</span><br><span class="line">round(avg(ordamt) over(partition by shop_id order by stat_date rows between 1 following and unbounded following), 2) as avg_amt7,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从当前行往后一行到当前行往后2行的平均值</span><br><span class="line">round(avg(ordamt) over(partition by shop_id order by stat_date rows between 1 following and 2 following), 2) as avg_amt8</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803973933.png" alt="img"></p>
<h4 id="min开窗函数"><a href="#min开窗函数" class="headerlink" title="min开窗函数"></a>min开窗函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、</span><br><span class="line">min(ordamt) over(partition by shop_id order by stat_date) as min_amt1,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、按当前行+往前1行+往后2行的行作为窗口的最小数</span><br><span class="line">min(ordamt) over(partition by shop_id order by stat_date rows between 1 preceding and 2 following) as min_amt2,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到末尾的最小数</span><br><span class="line">min(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following) as min_amt3,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的前一行的最小数</span><br><span class="line">min(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and 1 preceding) as min_amt4,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的最小数</span><br><span class="line">min(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and current row) as min_amt5,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从当前行的末尾的最小数</span><br><span class="line">min(ordamt) over(partition by shop_id order by stat_date rows between current row and unbounded following) as min_amt6,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、当前行后一行到末尾的最小数</span><br><span class="line">min(ordamt) over(partition by shop_id order by stat_date rows between 1 following and unbounded following) as min_amt7,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从当前行往后一行到当前行往后2行的最小数</span><br><span class="line">min(ordamt) over(partition by shop_id order by stat_date rows between 1 following and 2 following) as min_amt8</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803982963.png" alt="img"></p>
<h4 id="max开窗函数"><a href="#max开窗函数" class="headerlink" title="max开窗函数"></a>max开窗函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、</span><br><span class="line">max(ordamt) over(partition by shop_id order by stat_date) as max_amt1,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、按当前行+往前1行+往后2行的行作为窗口的最大数</span><br><span class="line">max(ordamt) over(partition by shop_id order by stat_date rows between 1 preceding and 2 following) as max_amt2,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到末尾的最大数</span><br><span class="line">max(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following) as max_amt3,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的前一行的最大数</span><br><span class="line">max(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and 1 preceding) as max_amt4,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的最大数</span><br><span class="line">max(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and current row) as max_amt5,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从当前行的末尾的最大数</span><br><span class="line">max(ordamt) over(partition by shop_id order by stat_date rows between current row and unbounded following) as max_amt6,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、当前行后一行到末尾的最大数</span><br><span class="line">max(ordamt) over(partition by shop_id order by stat_date rows between 1 following and unbounded following) as max_amt7,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从当前行往后一行到当前行往后2行的最大数</span><br><span class="line">max(ordamt) over(partition by shop_id order by stat_date rows between 1 following and 2 following) as max_amt8</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681803991327.png" alt="img"></p>
<h3 id="4-3-3窗口分析函数"><a href="#4-3-3窗口分析函数" class="headerlink" title="4.3.3窗口分析函数"></a>4.3.3窗口分析函数</h3><h4 id="first-value开窗函数"><a href="#first-value开窗函数" class="headerlink" title="first_value开窗函数"></a>first_value开窗函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、</span><br><span class="line">first_value(ordamt) over(partition by shop_id order by stat_date) as first_amt1,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、按当前行+往前1行+往后2行的行作为窗口的第一个值</span><br><span class="line">first_value(ordamt) over(partition by shop_id order by stat_date rows between 1 preceding and 2 following) as first_amt2,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到末尾的第一个值</span><br><span class="line">first_value(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following) as first_amt3,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的前一行的第一个值</span><br><span class="line">first_value(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and 1 preceding) as first_amt4,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的第一个值</span><br><span class="line">first_value(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and current row) as first_amt5,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从当前行的末尾的第一个值</span><br><span class="line">first_value(ordamt) over(partition by shop_id order by stat_date rows between current row and unbounded following) as first_amt6,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、当前行后一行到末尾的第一个值</span><br><span class="line">first_value(ordamt) over(partition by shop_id order by stat_date rows between 1 following and unbounded following) as first_amt7,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从当前行往后一行到当前行往后2行的第一个值</span><br><span class="line">first_value(ordamt) over(partition by shop_id order by stat_date rows between 1 following and 2 following) as first_amt8</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804001070.png" alt="img"></p>
<h4 id="last-value开窗函数"><a href="#last-value开窗函数" class="headerlink" title="last_value开窗函数"></a>last_value开窗函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、</span><br><span class="line">last_value(ordamt) over(partition by shop_id order by stat_date) as last_amt1,</span><br><span class="line">  -- 以按shop_id分组、按stat_date排序、按当前行+往前1行+往后2行的行作为窗口的最后一个值</span><br><span class="line">last_value(ordamt) over(partition by shop_id order by stat_date rows between 1 preceding and 2 following) as last_amt2,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到末尾的最后一个值</span><br><span class="line">last_value(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following) as last_amt3,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的前一行的最后一个值</span><br><span class="line">last_value(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and 1 preceding) as last_amt4,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从起点到当前行的最后一个值</span><br><span class="line">last_value(ordamt) over(partition by shop_id order by stat_date rows between unbounded preceding and current row) as last_amt5,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、从当前行的末尾的最后一个值</span><br><span class="line">last_value(ordamt) over(partition by shop_id order by stat_date rows between current row and unbounded following) as last_amt6,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、当前行后一行到末尾的最后一个值</span><br><span class="line">last_value(ordamt) over(partition by shop_id order by stat_date rows between 1 following and unbounded following) as last_amt7,</span><br><span class="line"> -- 以按shop_id分组、按stat_date排序、按从当前行往后一行到当前行往后2行的最后一个值</span><br><span class="line">last_value(ordamt) over(partition by shop_id order by stat_date rows between 1 following and 2 following) as last_amt8</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804008500.png" alt="img"></p>
<h4 id="lag开窗函数"><a href="#lag开窗函数" class="headerlink" title="lag开窗函数"></a>lag开窗函数</h4><p>注意:<br>1、使用第三个参数设置默认值时，默认值的数据类型需要和列的数据类型保持一致，否则设置不生效。<br>2、使用lag() 和 lead() 不能对窗口限定边界，必须是 unbounded 无界的，如果设置了边界，会出现如下报错信息。</p>
<p>Error while compiling statement: FAILED: SemanticException Failed to breakup Windowing invocations into Groups.<br>At least 1 group must only depend on input columns. Also check for circular dependencies.<br>Underlying error: Expecting left window frame boundary for function lag((tok_table_or_col ordamt), 1, ‘NA’)<br>Window Spec&#x3D;[PartitioningSpec&#x3D;[partitionColumns&#x3D;[(tok_table_or_col shop_id)]orderColumns&#x3D;[(tok_table_or_col<br>stat_date) ASC]]window(start&#x3D;range(1 FOLLOWING), end&#x3D;range(Unbounded FOLLOWING))] as _wcol2 to be unbounded.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">-- 以按shop_id分组、按stat_date排序、注意第三个参数默认值的类型需要和列类型匹配，否则不生效</span><br><span class="line">lag(ordamt, 1, 0) over(partition by shop_id order by stat_date) as last_amt1,</span><br><span class="line">lag(ordamt, 2, &#x27;NA&#x27;) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following) as last_amt2 </span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804016025.png" alt="img"></p>
<h4 id="lead开窗函数"><a href="#lead开窗函数" class="headerlink" title="lead开窗函数"></a>lead开窗函数</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">-- 以按shop_id分组、按stat_date排序、注意第三个参数默认值的类型需要和列类型匹配，否则不生效</span><br><span class="line">lead(ordamt, 1, 0) over(partition by shop_id order by stat_date) as last_amt1,</span><br><span class="line">lead(ordamt, 2, &#x27;NA&#x27;) over(partition by shop_id order by stat_date rows between unbounded preceding and unbounded following) as last_amt2 </span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804022719.png" alt="img"></p>
<h4 id="cume-dist开窗函数"><a href="#cume-dist开窗函数" class="headerlink" title="cume_dist开窗函数"></a>cume_dist开窗函数</h4><p>计算某个窗口或分区中某个值的累积分布。假定升序排序，则使用以下公式确定累积分布：<br>小于等于当前值x的行数 &#x2F; 窗口或partition分区内的总行数。其中，x 等于 order by 子句中指定的列的当前行中的值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">-- 统计小于等于当前销售额的订单占总订单的比例</span><br><span class="line">cume_dist() over(order by ordamt) as cume_dist1,</span><br><span class="line">-- 统计大于等于当前销售额的订单占总订单的比例</span><br><span class="line">cume_dist() over(order by ordamt desc) as cume_dist2,</span><br><span class="line">-- 统计分区内小于等于当前销售额的订单占总订单的比例</span><br><span class="line">round(cume_dist() over(partition by shop_id order by ordamt), 2) as cume_dist3</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804029518.png" alt="img"></p>
<h3 id="4-3-4窗口排序函数"><a href="#4-3-4窗口排序函数" class="headerlink" title="4.3.4窗口排序函数"></a>4.3.4窗口排序函数</h3><h4 id="rank开窗函数"><a href="#rank开窗函数" class="headerlink" title="rank开窗函数"></a>rank开窗函数</h4><p>rank 开窗函数基于 over 子句中的 order by 确定一组值中一个值的排名。<br>如果存在partition by ,则为每个分区组中的每个值排名。排名可能不是连续的。例如，如果两个行的排名为 1，则下一个排名为 3。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id排序</span><br><span class="line">rank() over(order by shop_id) as rank_amt1,</span><br><span class="line">  -- 以按shop_id分区、按stat_date排序</span><br><span class="line">rank() over(partition by shop_id order by stat_date) as rank_amt2</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804041076.png" alt="img"></p>
<h4 id="dense-rank开窗函数"><a href="#dense-rank开窗函数" class="headerlink" title="dense_rank开窗函数"></a>dense_rank开窗函数</h4><p>dense_rank与rank有一点不同,当排名一样的时候,接下来的行是连续的。如两个行的排名为 1，则下一个排名为 2。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id排序</span><br><span class="line">dense_rank() over(order by shop_id) as dense_amt1,</span><br><span class="line">  -- 以按shop_id分区、按stat_date排序</span><br><span class="line">dense_rank() over(partition by shop_id order by stat_date) as dense_amt2</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804049146.png" alt="img"></p>
<h4 id="ntile开窗函数"><a href="#ntile开窗函数" class="headerlink" title="ntile开窗函数"></a>ntile开窗函数</h4><p>将分区中已排序的行划分为大小尽可能相等的指定数量的排名的组，并返回给定行所在的组的排名。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分成两组、按stat_date排序</span><br><span class="line">ntile(2) over(partition by shop_id order by stat_date) as ntile_amt1,</span><br><span class="line">  -- 以按shop_id分成三组、按stat_date排序</span><br><span class="line">ntile(3) over(partition by shop_id order by stat_date) as ntile_amt2</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804057053.png" alt="img"></p>
<h4 id="row-number开窗函数"><a href="#row-number开窗函数" class="headerlink" title="row_number开窗函数"></a>row_number开窗函数</h4><p>从1开始对分区内的数据排序。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分区、按stat_date排序</span><br><span class="line">row_number() over(partition by shop_id order by stat_date) as row_amt</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804063310.png" alt="img"></p>
<h4 id="percent-rank开窗函数"><a href="#percent-rank开窗函数" class="headerlink" title="percent_rank开窗函数"></a>percent_rank开窗函数</h4><p>计算给定行的百分比排名。可以用来计算超过了百分之多少的人。如360小助手开机速度超过了百分之多少的人。<br>(当前行的rank值-1)&#x2F;(分组内的总行数-1)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">shop_id, stat_date, ordamt,</span><br><span class="line">  -- 以按shop_id分区、按stat_date排序</span><br><span class="line">row_number() over(partition by shop_id order by stat_date) as row_amt,</span><br><span class="line">round(percent_rank() over(partition by shop_id order by stat_date), 2) as percent_amt</span><br><span class="line">from shop_data;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1681804069826.png" alt="img"></p>
<h2 id="15-Hive挑战：HQL基础查询语句"><a href="#15-Hive挑战：HQL基础查询语句" class="headerlink" title="15.Hive挑战：HQL基础查询语句"></a>15.Hive挑战：HQL基础查询语句</h2><blockquote>
<h3 id="目的-14"><a href="#目的-14" class="headerlink" title="目的"></a>目的</h3><p>1.要求在实验结束后，可以熟练的使用基础HQL的进行连接、排序、分区、采样、合并</p>
<h3 id="要求-14"><a href="#要求-14" class="headerlink" title="要求"></a>要求</h3><p>1.增加对已经掌握的基础HQL语句的熟练程度<br>2.在规定时间内完成下述基础HQL语句挑战</p>
<h3 id="原理-14"><a href="#原理-14" class="headerlink" title="原理"></a>原理</h3><p>基于Hadoop，使用基础HQL语句实现对多张表的HQL操作</p>
</blockquote>
<h3 id="4-1准备数据-2"><a href="#4-1准备数据-2" class="headerlink" title="4.1准备数据"></a>4.1准备数据</h3><p>在桌面建立4个名字为student,course,teacher,score的文本文件，通过winSCP将文件传入平台&#x2F;usr&#x2F;cstor目录下，文本内容如下：</p>
<h3 id="1-student-txt"><a href="#1-student-txt" class="headerlink" title="1.student.txt"></a>1.student.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">01 赵雷 1990-01-01 男</span><br><span class="line">02 钱电 1990-12-21 男</span><br><span class="line">03 孙风 1990-05-20 男</span><br><span class="line">04 李云 1990-08-06 男</span><br><span class="line">05 周梅 1991-12-01 女</span><br><span class="line">06 吴兰 1992-03-01 女</span><br><span class="line">07 郑竹 1989-07-01 女</span><br><span class="line">08 王菊 1990-01-20 女</span><br></pre></td></tr></table></figure>

<h3 id="2-course-txt"><a href="#2-course-txt" class="headerlink" title="2.course.txt"></a>2.course.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">01 语文 02</span><br><span class="line">02 数学 01</span><br><span class="line">03 英语 03</span><br></pre></td></tr></table></figure>

<h3 id="3-teacher-txt"><a href="#3-teacher-txt" class="headerlink" title="3.teacher.txt"></a>3.teacher.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">01 张三</span><br><span class="line">02 李四</span><br><span class="line">03 王五</span><br></pre></td></tr></table></figure>

<h3 id="4-score-txt"><a href="#4-score-txt" class="headerlink" title="4.score.txt"></a>4.score.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">01 01 80</span><br><span class="line">01 02 90</span><br><span class="line">01 03 99</span><br><span class="line">02 01 70</span><br><span class="line">02 02 60</span><br><span class="line">02 03 80</span><br><span class="line">03 01 80</span><br><span class="line">03 02 80</span><br><span class="line">03 03 80</span><br><span class="line">04 01 50</span><br><span class="line">04 02 30</span><br><span class="line">04 03 20</span><br><span class="line">05 01 76</span><br><span class="line">05 02 87</span><br><span class="line">06 01 31</span><br><span class="line">06 03 34</span><br><span class="line">07 02 89</span><br><span class="line">07 03 98</span><br></pre></td></tr></table></figure>

<h3 id="4-2导入数据"><a href="#4-2导入数据" class="headerlink" title="4.2导入数据"></a>4.2导入数据</h3><h3 id="4-2-1建表"><a href="#4-2-1建表" class="headerlink" title="4.2.1建表"></a>4.2.1建表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table student(s_id string,s_name string,s_birth string,s_sex string) row format delimited fields terminated by &#x27; &#x27;;</span><br><span class="line">create table course(c_id string,c_name string,t_id string) row format delimited fields terminated by &#x27; &#x27;;</span><br><span class="line">create table teacher(t_id string,t_name string) row format delimited fields terminated by &#x27; &#x27;;</span><br><span class="line">create table score(s_id string,c_id string,s_score int) row format delimited fields terminated by &#x27; &#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2加载本地数据到hive表"><a href="#4-2-2加载本地数据到hive表" class="headerlink" title="4.2.2加载本地数据到hive表"></a>4.2.2加载本地数据到hive表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#x27;/usr/cstor/student.txt&#x27; into table student;</span><br><span class="line">load data local inpath &#x27;/usr/cstor/course.txt&#x27; into table course;</span><br><span class="line">load data local inpath &#x27;/usr/cstor/score.txt&#x27; into table score;</span><br><span class="line">load data local inpath &#x27;/usr/cstor/teacher.txt&#x27; into table teacher;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423681130.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423690075.png" alt="img"></p>
<h3 id="4-3HQL语句练习"><a href="#4-3HQL语句练习" class="headerlink" title="4.3HQL语句练习"></a>4.3HQL语句练习</h3><p>1.查询12月份过生日的学生:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from student</span><br><span class="line">where month(s_birth)=12;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423721334.png" alt="img"></p>
<p>2.查询本月过生日的学生:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from student</span><br><span class="line">where month(s_birth)=month(current_date);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423741173.png" alt="img"></p>
<p>3.查询本周过生日的学生:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from student</span><br><span class="line">where weekofyear(s_birth)=weekofyear(current_date);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423756567.png" alt="img"></p>
<p>4.查询各学生的年龄(周岁):</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select *,</span><br><span class="line">(case when month(current_date)&lt;month(s_birth)</span><br><span class="line">then year(current_date)-year(s_birth)-1 else year(current_date)-year(s_birth) end</span><br><span class="line">)as age</span><br><span class="line">from student;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423783032.png" alt="img"></p>
<p>5.查询选修了全部课程的学生信息:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">b.*</span><br><span class="line">from(</span><br><span class="line">select</span><br><span class="line">s_id,</span><br><span class="line">count(c_id) as num_course</span><br><span class="line">from score</span><br><span class="line">group by s_id</span><br><span class="line">) a</span><br><span class="line">join student b on a.s_id=b.s_id</span><br><span class="line">join (select count(*) as total_course from course) c on c.total_course=a.num_course;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423810449.png" alt="img"></p>
<p>6.检索至少选修两门课程的学生学号:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">s_id</span><br><span class="line">from score</span><br><span class="line">group by s_id</span><br><span class="line">having count(c_id)&gt;=2;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423831444.png" alt="img"></p>
<p>7.统计每门课程的学生选修人数（超过5人的课程才统计）:<br>– 要求输出课程号和选修人数，查询结果按人数降序排列，若人数相同，按课程号升序排列</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">c_id,</span><br><span class="line">count(s_score) as num_people</span><br><span class="line">from score</span><br><span class="line">group by c_id</span><br><span class="line">having num_people&gt;5</span><br><span class="line">order by num_people desc,c_id asc;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423848149.png" alt="img"></p>
<p>8.查询每门课程成绩最好的前三名:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select a.* from (</span><br><span class="line">select</span><br><span class="line">s_id,</span><br><span class="line">c_id,</span><br><span class="line">s_score,</span><br><span class="line">row_number() over(distribute by c_id sort by s_score desc) as rm</span><br><span class="line">from score</span><br><span class="line">) a</span><br><span class="line">where a.rm&lt;=3;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423873723.png" alt="img"></p>
<p>9.查询不同课程成绩相同的学生的学生编号、课程编号、学生成绩:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select distinct a.s_id,a.c_id,a.s_score</span><br><span class="line">from score a,score b</span><br><span class="line">where a.c_id!=b.c_id and a.s_score=b.s_score;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423892390.png" alt="img"></p>
<p>10.查询选修”张三”老师所授课程的学生中，成绩最高的学生信息及其成绩:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select t.s_id,t.s_name,t.s_birth,t.s_sex,t.s_score from(</span><br><span class="line">select</span><br><span class="line">a.s_id,a.s_name,a.s_birth,a.s_sex,b.s_score,max(b.s_score) over(partition by b.c_id) as max_score</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">join course c on b.c_id=c.c_id</span><br><span class="line">join teacher d on c.t_id=d.t_id and d.t_name=&#x27;张三&#x27;</span><br><span class="line">) t</span><br><span class="line">where t.s_score=t.max_score;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423914482.png" alt="img"></p>
<p>11.求每门课程的学生人数:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">c_id,</span><br><span class="line">count(s_id) as num_course</span><br><span class="line">from score</span><br><span class="line">group by c_id;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423957832.png" alt="img"></p>
<p>12.查询课程编号为01且课程成绩在80分以上的学生的学号和姓名:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">a.s_id,</span><br><span class="line">a.s_name,</span><br><span class="line">b.s_score</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">where b.c_id=&#x27;01&#x27; and b.s_score&gt;=80;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423976386.png" alt="img"></p>
<p>13.查询课程不及格的学生:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">a.s_name,</span><br><span class="line">c.c_name,</span><br><span class="line">b.s_score</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">join course c on b.c_id=c.c_id</span><br><span class="line">where b.s_score&lt;60;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676423998134.png" alt="img"></p>
<p>14.查询任何一门课程成绩在70分以上的学生姓名、课程名称和分数:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">a.s_name,</span><br><span class="line">c.c_name,</span><br><span class="line">b.s_score</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">join course c on b.c_id=c.c_id</span><br><span class="line">where b.s_score&gt;70;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424026990.png" alt="img"></p>
<p>15.查询所有学生的课程及分数情况:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">select p.*,(p.01_score+p.02_score+p.03_score) as total_score from(</span><br><span class="line">select</span><br><span class="line">t.s_id as s_id,</span><br><span class="line">t.s_name as s_name,              </span><br><span class="line">sum(t.01_score) as 01_score,</span><br><span class="line">sum(t.02_score) as 02_score,</span><br><span class="line">sum(t.03_score) as 03_score</span><br><span class="line">from(</span><br><span class="line">select a.s_id,a.s_name,b.s_score as 01_score,0 as 02_score,0 as 03_score from student a join score b on a.s_id=b.s_id where b.c_id=&#x27;01&#x27; union all</span><br><span class="line">select a.s_id,a.s_name,0 as 01_score,b.s_score as 02_score,0 as 03_score from student a join score b on a.s_id=b.s_id where b.c_id=&#x27;02&#x27; union all</span><br><span class="line">select a.s_id,a.s_name,0 as 01_score,0 as 02_score,b.s_score as 03_score from student a join score b on a.s_id=b.s_id where b.c_id=&#x27;03&#x27;</span><br><span class="line">) t</span><br><span class="line">group by t.s_id,t.s_name</span><br><span class="line">) p;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424049701.png" alt="img"></p>
<p>16.查询课程名称为”数学”，且分数低于60的学生姓名和分数:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">a.s_name,</span><br><span class="line">b.s_score</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">join course c on b.c_id=c.c_id</span><br><span class="line">where c.c_name=&#x27;数学&#x27; and b.s_score&lt;60;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424085698.png" alt="img"></p>
<p>17.查询平均成绩大于等于85的所有学生的学号、姓名和平均成绩:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select t.* from(</span><br><span class="line">select</span><br><span class="line">a.s_id as s_id,</span><br><span class="line">a.s_name as s_name,</span><br><span class="line">round(avg(b.s_score) over(distribute by b.s_id),2) as avg_score</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">) t</span><br><span class="line">where t.avg_score&gt;=85</span><br><span class="line">group by t.s_id,t.s_name,t.avg_score;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424102036.png" alt="img"></p>
<p>18.查询每门课程的平均成绩，结果按平均成绩降序排列，<br>平均成绩相同时，按课程编号升序排列:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select a.* from(</span><br><span class="line">select</span><br><span class="line">c_id,</span><br><span class="line">round(avg(s_score),2) as avg_score</span><br><span class="line">from score</span><br><span class="line">group by c_id</span><br><span class="line">) a</span><br><span class="line">order by a.avg_score desc,a.c_id asc;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424126537.png" alt="img"></p>
<p>19.查询1990年出生的学生名单:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from student where substr(s_birth,1,4)=&#x27;1990&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424175679.png" alt="img"></p>
<p>20.查询同名同性学生名单，并统计同名人数:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select a.s_name,a.num from(</span><br><span class="line">select</span><br><span class="line">s_name,</span><br><span class="line">s_sex,</span><br><span class="line">count(1) over(distribute by s_name,s_sex) as num</span><br><span class="line">from student</span><br><span class="line">) a</span><br><span class="line">where a.num&gt;=1;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424203199.png" alt="img"></p>
<p>21.查询名字中含有’风’字的学生信息:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from student where s_name like &#x27;%风%&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424217374.png" alt="img"></p>
<p>22.查询男生、女生人数:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(if(s_sex=&#x27;男&#x27;,s_sex,null)) ,count(if(s_sex=&#x27;女&#x27;,s_sex,null)) from student;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424231533.png" alt="img"></p>
<p>23.查询出只有两门课程的全部学生的学号和姓名:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select t.s_id,t.s_name from(</span><br><span class="line">select</span><br><span class="line">a.s_id as s_id,</span><br><span class="line">a.s_name as s_name,</span><br><span class="line">count(b.s_score) as num_course</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">group by a.s_id,a.s_name</span><br><span class="line">) t</span><br><span class="line">where t.num_course=2;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676424249022.png" alt="img"></p>
<p>24.查询每门课程被选修的学生数:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">c_id,</span><br><span class="line">count(s_score)</span><br><span class="line">from score</span><br><span class="line">group by c_id;</span><br></pre></td></tr></table></figure>

<p><img src="http://10.131.2.101/static/upload/resource/exp/ins/a4f191510cca49e7be9749048455ee26/image/image.1676424269787.png" alt="img"></p>
<h2 id="16-HBase实验：HBase简介与安装配置"><a href="#16-HBase实验：HBase简介与安装配置" class="headerlink" title="16.HBase实验：HBase简介与安装配置"></a>16.HBase实验：HBase简介与安装配置</h2><blockquote>
<h3 id="目的-15"><a href="#目的-15" class="headerlink" title="目的"></a>目的</h3><p>1.本实验我们将学习HBase和学会部署HBase环境。</p>
<h3 id="要求-15"><a href="#要求-15" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能够阐述HBase的概述及历史，了解HBase的数据模型，熟悉HBase的系统架构，掌握HBase的环境部署。</p>
<h3 id="原理-15"><a href="#原理-15" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1HBase-概述"><a href="#3-1HBase-概述" class="headerlink" title="3.1HBase 概述"></a>3.1HBase 概述</h2><p>HBase 是一个开源的非关系型分布式数据库（NoSQL），它参考了谷歌的 BigTable 建模，实现的编程语言为 Java。它是  Apache 软件基金会的 Hadoop 项目的一部分，运行于 HDFS 文件系统之上，为 Hadoop 提供类似于 BigTable  规模的服务，可以存储海量稀疏的数据，并具备一定的容错性、高可靠性及伸缩性。主要应用场景是实时随机读写超大规模的数据。</p>
<p>HBase 在列上实现了 BigTable 论文提到的压缩算法、内存操作和布隆过滤器。HBase 的表能够作为 MapReduce  任务的输入和输出，可以通过 Java API 来存取数据，也可以通过 REST、Avro 或者 Thrift 的 API 来访问。</p>
<p>HBase 不能取代 RDBMS，因为二者的应用场景不同。HBase  为了解决海量数据的扩展性，支持简单的增加节点来实现线性扩展，从而在集群上管理海量的非结构化或半结构化的稀疏数据。HBase 仅能通过主键（raw key）或主键的 range 检索数据，支持单行事务。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429137614.png" alt="img"></p>
<p>上图描述 Hadoop EcoSystem 中的各层系统。其中，HBase 位于结构化存储层，Hadoop HDFS 为 HBase  提供了高可靠性的底层存储支持，Hadoop MapReduce 为 HBase 提供了高性能的计算能力，Zookeeper 为 HBase  提供了稳定服务和 failover 机制。</p>
<p>此外，Pig 和 Hive 还为 HBase 提供了高层语言支持，使得在 HBase 上进行数据统计处理变的非常简单。Sqoop 则为 HBase 提供了方便的 RDBMS 数据导入功能，使得传统数据库数据向 HBase 中迁移变的非常方便。</p>
<p>Apache HBase 最初是 Powerset 公司为了处理自然语言搜索产生的海量数据而开展的项目。</p>
<h2 id="3-2HBase-历史"><a href="#3-2HBase-历史" class="headerlink" title="3.2HBase 历史"></a>3.2HBase 历史</h2><p>下图展示了 HBase 的发展历程。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429153360.png" alt="img"></p>
<h2 id="3-3HBase-数据模型"><a href="#3-3HBase-数据模型" class="headerlink" title="3.3HBase 数据模型"></a>3.3HBase 数据模型</h2><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429171615.png" alt="img"></p>
<p>行健（Row Key）：表的主键，表中的记录默认按照行健升序排序<br>时间戳（Timestamp）：每次数据操作对应的时间戳，可以看作是数据的版本号<br>列族（Column  Family）：表在水平方向有一个或者多个列族组成，一个列族中可以由任意多个列组成，列族支持动态扩展，无需预先定义列的数量以及类型，所有列均以二进制格式存储，用户需要自行进行类型转换。所有的列族成员的前缀是相同的，例如 abc:a1 和 abc:a2 两个列都属于 abc 这个列族。<br>表和区域（Table&amp;Region）：当表随着记录数不断增加而变大后，会逐渐分裂成多份，成为区域，一个区域是对表的水平划分，不同的区域会被 Master 分配给相应的 RegionServer 进行管理<br>单元格（Cell）：表存储数据的单元。由{行健，列（列族：标签），时间戳}唯一确定，其中的数据是没有类型的，以二进制的形式存储。</p>
<h2 id="3-4HBase-架构"><a href="#3-4HBase-架构" class="headerlink" title="3.4HBase 架构"></a>3.4HBase 架构</h2><h3 id="3-4-1Client"><a href="#3-4-1Client" class="headerlink" title="3.4.1Client"></a>3.4.1Client</h3><p>HBase Client 使用 HBase 的 RPC 机制与 HMaster 和 HRegionServer  进行通信，对于管理类操作，Client 与 HMaster 进行 RPC。对于数据读写类操作，Client 与 HRegionServer 进行 RPC。</p>
<h3 id="3-4-2Zookeeper"><a href="#3-4-2Zookeeper" class="headerlink" title="3.4.2Zookeeper"></a>3.4.2Zookeeper</h3><p>Zookeeper Quorum 中除了存储了 -ROOT- 表的地址和 HMaster 的地址，HRegionServer 也会把自己以 Ephemeral 方式注册到 Zookeeper 中，使得 HMaster 可以随时感知到各个 HRegionServer  的健康状态。此外，Zookeeper 也避免了 HMaster 的单点问题，见下文描述。</p>
<h3 id="3-4-3HMaster"><a href="#3-4-3HMaster" class="headerlink" title="3.4.3HMaster"></a>3.4.3HMaster</h3><p>HMaster 没有单点问题，HBase 中可以启动多个 HMaster，通过 Zookeeper 的 Master Election 机制保证总有一个 Master 运行，HMaster 在功能上主要负责 Table 和 Region 的管理工作：<br>1.管理用户对 Table 的增、删、改、查操作。<br>2.管理 HRegionServer 的负载均衡，调整 Region 分布。<br>3.在 Region Split 后，负责新 Region 的分配。<br>4.在 HRegionServer 停机后，负责失效 HRegionServer 上的 Regions 迁移。</p>
<h3 id="3-4-4HRegionServer"><a href="#3-4-4HRegionServer" class="headerlink" title="3.4.4HRegionServer"></a>3.4.4HRegionServer</h3><p>用户 I&#x2F;O 请求，向 HDFS 文件系统中读写数据，是 HBase 中最核心的模块。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429201973.png" alt="img"></p>
<p>HRegionServer 内部管理了一系列 HRegion 对象，每个 HRegion 对应了 Table 中的一个  Region，HRegion 中由多个 HStore 组成。每个 HStore 对应了 Table 中的一个 Column Family  的存储，可以看出每个 Column Family 其实就是一个集中的存储单元，因此最好将具备共同 IO 特性的 column 放在一个  Column Family 中，这样最高效。</p>
<p>HStore 存储是 HBase 存储的核心了，其中由两部分组成，一部分是 MemStore，一部分是  StoreFiles。MemStore 是 Sorted Memory Buffer，用户写入的数据首先会放入 MemStore，当  MemStore 满了以后会 Flush 成一个 StoreFile（底层实现是 HFile），当 StoreFile  文件数量增长到一定阈值，会触发 Compact 合并操作，将多个 StoreFiles 合并成一个  StoreFile，合并过程中会进行版本合并和数据删除，因此可以看出 HBase 其实只有增加数据，所有的更新和删除操作都是在后续的  compact 过程中进行的，这使得用户的写操作只要进入内存中就可以立即返回，保证了 HBase I&#x2F;O 的高性能。当 StoreFiles  Compact 后，会逐步形成越来越大的 StoreFile，当单个 StoreFile 大小超过一定阈值后，会触发 Split  操作，同时把当前 Region Split 成 2 个 Region，父 Region 会下线，新 Split 出的 2 个孩子 Region  会被 HMaster 分配到相应的 HRegionServer 上，使得原先 1 个 Region 的压力得以分流到 2 个 Region 上。<br>下图描述了 Compaction 和 Split 的过程：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429227267.png" alt="img"></p>
<p>在理解了上述 HStore 的基本原理后，还必须了解一下 HLog 的功能，因为上述的 HStore  在系统正常工作的前提下是没有问题的，但是在分布式系统环境中，无法避免系统出错或者宕机，因此一旦 HRegionServer  意外退出，MemStore 中的内存数据将会丢失，这就需要引入 HLog 了。每个 HRegionServer 中都有一个 HLog  对象，HLog 是一个实现 Write Ahead Log 的类，在每次用户操作写入 MemStore 的同时，也会写一份数据到 HLog  文件中（HLog 文件格式见后续），HLog 文件定期会滚动出新的，并删除旧的文件（已持久化到 StoreFile 中的数据）。当  HRegionServer 意外终止后，HMaster 会通过 Zookeeper 感知到，HMaster 首先会处理遗留的 HLog  文件，将其中不同 Region 的 Log 数据进行拆分，分别放到相应 region 的目录下，然后再将失效的 region  重新分配，领取到这些 region 的 HRegionServer 在 Load Region 的过程中，会发现有历史 HLog  需要处理，因此会 Replay HLog 中的数据到 MemStore 中，然后 flush 到 StoreFiles，完成数据恢复。</p>
<h2 id="3-5HBase-访问接口"><a href="#3-5HBase-访问接口" class="headerlink" title="3.5HBase 访问接口"></a>3.5HBase 访问接口</h2><p>1.Native Java API，最常规和高效的访问方式，适合 Hadoop MapReduce Job 并行批处理 HBase 表数据。<br>2.HBase Shell，HBase 的命令行工具，最简单的接口，适合 HBase 管理使用。<br>3.Thrift Gateway，利用 Thrift 序列化技术，支持 C++，PHP，Python 等多种语言，适合其他异构系统在线访问 HBase 表数据。<br>4.REST Gateway，支持 REST 风格的 Http API 访问 HBase, 解除了语言限制。<br>5.Pig，可以使用 Pig Latin 流式编程语言来操作 HBase 中的数据，和 Hive 类似，本质最终也是编译成 MapReduce Job 来处理 HBase 表数据，适合做数据统计。</p>
<h2 id="3-6HBase-存储格式"><a href="#3-6HBase-存储格式" class="headerlink" title="3.6HBase 存储格式"></a>3.6HBase 存储格式</h2><p>HBase 中的所有数据文件都存储在 Hadoop HDFS 文件系统上，主要包括上述提出的两种文件类型：<br>HFile：HBase 中 KeyValue 数据的存储格式，HFile 是 Hadoop 的二进制格式文件，实际上 StoreFile 就是对 HFile 做了轻量级包装，即 StoreFile 底层就是 HFile。<br>HLog File：HBase 中 WAL（Write Ahead Log） 的存储格式，物理上是 Hadoop 的 Sequence File。</p>
<h3 id="3-6-1HFile"><a href="#3-6-1HFile" class="headerlink" title="3.6.1HFile"></a>3.6.1HFile</h3><p>下图是 HFile 的存储格式：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429270206.png" alt="img"></p>
<p>首先 HFile 文件是不定长的，长度固定的只有其中的两块：Trailer 和 FileInfo。正如图中所示的，Trailer  中有指针指向其他数据块的起始点。File Info 中记录了文件的一些 Meta  信息，例如：AVG_KEY_LEN、AVG_VALUE_LEN、LAST_KEY、COMPARATOR 和 MAX_SEQ_ID_KEY  等。Data Index 和 Meta Index 块记录了每个 Data 块和 Meta 块的起始点。</p>
<p>Data Block 是 HBase I&#x2F;O 的基本单元，为了提高效率，HRegionServer 中有基于 LRU 的 Block  Cache 机制。每个 Data 块的大小可以在创建一个 Table 的时候通过参数指定，大号的 Block 有利于顺序 Scan，小号  Block 利于随机查询。 每个 Data 块除了开头的 Magic 以外就是一个个 KeyValue 对拼接而成，Magic  内容就是一些随机数字，目的是防止数据损坏。后面会详细介绍每个 KeyValue 对的内部构造。<br>HFile 里面的每个 KeyValue 对就是一个简单的 byte 数组。但是这个 byte 数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429303211.png" alt="img"></p>
<p>开始是两个固定长度的数值，分别表示 Key 的长度和 Value 的长度。紧接着是 Key，开始是固定长度的数值，表示 RowKey  的长度，紧接着是 RowKey，然后是固定长度的数值，表示 Family 的长度，然后是 Family，接着是  Qualifier，然后是两个固定长度的数值，表示 Time Stamp 和 Key Type（Put&#x2F;Delete）。Value  部分没有这么复杂的结构，就是纯粹的二进制数据了。</p>
<p>HLog File</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429333574.png" alt="img"></p>
<p>上图中示意了 HLog 文件的结构，其实 HLog 文件就是一个普通的 Hadoop Sequence File，Sequence  File 的 Key 是 HLogKey 对象，HLogKey 中记录了写入数据的归属信息，除了 table 和 region  名字外，同时还包括 sequence number 和 timestamp，timestamp 是“写入时间”，sequence number  的起始值为 0，或者是最近一次存入文件系统中 sequence number。<br>HLog Sequece File 的 Value 是 HBase 的 KeyValue 对象，即对应 HFile 中的 KeyValue，可参见上文描述。</p>
<h2 id="3-7HBase-应用场景"><a href="#3-7HBase-应用场景" class="headerlink" title="3.7HBase 应用场景"></a>3.7HBase 应用场景</h2><h3 id="3-7-1HBase-的优势主要在以下几方面："><a href="#3-7-1HBase-的优势主要在以下几方面：" class="headerlink" title="3.7.1HBase 的优势主要在以下几方面："></a>3.7.1HBase 的优势主要在以下几方面：</h3><p>1.海量数据存储<br>2.快速随机访问<br>3.大量写操作的应用</p>
<h3 id="3-7-2常见的应用场景："><a href="#3-7-2常见的应用场景：" class="headerlink" title="3.7.2常见的应用场景："></a>3.7.2常见的应用场景：</h3><p>1.互联网搜索引擎数据存储（BigTable 要解决的问题）<br>2.审计日志系统<br>3.实时系统<br>4.消息中心<br>5.内容服务系统</p>
</blockquote>
<h3 id="4-1编辑-hbase-env-sh"><a href="#4-1编辑-hbase-env-sh" class="headerlink" title="4.1编辑 hbase-env.sh"></a>4.1编辑 hbase-env.sh</h3><p>1.打开 hbase-env.sh 文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hbase/conf </span><br><span class="line"># vim hbase-env.sh</span><br></pre></td></tr></table></figure>

<p>2.修改该文件配置。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_161</span><br><span class="line">export HBASE_CLASSPATH=/usr/cstor/hadoop/etc/hadoop</span><br><span class="line">export HBASE_MANAGES_ZK=true</span><br></pre></td></tr></table></figure>

<p><img src="http://10.131.2.101/static/upload/resource/exp/ins/b697f1d2bfaa4931b92b1ebd0e28def5/image/image.1676429364944.png" alt="img"></p>
<h3 id="4-2编辑-hbase-site-xml"><a href="#4-2编辑-hbase-site-xml" class="headerlink" title="4.2编辑 hbase-site.xml"></a>4.2编辑 hbase-site.xml</h3><p>1.打开 hbase-site.xml 配置文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hbase/conf </span><br><span class="line"># vim hbase-site.xml</span><br></pre></td></tr></table></figure>

<p>2.配置 hbase-site.xml 文件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hdfs://master:8020/hbase&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">//注意：这里的配置应为实验环境的主机名</span><br><span class="line">     &lt;value&gt;master,slave1,slave2&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">     &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;/hbase&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hbase.tmp.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;/usr/cstor/hbase/data/tmp&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line"> &lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>配置 regionservers文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim regionservers</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429487571.png" alt="img"></p>
<h3 id="4-3启动-HBase"><a href="#4-3启动-HBase" class="headerlink" title="4.3启动 HBase"></a>4.3启动 HBase</h3><p>将配置拷贝到从节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /usr/cstor/hbase/conf slave1:/usr/cstor/hbase/</span><br><span class="line">scp -r /usr/cstor/hbase/conf slave2:/usr/cstor/hbase/</span><br></pre></td></tr></table></figure>

<p>通过如下命令启动 Hbase：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hbase/bin </span><br><span class="line"># ./start-hbase.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429513836.png" alt="img"></p>
<h3 id="4-4验证启动"><a href="#4-4验证启动" class="headerlink" title="4.4验证启动"></a>4.4验证启动</h3><p>1.使用 jps 查看master节点状态。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429531288.png" alt="img"></p>
<p>使用 jps 查看从节点状态。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429545233.png" alt="img"></p>
<p>2.进入 hbase 的 shell 命令行，创建表 member 并进行查看。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hbase shell </span><br><span class="line">hbase&gt; create &#x27;member&#x27;, &#x27;m_id&#x27;, &#x27;address&#x27;, &#x27;info&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676429562453.png" alt="img"></p>
<h2 id="17-Hive挑战：HQL复杂查询语句"><a href="#17-Hive挑战：HQL复杂查询语句" class="headerlink" title="17.Hive挑战：HQL复杂查询语句"></a>17.Hive挑战：HQL复杂查询语句</h2><blockquote>
<h3 id="目的-16"><a href="#目的-16" class="headerlink" title="目的"></a>目的</h3><p>1.要求在实验结束后，可以熟练的使用复杂HQL的进行连接、排序、分区、采样、合并</p>
<h3 id="要求-16"><a href="#要求-16" class="headerlink" title="要求"></a>要求</h3><p>1.增加对已经掌握的复杂HQL语句的熟练程度<br>2.在规定时间内完成下述复杂HQL语句挑战</p>
<h3 id="原理-16"><a href="#原理-16" class="headerlink" title="原理"></a>原理</h3><p>基于Hadoop，使用复杂HQL语句实现对多张表的HQL操作</p>
</blockquote>
<h3 id="4-1准备数据-3"><a href="#4-1准备数据-3" class="headerlink" title="4.1准备数据"></a>4.1准备数据</h3><p>在桌面建立4个名字为student,course,teacher,score的文本文件，通过winSCP将文件传入平台&#x2F;usr&#x2F;cstor目录下，文本内容如下：</p>
<h3 id="1-student-txt-1"><a href="#1-student-txt-1" class="headerlink" title="1.student.txt"></a>1.student.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">01 赵雷 1990-01-01 男</span><br><span class="line">02 钱电 1990-12-21 男</span><br><span class="line">03 孙风 1990-05-20 男</span><br><span class="line">04 李云 1990-08-06 男</span><br><span class="line">05 周梅 1991-12-01 女</span><br><span class="line">06 吴兰 1992-03-01 女</span><br><span class="line">07 郑竹 1989-07-01 女</span><br><span class="line">08 王菊 1990-01-20 女</span><br></pre></td></tr></table></figure>

<h3 id="2-course-txt-1"><a href="#2-course-txt-1" class="headerlink" title="2.course.txt"></a>2.course.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">01 语文 02</span><br><span class="line">02 数学 01</span><br><span class="line">03 英语 03</span><br></pre></td></tr></table></figure>

<h3 id="3-teacher-txt-1"><a href="#3-teacher-txt-1" class="headerlink" title="3.teacher.txt"></a>3.teacher.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">01 张三</span><br><span class="line">02 李四</span><br><span class="line">03 王五</span><br></pre></td></tr></table></figure>

<h3 id="4-score-txt-1"><a href="#4-score-txt-1" class="headerlink" title="4.score.txt"></a>4.score.txt</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">01 01 80</span><br><span class="line">01 02 90</span><br><span class="line">01 03 99</span><br><span class="line">02 01 70</span><br><span class="line">02 02 60</span><br><span class="line">02 03 80</span><br><span class="line">03 01 80</span><br><span class="line">03 02 80</span><br><span class="line">03 03 80</span><br><span class="line">04 01 50</span><br><span class="line">04 02 30</span><br><span class="line">04 03 20</span><br><span class="line">05 01 76</span><br><span class="line">05 02 87</span><br><span class="line">06 01 31</span><br><span class="line">06 03 34</span><br><span class="line">07 02 89</span><br><span class="line">07 03 98</span><br></pre></td></tr></table></figure>

<h3 id="4-2导入数据-1"><a href="#4-2导入数据-1" class="headerlink" title="4.2导入数据"></a>4.2导入数据</h3><h3 id="4-2-1建表-1"><a href="#4-2-1建表-1" class="headerlink" title="4.2.1建表"></a>4.2.1建表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table student(s_id string,s_name string,s_birth string,s_sex string) row format delimited fields terminated by &#x27; &#x27;;</span><br><span class="line">create table course(c_id string,c_name string,t_id string) row format delimited fields terminated by &#x27; &#x27;;</span><br><span class="line">create table teacher(t_id string,t_name string) row format delimited fields terminated by &#x27; &#x27;;</span><br><span class="line">create table score(s_id string,c_id string,s_score int) row format delimited fields terminated by &#x27; &#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2加载本地数据到hive表-1"><a href="#4-2-2加载本地数据到hive表-1" class="headerlink" title="4.2.2加载本地数据到hive表"></a>4.2.2加载本地数据到hive表</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#x27;/usr/cstor/student.txt&#x27; into table student;</span><br><span class="line">load data local inpath &#x27;/usr/cstor/course.txt&#x27; into table course;</span><br><span class="line">load data local inpath &#x27;/usr/cstor/score.txt&#x27; into table score;</span><br><span class="line">load data local inpath &#x27;/usr/cstor/teacher.txt&#x27; into table teacher;</span><br></pre></td></tr></table></figure>

<h3 id="4-3HQL语句练习-1"><a href="#4-3HQL语句练习-1" class="headerlink" title="4.3HQL语句练习"></a>4.3HQL语句练习</h3><p>1.查询各科成绩前三名的记录三个语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">select a.* from (</span><br><span class="line">select</span><br><span class="line">s_id,</span><br><span class="line">c_id,</span><br><span class="line">s_score,</span><br><span class="line">row_number() over(distribute by c_id sort by s_score desc) as rm</span><br><span class="line">from score</span><br><span class="line">) a</span><br><span class="line">where a.rm&lt;=3;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676353996078.png" alt="img"></p>
<p>2.查询学生平均成绩及其名次:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">a.*,</span><br><span class="line">row_number() over(order by a.avg_score desc) as rm</span><br><span class="line">from(</span><br><span class="line">select</span><br><span class="line">s_id,</span><br><span class="line">round(avg(s_score),2) as avg_score</span><br><span class="line">from score</span><br><span class="line">group by s_id</span><br><span class="line">order by avg_score desc</span><br><span class="line">) a;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354014198.png" alt="img"></p>
<p>3.统计各科成绩各分数段人数：<br>课程编号,课程名称,[100-85],[85-70],[70-60],[0-60]及所占百分比</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">a.c_id,</span><br><span class="line">b.c_name,</span><br><span class="line">count(if(a.s_score&gt;85 and a.s_score&lt;=100,a.s_score,null)) as 85_100,</span><br><span class="line">round(count(if(a.s_score&gt;85 and a.s_score&lt;=100,a.s_score,null))/count(a.s_score)*100,2) as percentage,</span><br><span class="line">count(if(a.s_score&gt;70 and a.s_score&lt;=85,a.s_score,null)) as 85_100,</span><br><span class="line">round(count(if(a.s_score&gt;70 and a.s_score&lt;=85,a.s_score,null))/count(a.s_score)*100,2) as percentage,</span><br><span class="line">count(if(a.s_score&gt;60 and a.s_score&lt;=70,a.s_score,null)) as 85_100,</span><br><span class="line">round(count(if(a.s_score&gt;60 and a.s_score&lt;=70,a.s_score,null))/count(a.s_score)*100,2) as percentage,</span><br><span class="line">count(if(a.s_score&gt;0 and a.s_score&lt;=60,a.s_score,null)) as 85_100,</span><br><span class="line">round(count(if(a.s_score&gt;0 and a.s_score&lt;=60,a.s_score,null))/count(a.s_score)*100,2) as percentage</span><br><span class="line">from score a</span><br><span class="line">join course b on a.c_id=b.c_id</span><br><span class="line">group by a.c_id,</span><br><span class="line">b.c_name;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354057516.png" alt="img"></p>
<p>4.查询所有课程的成绩第2名到第3名的学生信息及该课程成绩:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select b.*,a.rm,a.s_score,a.c_id from (</span><br><span class="line">select</span><br><span class="line">*,</span><br><span class="line">row_number() over(distribute by c_id sort by s_score desc) as rm</span><br><span class="line">from score</span><br><span class="line">) a</span><br><span class="line">join student b on b.s_id=a.s_id</span><br><span class="line">where a.rm=2 or a.rm=3;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354088062.png" alt="img"></p>
<p>5.查询不同老师所教不同课程平均分从高到低显示:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">a.c_id,</span><br><span class="line">b.c_name,</span><br><span class="line">c.t_name,</span><br><span class="line">round(avg(a.s_score),2) as avg_score</span><br><span class="line">from score a</span><br><span class="line">join course b on a.c_id=b.c_id</span><br><span class="line">join teacher c on b.t_id=c.t_id</span><br><span class="line">group by a.c_id,b.c_name,c.t_name</span><br><span class="line">order by avg_score desc;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354116926.png" alt="img"></p>
<p>6.查询学生的总成绩并进行排名:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select s_id,</span><br><span class="line">sum(s_score) as total_score,</span><br><span class="line">row_number() over(sort by sum(s_score) desc) as rm</span><br><span class="line">from score</span><br><span class="line">group by s_id;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354221228.png" alt="img"></p>
<p>7.按各科成绩进行排序，并显示排名:– row_number() over()分组排序功能</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">*,</span><br><span class="line">row_number() over(distribute by c_id sort by s_score desc) as rm</span><br><span class="line">from score;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354235175.png" alt="img"></p>
<p>8.查询各科成绩最高分、最低分和平均分：以如下形式显示：课程ID，课程name，最高分，最低分，平均分，及格率，中等率，优良率，优秀率<br>– 及格为&gt;&#x3D;60，中等为：70-80，优良为：80-90，优秀为：&gt;&#x3D;90</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">a.c_id,</span><br><span class="line">b.c_name,</span><br><span class="line">MAX(a.s_score) AS max_score,</span><br><span class="line">MIN(a.s_score) AS min_score,</span><br><span class="line">ROUND (AVG(a.s_score),2) AS avg_score,</span><br><span class="line">round(count(if(a.s_score&gt;=60,a.s_score,null))/count(a.s_score)*100,2) as jige,</span><br><span class="line">round(count(if(a.s_score&gt;=70 and a.s_score&lt;80,a.s_score,null))/count(a.s_score)*100,2) as zd,</span><br><span class="line">round(count(if(a.s_score&gt;=80 and a.s_score&lt;90,a.s_score,null))/count(a.s_score)*100,2) as yl,</span><br><span class="line">round(count(if(a.s_score&gt;=90,a.s_score,null))/count(a.s_score)*100,2) as yx</span><br><span class="line">FROM score a</span><br><span class="line">JOIN course b ON a.c_id=b.c_id</span><br><span class="line">GROUP BY a.c_id,b.c_name;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354268224.png" alt="img"></p>
<p>9.按平均成绩从高到低显示学完所有课程学生的成绩以及平均成绩</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">a.s_id,</span><br><span class="line">b.s_score as 01_score,</span><br><span class="line">c.s_score as 02_score,</span><br><span class="line">d.s_score as 03_score,</span><br><span class="line">ROUND ((b.s_score+c.s_score+d.s_score)/3,2) as avg_score</span><br><span class="line">FROM score a</span><br><span class="line">join score b on a.s_id=b.s_id and b.c_id=&#x27;01&#x27;</span><br><span class="line">join score c on a.s_id=c.s_id and c.c_id=&#x27;02&#x27;</span><br><span class="line">join score d on a.s_id=d.s_id and d.c_id=&#x27;03&#x27;</span><br><span class="line">GROUP BY a.s_id,b.s_score,c.s_score,d.s_score</span><br><span class="line">ORDER BY ROUND ((b.s_score+c.s_score+d.s_score)/3,2) DESC ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354286503.png" alt="img"></p>
<p>10.检索’01’课程分数小于60，按分数降序排列的学生信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.*,b.c_id,b.s_score</span><br><span class="line">FROM student a</span><br><span class="line">JOIN score b ON a.s_id=b.s_id AND b.s_score&lt;60</span><br><span class="line">WHERE b.c_id=&#x27;01&#x27;</span><br><span class="line">ORDER BY b.s_score DESC;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354309669.png" alt="img"></p>
<p>11.查询两门及其以上不及格课程的同学的学号，姓名及其平均成绩</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.s_id,a.s_name,ROUND(AVG(b.s_score),2)</span><br><span class="line">FROM student a</span><br><span class="line">JOIN score b ON a.s_id=b.s_id AND b.s_score&lt;60</span><br><span class="line">GROUP BY a.s_id,a.s_name</span><br><span class="line">HAVING COUNT(1)&gt;=2;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354327939.png" alt="img"></p>
<p>12.查询没学过’张三’老师讲授的任一门课程的学生姓名</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select a.* from student a where not exists(select 1 from score b,course c,teacher d where a.s_id=b.s_id and b.c_id=c.c_id and c.t_id=d.t_id and d.t_name=&#x27;张三&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354363576.png" alt="img"></p>
<p>13.查询和’01’号的同学学习的课程完全相同的其他同学的信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select s.s_name from student s join score sc on s.s_id=sc.s_id</span><br><span class="line">join(select collect_set(c_id) as sub,count(c_id) as num from score where s_id=&#x27;01&#x27; ) sc2</span><br><span class="line">where array_contains(sc2.sub,sc.c_id)</span><br><span class="line">group by s.s_id,s.s_name,s.s_birth,s.s_sex,sc2.num</span><br><span class="line">having count(sc.c_id)=sc2.num;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354391030.png" alt="img"></p>
<p>14.查询至少有一门课与学号为’01’的同学所学相同的同学的信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select distinct</span><br><span class="line">a.*</span><br><span class="line">from student a</span><br><span class="line">join score b on b.s_id=&#x27;01&#x27;</span><br><span class="line">join score c on b.c_id=c.c_id and a.s_id=c.s_id</span><br><span class="line">where a.s_id&lt;&gt;&#x27;01&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354415807.png" alt="img"></p>
<p>15.查询没有学全所有课程的同学的信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select distinct</span><br><span class="line">a.*</span><br><span class="line">from student a</span><br><span class="line">join score b</span><br><span class="line">left join score c on a.s_id=c.s_id and b.c_id=c.c_id</span><br><span class="line">where c.s_score is null;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354431712.png" alt="img"></p>
<p>16.查询学过编号为’01’但是没有学过编号为’02’的课程的同学的信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">a.s_id,a.s_name,a.s_birth,a.s_sex</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id and b.c_id=&#x27;01&#x27;</span><br><span class="line">where not exists(select 1 from score c where a.s_id=c.s_id and c.c_id=&#x27;02&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354464864.png" alt="img"></p>
<p>17.查询学过编号为’01’并且也学过编号为’02’的课程的同学的信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.*</span><br><span class="line">FROM student a</span><br><span class="line">JOIN score b ON a.s_id=b.s_id AND b.c_id=&#x27;01&#x27;</span><br><span class="line">JOIN score c ON a.s_id=c.s_id AND c.c_id=&#x27;02&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354482635.png" alt="img"></p>
<p>18.查询没学过’张三’老师授课的同学的信息 w表为题7的结果表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select a.*</span><br><span class="line">from student a</span><br><span class="line">where not exists(select 1 from score b,course c,teacher d where a.s_id=b.s_id and b.c_id=c.c_id and c.t_id=d.t_id and d.t_name=&#x27;张三&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354533254.png" alt="img"></p>
<p>19.查询学过’张三’老师授课的同学的信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select a.*</span><br><span class="line">from student a</span><br><span class="line">where exists(select 1 from score b</span><br><span class="line">join course c on b.c_id=c.c_id</span><br><span class="line">join teacher d on c.t_id=d.t_id and d.t_name=&#x27;张三&#x27;</span><br><span class="line">where a.s_id=b.s_id);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354545812.png" alt="img"></p>
<p>20.查询’李’姓老师的数量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT COUNT(*)</span><br><span class="line">FROM teacher a</span><br><span class="line">WHERE a.t_name LIKE &#x27;李%&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354584136.png" alt="img"></p>
<p>21.查询所有同学的学生编号、学生姓名、选课总数、所有课程的总成绩</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">a.s_id,a.s_name,COUNT(b.c_id) AS num_subject,SUM(b.s_score) AS total_score</span><br><span class="line">FROM student a</span><br><span class="line">JOIN score b ON a.s_id=b.s_id</span><br><span class="line">GROUP BY a.s_id,a.s_name;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354628411.png" alt="img"></p>
<p>22.查询平均成绩小于60分的同学的学生编号和学生姓名和平均成绩<br>– (包括有成绩的和无成绩的)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">select * from(</span><br><span class="line">select</span><br><span class="line">a.s_id,a.s_name,round(avg(b.s_score),2) as avg_score</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">group by a.s_id,a.s_name</span><br><span class="line">) t</span><br><span class="line">where t.avg_score&lt;60</span><br><span class="line">union all</span><br><span class="line">SELECT c.s_id,c.s_name,null AS avg_score</span><br><span class="line">FROM student c</span><br><span class="line">left join score d on c.s_id=d.s_id</span><br><span class="line">where d.s_score is null;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354649351.png" alt="img"></p>
<p>23.查询平均成绩大于等于60分的同学的学生编号和学生姓名和平均成绩</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select * from(</span><br><span class="line">select</span><br><span class="line">a.s_id,a.s_name,round(avg(b.s_score),2) as avg_score</span><br><span class="line">from student a</span><br><span class="line">join score b on a.s_id=b.s_id</span><br><span class="line">group by a.s_id,a.s_name</span><br><span class="line">) t</span><br><span class="line">where t.avg_score&gt;=60;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354676996.png" alt="img"></p>
<p>24.查询’01’课程比’02’课程成绩低的学生的信息及课程分数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">a.*,b.s_score AS 01_score,c.s_score AS 02_score</span><br><span class="line">FROM student a</span><br><span class="line">JOIN score b ON a.s_id=b.s_id AND b.c_id=&#x27;01&#x27;</span><br><span class="line">LEFT JOIN score c ON a.s_id=c.s_id AND c.c_id=&#x27;02&#x27;</span><br><span class="line">WHERE b.s_score&lt;c.s_score;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354692568.png" alt="img"></p>
<p>25.查询’01’课程比’02’课程成绩高的学生的信息及课程分数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">a.*,b.s_score AS 01_score,c.s_score AS 02_score</span><br><span class="line">FROM student a</span><br><span class="line">JOIN score b ON a.s_id=b.s_id AND b.c_id=&#x27;01&#x27;</span><br><span class="line">LEFT JOIN score c ON a.s_id=c.s_id AND c.c_id=&#x27;02&#x27;</span><br><span class="line">WHERE b.s_score&gt;c.s_score;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676354704756.png" alt="img"></p>
<h2 id="18-HBase实验：HBase常用操作"><a href="#18-HBase实验：HBase常用操作" class="headerlink" title="18.HBase实验：HBase常用操作"></a>18.HBase实验：HBase常用操作</h2><blockquote>
<h3 id="目的-17"><a href="#目的-17" class="headerlink" title="目的"></a>目的</h3><p>1.学习了解HBase的有关知识。</p>
<h3 id="要求-17"><a href="#要求-17" class="headerlink" title="要求"></a>要求</h3><p>本实验要求学生能：<br>1.了解HBase 的数据模型和基本操作</p>
<h3 id="原理-17"><a href="#原理-17" class="headerlink" title="原理"></a>原理</h3><p>table: 表<br>columnFamily:列族，一个表下可以有多个列族，但是不建议设置多个列族，HBase建议设计长窄型的表而不是短宽型。<br>qualifier:列，一个列族下可以有多列，一个表中的列可以是不对齐的，但是这样效率不高，同一张表中的列最好是相同的。<br>cell:一列数据下的一个单元格，一个列下可以有多个单元格，根据版本号区分，默认每次读取最新版本的数据，cell下的存储是数据本身。<br>row: 行，多列数据组成一行，一行中有多个qualifier。<br>rowKey: 行健，用于唯一标识一行数据，一行下有多列，行健的设计直接关系到查询的效率。</p>
</blockquote>
<h3 id="4-1-准备环境"><a href="#4-1-准备环境" class="headerlink" title="4.1 准备环境"></a>4.1 准备环境</h3><p>查看进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430525396.png" alt="img"></p>
<p>停止yarn，并查看是否停止成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/</span><br><span class="line"># sbin/stop-yarn.sh</span><br><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430545671.png" alt="img"></p>
<p>查看从节点进程状态</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430557927.png" alt="img"></p>
<p>发现主节点yarn进程还在，使用kill命令强制停止，并查看进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kill -9 2767</span><br><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430573100.png" alt="img"></p>
<h3 id="4-2-修改mapred-site-xml配置文件"><a href="#4-2-修改mapred-site-xml配置文件" class="headerlink" title="4.2 修改mapred-site.xml配置文件"></a>4.2 修改mapred-site.xml配置文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd etc/hadoop</span><br><span class="line"># vim mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>将mapreduce.framework.name这一配置项设为本地local，如下图</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430587454.png" alt="img"></p>
<p>将修改的配置文件复制至从节点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># scp -r /usr/cstor/hadoop/etc/hadoop/mapred-site.xml slave1:/usr/cstor/hadoop/etc/hadoop/</span><br><span class="line"># scp -r /usr/cstor/hadoop/etc/hadoop/mapred-site.xml slave2:/usr/cstor/hadoop/etc/hadoop/</span><br></pre></td></tr></table></figure>

<p>最后我们启动yarn</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop</span><br><span class="line"># sbin/start-yarn.sh</span><br><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430604554.png" alt="img"></p>
<h3 id="4-3-连接到-HBase"><a href="#4-3-连接到-HBase" class="headerlink" title="4.3 连接到 HBase"></a>4.3 连接到 HBase</h3><p>在对 HBase 进行操作时，可以利用其提供的交互式命令行 HBase Shell 进行。<br>请在终端中输入以下命令进入 HBase Shell：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hbase shell</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430615907.png" alt="img"></p>
<h3 id="4-4-HBase-数据模型和操作"><a href="#4-4-HBase-数据模型和操作" class="headerlink" title="4.4 HBase 数据模型和操作"></a>4.4 HBase 数据模型和操作</h3><p>本节主要讲解连接到 HBase、表、行、列与列族、查看元数据、退出 HBase Shell 以及关闭 HBase 服务。</p>
<h3 id="4-4-1-表"><a href="#4-4-1-表" class="headerlink" title="4.4.1 表"></a>4.4.1 表</h3><p>在 HBase 中，数据存储在具有行和列的表中。这是一个与关系数据库（RDBMS）重叠的术语，但它们不是同一个概念。相反，可以将 HBase 表视作多维映射的结构。<br>关于表格式（Schema）的设计，可以参考下面这篇资料。建议在进行后续操作之前先对其有一个理论上的了解。<br>• Introduction to Basic Schema Design<br>创建表<br>HBase 的表（Table）由多个行组成。可以使用 create 命令创建一个新的表，创建时必须指定表名和列簇（ColumnFamily）的名称。<br>注意：hbase(main):001:0&gt; 是命令行提示符，在输入命令时不必输入它。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &#x27;test&#x27;, &#x27;cf&#x27;</span><br></pre></td></tr></table></figure>

<p>其中的 test 是表名，而 cf 命名了一个列族。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430653584.png" alt="img"></p>
<p>列出表<br>为了确认表的存在，可以使用 list 命令列出关于表的信息。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; list &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430676141.png" alt="img"></p>
<p>描述表信息<br>此外，还可以使用 describe 命令查看表的详细信息，包括配置的默认值。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):003:0&gt; describe &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430691764.png" alt="img"></p>
<p>禁用和启用表<br>如果想删除一个表或者更改它的设置，以及在其他一些情况下，都需要首先使用 disable 命令禁用该表。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):004:0&gt; disable &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430744416.png" alt="img"></p>
<p>修改完成之后，可以使用 enable 命令重新启用它。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):005:0&gt; enable &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430771114.png" alt="img"></p>
<p>删除表<br>如果要删除一个表，则使用 drop 命令。但删除的前提是禁用它。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):006:0&gt; create &#x27;aaa&#x27;, &#x27;cf&#x27; </span><br><span class="line">hbase(main):007:0&gt; disable &#x27;aaa&#x27; </span><br><span class="line">hbase(main):008:0&gt; drop &#x27;aaa&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430790984.png" alt="img"></p>
<h3 id="4-4-2-行"><a href="#4-4-2-行" class="headerlink" title="4.4.2 行"></a>4.4.2 行</h3><p>HBase 中的行（Row）由一个行键（Row Key）和一个或多个列（Column）组成，这些列的值与它们相关联。行在存储时按行键的字母顺序排序。因此，行键的设计非常重要。设计的目标是为了让各条记录以彼此相邻的方式进行存储。<br>在此处，仍然是建议查阅资料 Introduction to Basic Schema Design 的相关章节来了解其设计规范。<br>插入数据<br>如果要将数据插入到表中，需要使用 put 命令。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):010:0&gt; put &#x27;test&#x27;, &#x27;row1&#x27;, &#x27;cf:a&#x27;, &#x27;value1&#x27;</span><br></pre></td></tr></table></figure>

<p>这个语句插入了行键为 row1 、列族 a 下的值为 value1 的一条记录到 test 表中。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430822050.png" alt="img"></p>
<p>请在 HBase Shell 中输入以下语句继续插入数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):011:0&gt; put &#x27;test&#x27;, &#x27;row2&#x27;, &#x27;cf:b&#x27;, &#x27;value2&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430853728.png" alt="img"></p>
<p>请在 HBase Shell 中输入以下语句继续插入数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):012:0&gt; put &#x27;test&#x27;, &#x27;row3&#x27;, &#x27;cf:c&#x27;, &#x27;value3&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430875059.png" alt="img"></p>
<p>在这里，我们插入了三个值，每次插入了一个。HBase 中的列由列族前缀（本例中为 cf ）、冒号和列限定符后缀（本例中为 a ）组成。</p>
<p>扫描表中数据<br>从 HBase 获取数据的一种方法是扫描。可以使用 scan 命令扫描数据表。我们可以通过一些参数限制扫描的范围，如果不加参数的话，则会取走该表中所有的数据。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):013:0&gt; scan &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430936039.png" alt="img"></p>
<p>可以看到每个值旁边有一个时间戳（Timestamp），它是一个给定版本的值的标识符。默认情况下，时间戳表示写入数据时 RegionServer 上的时间，但是可以在将数据插入到单元格时指定不同时间戳的值。<br>获取一行数据<br>如果一次想要获取一行数据，则使用 get 命令。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):014:0&gt; get &#x27;test&#x27;, &#x27;row1&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430953979.png" alt="img"></p>
<p>删除数据（delete）<br>难免有数据插入不当的情况，可用delete命令删除：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):015:0&gt; delete &#x27;test&#x27;,&#x27;row1&#x27;,&#x27;cf:a&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430977992.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):016:0&gt; get &#x27;test&#x27;,&#x27;row1&#x27;,&#x27;cf:a&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676430993790.png" alt="img"></p>
<p>删除数据（deleteall）<br>delete这个方法只能删除具体到哪一行中的某个列族下的某一列数据，想要删除一整行数据，需用deleteall命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):017:0&gt; deleteall &#x27;test&#x27;,&#x27;row1&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431080472.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):018:0&gt; get &#x27;test&#x27;,&#x27;row1&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431103210.png" alt="img"></p>
<p>若需删除整张表的数据，可用truncate命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):019:0&gt; truncate &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431134079.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):020:0&gt; scan &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431155269.png" alt="img"></p>
<p>数据导入<br>vim编辑一个cstor.csv文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim cstor.csv</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431196530.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431207235.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431222359.png" alt="img"></p>
<p>将文件上传至hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -put cstor.csv /</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431441183.png" alt="img"></p>
<p>执行文件导入<br>格式：hbase [类] [分隔符] [行键，列族] [表] [导入文件] （默认分隔符为空格）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=&#x27;,&#x27; -Dimporttsv.columns=HBASE_ROW_KEY,cf test /cstor.csv</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431470831.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431488874.png" alt="img"></p>
<p>查看是否导入成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):005:0&gt; scan &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431507418.png" alt="img"></p>
<p>单元格（Cell）是行、列族和列限定符的组合，包含了值和时间戳，时间戳表示这个值的版本。也就是说，HBase 里可以记录同一个值的不同版本。</p>
<h3 id="4-4-3-列与列族"><a href="#4-4-3-列与列族" class="headerlink" title="4.4.3 列与列族"></a>4.4.3 列与列族</h3><p>HBase 中的一个列（Column）由一个列族和一个列限定符组成，它由一个 : （冒号）字符分隔。<br>列族（Column Family）出于性能的原因，在物理上连续地排列一组列及它的值。每个列族都有一组存储属性，比如是否应该将其值缓存在内存中、如何压缩数据或对其行键进行编码等等。表中的每一行都有相同的列族，尽管给定的行可能不会在给定的列族中存储任何数据。<br>列限定符<br>列族中的列限定符（Column Qualifier）用于提供指定数据块的索引。举个例子，如果指定一个列族，则其列限定符可能是 content:age，另一个可能是  content:address。虽然在表创建时就已经固定了列族，但列限定符是可变的，并且在不同的行之间可能有很大的差异。</p>
<h3 id="4-4-4-查看元数据"><a href="#4-4-4-查看元数据" class="headerlink" title="4.4.4 查看元数据"></a>4.4.4 查看元数据</h3><p>元数据表是 HBase 维护数据的一种方式，可以通过 list_namespace 命令查看 HBase 当前有哪些命名空间，即数据库。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):021:0&gt; list_namespace</span><br></pre></td></tr></table></figure>

<p>我们可以看到初始状态下，HBase 有两个数据库，一个是 default，另一个是 hbase 。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431528488.png" alt="img"></p>
<p>刚刚创建的 test 表都在 default 这个命名空间下，而元数据表在 hbase 下。尝试查看 hbase 下面都有哪些表。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):022:0&gt; list_namespace_tables &#x27;hbase&#x27;</span><br></pre></td></tr></table></figure>

<p>可以看到，目前一共有两个表，分别是 meta 表和 namespace 表。它们都是系统创建的表（System Table）。除此之外，我们自己创建的都是用户表（User Table）</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431549784.png" alt="img"></p>
<p>为了深入观察这些表所保存的信息，可以创建一个自定义的命名空间，并在该命名空间中创建一张新表，同时插入一些数据。<br>请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):023:0&gt; create_namespace &#x27;namespace_test&#x27; </span><br><span class="line">hbase(main):024:0&gt; create &#x27;namespace_test:test2&#x27;, &#x27;cf2&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431570911.png" alt="img"></p>
<p>使用以下语句插入一些数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):025:0&gt; put &#x27;namespace_test:test2&#x27;,&#x27;row1&#x27;,&#x27;cf2:id&#x27;, 101 </span><br><span class="line">hbase(main):026:0&gt; put &#x27;namespace_test:test2&#x27;,&#x27;row2&#x27;,&#x27;cf2:name&#x27;, &#x27;aaa&#x27; </span><br><span class="line">hbase(main):027:0&gt; put &#x27;namespace_test:test2&#x27;,&#x27;row3&#x27;,&#x27;cf2:age&#x27;, 18</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431597428.png" alt="img"></p>
<p>插入了数据整合，再次查看 namespace 下的内容。请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):028:0&gt; list_namespace</span><br></pre></td></tr></table></figure>

<p>可以看到已经有刚刚创建的命名空间（可理解为数据库）。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431620662.png" alt="img"></p>
<p>随后查看其中有哪些元信息，请在 HBase Shell 中输入以下语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):029:0&gt; scan &#x27;hbase:meta&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431636554.png" alt="img"></p>
<h3 id="4-4-5-退出-HBase-Shell"><a href="#4-4-5-退出-HBase-Shell" class="headerlink" title="4.4.5 退出 HBase Shell"></a>4.4.5 退出 HBase Shell</h3><p>以上就是 HBase 的一些基本使用技巧。当不再使用 HBase Shell 时，请使用以下命令来退出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):030:0&gt; quit</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676431651245.png" alt="img"></p>
<h2 id="19-HBase挑战：Hive整合HBase"><a href="#19-HBase挑战：Hive整合HBase" class="headerlink" title="19.HBase挑战：Hive整合HBase"></a>19.HBase挑战：Hive整合HBase</h2><blockquote>
<h3 id="目的-18"><a href="#目的-18" class="headerlink" title="目的"></a>目的</h3><p>1.实现Hive与HBase的通信</p>
<h3 id="要求-18"><a href="#要求-18" class="headerlink" title="要求"></a>要求</h3><p>1.在hive中建表通过hbase查看表，通过hbase插入数据，用hive查看表内的数据</p>
<h3 id="原理-18"><a href="#原理-18" class="headerlink" title="原理"></a>原理</h3><p>Hive与HBase整合的是利用两者本身对外的API接口互相通信来完成的，这种相互通信是通过$HIVE_HOME&#x2F;lib&#x2F;hive-hbase-handler-*.jar工具类实现的。通过HBaseStorageHandler，Hive可以获取到Hive表所对应的HBase表名，列簇和列，InputFormat、OutputFormat类，创建和删除HBase表等。</p>
</blockquote>
<h3 id="4-1参考之前课程部署好Hive与HBase"><a href="#4-1参考之前课程部署好Hive与HBase" class="headerlink" title="4.1参考之前课程部署好Hive与HBase"></a>4.1参考之前课程部署好Hive与HBase</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p>输入jps查看是否有Hive和HBase进程</p>
<h3 id="4-2连接hive与hbase"><a href="#4-2连接hive与hbase" class="headerlink" title="4.2连接hive与hbase"></a>4.2连接hive与hbase</h3><p>进入&#x2F;usr&#x2F;cstor&#x2F;hive&#x2F;lib目录将hive-hbase-handler-3.1.2.jar复制到各个节点hbase的lib目录下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># scp -r hive-hbase-handler-3.1.2.jar /usr/cstor/hbase/lib/</span><br><span class="line">  # scp -r hive-hbase-handler-3.1.2.jar slave1:/usr/cstor/hbase/lib/</span><br><span class="line">  # scp -r hive-hbase-handler-3.1.2.jar slave2:/usr/cstor/hbase/lib/</span><br></pre></td></tr></table></figure>

<p><img src="http://10.131.2.101/static/upload/resource/exp/ins/2bf3898384f248c7b504db58fc35d521/image/image.1676432408718.png" alt="img"></p>
<h3 id="4-3启动并进入hive与hbase"><a href="#4-3启动并进入hive与hbase" class="headerlink" title="4.3启动并进入hive与hbase"></a>4.3启动并进入hive与hbase</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hive</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432430347.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hbase shell</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432443374.png" alt="img"></p>
<h3 id="4-4测试hive与hbase是否成功通信"><a href="#4-4测试hive与hbase是否成功通信" class="headerlink" title="4.4测试hive与hbase是否成功通信"></a>4.4测试hive与hbase是否成功通信</h3><p>hive中建一张t_employee的表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Create table t_employee(id int,name string) stored by &#x27;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#x27; with serdeproperties(&#x27;hbase.columns.mapping&#x27;=&#x27;:key,st1:name&#x27;) tblproperties(&#x27;hbase.table.name&#x27;=&#x27;t_employee&#x27;,&#x27;hbase.mapred.output.outputtable&#x27; = &#x27;t_employee&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432468495.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show tables;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432528050.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe t_employee;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432543514.png" alt="img"></p>
<p>hbase中查看创建的表并插入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">list</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432573832.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">describe ‘t_employee’</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432602462.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">put &#x27;t_employee&#x27;,&#x27;1001&#x27;,&#x27;st1:name&#x27;,&#x27;zhaoqian&#x27;</span><br><span class="line">put &#x27;t_employee&#x27;,&#x27;1002&#x27;,&#x27;st1:name&#x27;,&#x27;sunli&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432622973.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scan ‘t_employee’</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432652505.png" alt="img"></p>
<p>hive中查看hbase中插入的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from t_employee;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432679006.png" alt="img"></p>
<h2 id="20-HBase挑战：HBase实现Web日志场景数据处理"><a href="#20-HBase挑战：HBase实现Web日志场景数据处理" class="headerlink" title="20.HBase挑战：HBase实现Web日志场景数据处理"></a>20.HBase挑战：HBase实现Web日志场景数据处理</h2><blockquote>
<h3 id="目的-19"><a href="#目的-19" class="headerlink" title="目的"></a>目的</h3><p>1.如果 HBase 中存在某网站的访问日志记录，那么我们可以通过对 HBase 的检索进行一些初步的分析，从而得到更加深度的访问信息。<br>2.在 HBase 中最常用的查询命令是 scan，即对某张表及其内部的行进行扫描。在扫描时常会用到过滤器来进行扫描条件上的约束。</p>
<h3 id="要求-19"><a href="#要求-19" class="headerlink" title="要求"></a>要求</h3><p>在此次挑战中，你需要将一个 CSV 文件导入到 HBase 的表中。<br>在以下目录中获取实验所需要的数据文件。</p>
<p>&#x2F;root&#x2F;data&#x2F;hbase&#x2F;weblog&#x2F;log.csv</p>
<p>将 CSV 文件导入到 HBase 的 access_log 表内。<br>请根据以上提示，利用过滤器检索出所有访问记录中，来自加拿大（即 country 字段的值是 ca）的记录有多少条。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432921110.png" alt="img"></p>
<p>将查询结果中的记录数量写入到 &#x2F;root&#x2F;log_analysis_result 文件中。<br>本挑战不限制查询和扫描的方式，但 HBase 中的表模式应当与提示中的描述相同。</p>
<h3 id="原理-19"><a href="#原理-19" class="headerlink" title="原理"></a>原理</h3><p>知识点<br>1.了解HBase 过滤器的用法<br>2.了解HBase 导入数据的方式<br>3.了解HBase 表操作<br>4.了解Bash 输出重定向<br>提示语<br>1.可供参考的 HBase 数据导入语句是：<br>$ hbase org.apache.hadoop.hbase.mapreduce.ImportTsv  -Dimporttsv.separator&#x3D;”分隔符”  -Dimporttsv.columns&#x3D;HBASE_ROW_KEY,列族名称:列名,列族名称:列名,列族名称:列名,列族名称:列名,列族名称:列名,列族名称:列名 表明 待导入文件的 HDFS 路径<br>2.可供参考的 HBase 中的 access_log 表模式为：<br>第 1 列：对应于 HBASE_ROW_KEY 。<br>第 2 至 3 列：对应于列族 cf1 ，对应的列名分别是 date 和 id 。<br>第 4 至 7 列：对应于列族 cf2 ，对应的列名分别是 url 、pre_url 、ip 、country 。<br>3.HBase 中创建表的命令为 create ‘表名’, ‘列族名称’, ‘列族名称’。<br>SingleColumnValueFilter 来对单个列的值进行限定，在 HBase Shell 中的使用方法如下：<br>scan ‘表名’, FILTER&#x3D;&gt;”SingleColumnValueFilter(‘列族名称’,’列名’,判断条件,’binary:匹配值’)”<br>4.将查询结果中的记录数量写入以下目录：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># echo 计数值 &gt; /root/log_analysis_result</span><br></pre></td></tr></table></figure>

<p>注意：<br>请务必先独立思考获得 PASS 之后再查看参考代码，直接拷贝代码收获不大</p>
</blockquote>
<h3 id="4-1-准备环境-1"><a href="#4-1-准备环境-1" class="headerlink" title="4.1 准备环境"></a>4.1 准备环境</h3><p>查看进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432960810.png" alt="img"></p>
<p>停止yarn，并查看是否停止成功</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/</span><br><span class="line"># sbin/stop-yarn.sh</span><br><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432972769.png" alt="img"></p>
<p>查看从节点进程状态</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676432981878.png" alt="img"></p>
<p>发现主节点yarn进程还在，使用kill命令强制停止，并查看进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># kill -9 2767</span><br><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433000840.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd etc/hadoop</span><br><span class="line"># vim mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>将mapreduce.framework.name这一配置项设为本地local，如下图</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433016208.png" alt="img"></p>
<p>将修改的配置文件复制至从节点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># scp -r /usr/cstor/hadoop/etc/hadoop/mapred-site.xml slave1:/usr/cstor/hadoop/etc/hadoop/</span><br><span class="line"># scp -r /usr/cstor/hadoop/etc/hadoop/mapred-site.xml slave2:/usr/cstor/hadoop/etc/hadoop/</span><br></pre></td></tr></table></figure>

<p>最后我们启动yarn</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop</span><br><span class="line"># sbin/start-yarn.sh</span><br><span class="line"># jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433039158.png" alt="img"></p>
<p>hdfs上创建目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir -p /user/hadoop/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433049215.png" alt="img"></p>
<p>将本地数据上传至hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -put /root/data/hbase/weblog/log.csv /user/hadoop/</span><br></pre></td></tr></table></figure>

<h3 id="4-2-数据处理"><a href="#4-2-数据处理" class="headerlink" title="4.2 数据处理"></a>4.2 数据处理</h3><p>进入shell</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hbase shell</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433080254.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):001:0&gt; create &#x27;access_log&#x27;, &#x27;cf1&#x27;, &#x27;cf2&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433100971.png" alt="img"></p>
<p>新开一个终端，在终端执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.separator=&#x27;,&#x27; -Dimporttsv.columns=HBASE_ROW_KEY,cf1:date,cf1:id,cf2:url,cf2:pre_url,cf2:ip,cf2:country access_log /user/hadoop/log.csv</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433125877.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433134186.png" alt="img"></p>
<p>切换到 hbase shell 中执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):002:0&gt; scan &#x27;access_log&#x27;, FILTER=&gt;&quot;SingleColumnValueFilter(&#x27;cf2&#x27;,&#x27;country&#x27;,=,&#x27;binary:ca&#x27;)&quot;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433150346.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase(main):003:0&gt; exit</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433161864.png" alt="img"></p>
<p>在终端执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># echo 1037 &gt; /root/log_analysis_result</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/image.1676433171344.png" alt="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">tong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/">http://example.com/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/./img/touxiang.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/./img/WeChat_pay.jpg" target="_blank"><img class="post-qr-code-img" src="/./img/WeChat_pay.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/./img/Alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/./img/Alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/03/12/Java/mooc%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E2%80%94%E2%80%94Java/" title="mooc面向对象程序设计——Java"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">mooc面向对象程序设计——Java</div></div><div class="info-2"><div class="info-item-1">mooc面向对象程序设计——Java 2023年11月13日 ~ 2024年01月31日	第14次开课	翁恺 第1周	类与对象1.1...</div></div></div></a><a class="pagination-related" href="/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/" title="Hadoop应用与开发2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hadoop应用与开发2</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发221.Spark实验：SparkSQL入门实战 目的1.学习spark的入门编程 要求2.掌握spark基础操作和原理 原理Spark SQLSpark SQL 是 Spark 用来处理结构化数据的模块，通过 Spark SQL 我们可以像之前使用 SQL 语句分析关系型数据库表一样方便地在  Spark 上对海量结构化数据进行快速分析，我们只需要关注数据分析的逻辑而不需要担心底层分布式存储、计算、通信、以及作业解析和调度的细节。 Spark SQL 支持从 JSON 文件、CSV 文件、Hive 表、Parquest 文件中读取数据，通过 SQL  语句对数据进行交互式查询；也可以读取传统关系型数据库中的数据进行数据分析。同时还能与传统的 RDD 编程相结合，让我们能够同时使用 SQL 和  RDD 进行复杂的数据分析。 另外，它还能与其它核心组件比如： Spark Streaming、MLlib、GraphX 等联合使用进行数据分析，所以 Spark SQL...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/" title="Hadoop应用与开发3"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-16</div><div class="info-item-2">Hadoop应用与开发3</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发342.Flink实验：Flink简介与配置 目的1.了解Flink特性、功能模块、编程模型、构架模型并学会Flink各种模式的环境部署。 要求本次试验后，要求学生能：1.深入了解Flink的特性、功能模块、编程模型、构架模型；2.掌握Flink各种模式的环境部署 原理3.1Flink介绍Flink起源于一个名为Stratosphere的研究项目，目的是建立下一代大数据分析平台，于2014年4月16日成为Apache孵化器项目。 Apache Flink是一个面向数据流处理和批量数据处理的可分布式的开源计算框架，它基于同一个Flink流式执行模型（streaming execution...</div></div></div></a><a class="pagination-related" href="/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/" title="Hadoop应用与开发2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-11</div><div class="info-item-2">Hadoop应用与开发2</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发221.Spark实验：SparkSQL入门实战 目的1.学习spark的入门编程 要求2.掌握spark基础操作和原理 原理Spark SQLSpark SQL 是 Spark 用来处理结构化数据的模块，通过 Spark SQL 我们可以像之前使用 SQL 语句分析关系型数据库表一样方便地在  Spark 上对海量结构化数据进行快速分析，我们只需要关注数据分析的逻辑而不需要担心底层分布式存储、计算、通信、以及作业解析和调度的细节。 Spark SQL 支持从 JSON 文件、CSV 文件、Hive 表、Parquest 文件中读取数据，通过 SQL  语句对数据进行交互式查询；也可以读取传统关系型数据库中的数据进行数据分析。同时还能与传统的 RDD 编程相结合，让我们能够同时使用 SQL 和  RDD 进行复杂的数据分析。 另外，它还能与其它核心组件比如： Spark Streaming、MLlib、GraphX 等联合使用进行数据分析，所以 Spark SQL...</div></div></div></a><a class="pagination-related" href="/2025/06/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%914/" title="Hadoop应用与开发4"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">Hadoop应用与开发4</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发459.Hadoop实验：HDFS基础与部署 目的1.理解大数据生态圈各组件特性2.理解HDFS存在的优势3.理解HDFS体系架构4.学会在环境中部署HDFS学会HDFS基本命令 要求1.要求实验结束时，能够构建出HDFS集群2.要求能够在构建出的HDFS集群上使用基本的HDFS命令3.要求能够大致了解Hadoop生态圈中各个组件 原理3.1 Hadoop 生态：Hadoop是一个提供高可靠，可扩展（横向）的分布式计算的开源软件平台。1.Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构。2.主要解决，海量数据的存储和海量数据的分析计算问题。3.我们现在讲HADOOP 通常是指一个更广泛的概念——HADOOP...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/./img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">tong</div><div class="author-info-description">这里是描述</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">现在没有公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91"><span class="toc-text">Hadoop应用与开发</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%9F%BA%E4%BA%8EMapReduce%E5%AE%9E%E7%8E%B0%E5%8D%95%E8%AF%8D%E8%AE%A1%E6%95%B0"><span class="toc-text">1.Hadoop实验：基于MapReduce实现单词计数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-MapReduce%E7%BC%96%E7%A8%8B"><span class="toc-text">3.1 MapReduce编程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Java-API%E8%A7%A3%E6%9E%90"><span class="toc-text">3.2 Java API解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%AE%9E%E9%AA%8C%E5%87%86%E5%A4%87"><span class="toc-text">4.1实验准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BC%96%E5%86%99MapReduce%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.2 编写MapReduce程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E4%BD%BF%E7%94%A8IDEA%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E5%B0%86%E8%AF%A5%E4%BB%A3%E7%A0%81%E6%89%93%E5%8C%85"><span class="toc-text">4.3使用IDEA开发工具将该代码打包</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9AHDFS%E5%8E%9F%E7%90%86%E4%B8%8E%E6%93%8D%E4%BD%9C"><span class="toc-text">2.Hadoop实验：HDFS原理与操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-1"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-1"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-1"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-HDFS-%E8%AF%BB%E6%93%8D%E4%BD%9C"><span class="toc-text">3.1 HDFS 读操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-HDFS-%E5%86%99%E6%93%8D%E4%BD%9C"><span class="toc-text">3.2 HDFS 写操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B1"><span class="toc-text">4.1 实验案例1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81"><span class="toc-text">4.1.1 程序代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%A1%88%E4%BE%8B1%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B"><span class="toc-text">4.2 案例1实现过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E5%88%9B%E5%BB%BA%E4%BB%A3%E7%A0%81%E7%9B%AE%E5%BD%95"><span class="toc-text">4.2.1 创建代码目录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E4%BF%AE%E6%94%B9%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-text">4.2.2 修改环境配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-%E5%BB%BA%E7%AB%8B%E4%BE%8B%E5%AD%90%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%88%B0-HDFS-%E4%B8%AD"><span class="toc-text">4.2.3 建立例子文件上传到 HDFS 中</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4-%E9%85%8D%E7%BD%AE%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83"><span class="toc-text">4.2.4 配置本地环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-5%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-text">4.2.5编写代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-6%E7%BC%96%E8%AF%91%E4%BB%A3%E7%A0%81"><span class="toc-text">4.2.6编译代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-7-%E4%BD%BF%E7%94%A8%E7%BC%96%E8%AF%91%E4%BB%A3%E7%A0%81%E8%AF%BB%E5%8F%96-HDFS-%E6%96%87%E4%BB%B6"><span class="toc-text">4.2.7 使用编译代码读取 HDFS 文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B2"><span class="toc-text">4.3 实验案例2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81"><span class="toc-text">4.3.1 程序代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%A1%88%E4%BE%8B2%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B"><span class="toc-text">4.4 案例2实现过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1-%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-text">4.4.1 编写代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2-%E7%BC%96%E8%AF%91%E4%BB%A3%E7%A0%81"><span class="toc-text">4.4.2 编译代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-3-%E5%BB%BA%E7%AB%8B%E6%B5%8B%E8%AF%95%E6%96%87%E4%BB%B6"><span class="toc-text">4.4.3 建立测试文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-4-%E4%BD%BF%E7%94%A8%E7%BC%96%E8%AF%91%E4%BB%A3%E7%A0%81%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E5%88%B0-HDFS"><span class="toc-text">4.4.4 使用编译代码上传文件内容到 HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-5-%E9%AA%8C%E8%AF%81%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F"><span class="toc-text">4.4.5 验证是否成功</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B3"><span class="toc-text">4.5实验案例3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-1-%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81"><span class="toc-text">4.5.1 程序代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E6%A1%88%E4%BE%8B3%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B"><span class="toc-text">4.6 案例3实现过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-1-%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-text">4.6.1 编写代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-2-%E7%BC%96%E8%AF%91%E4%BB%A3%E7%A0%81"><span class="toc-text">4.6.2 编译代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-3-%E5%BB%BA%E7%AB%8B%E6%B5%8B%E8%AF%95%E6%96%87%E4%BB%B6"><span class="toc-text">4.6.3 建立测试文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-4-%E4%BD%BF%E7%94%A8%E7%BC%96%E8%AF%91%E4%BB%A3%E7%A0%81%E6%8A%8A%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9%E4%BB%8E-HDFS-%E8%BE%93%E5%87%BA%E5%88%B0%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%AD"><span class="toc-text">4.6.4 使用编译代码把文件内容从 HDFS 输出到文件系统中</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-5-%E9%AA%8C%E8%AF%81%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F"><span class="toc-text">4.6.5 验证是否成功</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-eclipse%E4%B8%8AHadoop%E6%8F%92%E4%BB%B6%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="toc-text">4.7 eclipse上Hadoop插件的配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-1-%E9%85%8D%E7%BD%AE%E6%9C%AC%E6%9C%BA%E7%9A%84java%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-text">4.7.1 配置本机的java环境变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-2%E9%85%8D%E7%BD%AEHadoop%E6%8F%92%E4%BB%B6"><span class="toc-text">4.7.2配置Hadoop插件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-3%E9%AA%8C%E8%AF%81eclipse%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E8%A7%81Hadoop%E9%A1%B9%E7%9B%AE"><span class="toc-text">4.7.3验证eclipse是否可以见Hadoop项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-4%E4%BD%BF%E7%94%A8eclipse%E5%AF%BC%E5%87%BAjar%E5%8C%85"><span class="toc-text">4.7.4使用eclipse导出jar包</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%9F%BA%E4%BA%8EMapReduce%E5%AE%9E%E7%8E%B0Join%E6%93%8D%E4%BD%9C"><span class="toc-text">3.Hadoop实验：基于MapReduce实现Join操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-2"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-2"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-2"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E6%A6%82%E8%BF%B0"><span class="toc-text">3.1 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%8E%9F%E7%90%86"><span class="toc-text">3.2 原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%87%86%E5%A4%87%E9%98%B6%E6%AE%B5"><span class="toc-text">4.1 准备阶段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-text">4.1.1 程序分析执行过程如下：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.2 编写程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-MRJoin-java%E7%9A%84%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-text">4.2.1 MRJoin.java的完整代码如下：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%89%93%E5%8C%85%E5%B9%B6%E6%8F%90%E4%BA%A4"><span class="toc-text">4.3 打包并提交</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E6%89%A7%E8%A1%8C%E5%91%BD%E4%BB%A4"><span class="toc-text">4.3.1 执行命令</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%9F%BA%E4%BA%8EMapReduce%E5%AE%9E%E7%8E%B0%E8%AE%A1%E6%95%B0%E5%99%A8"><span class="toc-text">4.Hadoop实验：基于MapReduce实现计数器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-3"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-3"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-3"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1MapReduce%E8%AE%A1%E6%95%B0%E5%99%A8%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">3.1MapReduce计数器是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2MapReduce%E8%AE%A1%E6%95%B0%E5%99%A8%E8%83%BD%E5%81%9A%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">3.2MapReduce计数器能做什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3%E5%86%85%E7%BD%AE%E8%AE%A1%E6%95%B0%E5%99%A8"><span class="toc-text">3.3内置计数器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%A1%E6%95%B0%E5%99%A8"><span class="toc-text">3.4自定义计数器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%AE%9E%E9%AA%8C%E5%88%86%E6%9E%90%E8%AE%BE%E8%AE%A1"><span class="toc-text">4.1实验分析设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.2编写程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E6%89%93%E5%8C%85%E5%B9%B6%E6%8F%90%E4%BA%A4"><span class="toc-text">4.3打包并提交</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%9F%BA%E4%BA%8EMapReduce%E5%AE%9E%E7%8E%B0%E5%8E%BB%E9%87%8D%E5%90%88%E5%B9%B6"><span class="toc-text">5.Hadoop实验：基于MapReduce实现去重合并</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-4"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-4"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-4"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-text">3.1 数据准备</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.1 编写程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-text">4.1.1 示例代码如下：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%AF%BC%E5%87%BAJar%E5%8C%85"><span class="toc-text">4.2 导出Jar包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%BF%90%E8%A1%8CJar%E5%8C%85"><span class="toc-text">4.3 运行Jar包</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%9F%BA%E4%BA%8EMapReduce%E5%AE%9E%E7%8E%B0%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95"><span class="toc-text">6.Hadoop实验：基于MapReduce实现倒排索引</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-5"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-5"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-5"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.1编写程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E6%89%93%E5%8C%85%E5%B9%B6%E6%8F%90%E4%BA%A4"><span class="toc-text">4.2打包并提交</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E4%B8%8A%E4%BC%A0%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6"><span class="toc-text">4.3上传数据文件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9AHDFS%E5%9F%BA%E7%A1%80%E4%B8%8E%E9%83%A8%E7%BD%B2"><span class="toc-text">7.Hadoop实验：HDFS基础与部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-6"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-6"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-6"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Hadoop-%E7%94%9F%E6%80%81%EF%BC%9A"><span class="toc-text">3.1 Hadoop 生态：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-HDFS%E4%BC%98%E7%82%B9%EF%BC%9A"><span class="toc-text">3.2 HDFS优点：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%AE%BE%E7%BD%AE-Host-%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6"><span class="toc-text">4.1 设置 Host 映射文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%AE%BE%E7%BD%AE%E9%9B%86%E7%BE%A4%E8%8A%82%E7%82%B9%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="toc-text">4.2 设置集群节点免密登录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%AE%BE%E7%BD%AE%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83"><span class="toc-text">4.3 设置操作系统环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E8%AE%BE%E7%BD%AEJDK%E5%AE%89%E8%A3%85%E7%9B%AE%E5%BD%95"><span class="toc-text">4.3.1设置JDK安装目录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-%E9%85%8D%E7%BD%AEHDFS"><span class="toc-text">4.3.2 配置HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-HDFS%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%85%B3%E9%94%AE%E5%B1%9E%E6%80%A7%E4%BB%8B%E7%BB%8D"><span class="toc-text">4.3.3 HDFS配置文件关键属性介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-%E6%8B%B7%E8%B4%9D%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E8%87%B3%E5%85%B6%E5%AE%83%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-text">4.3.4 拷贝集群配置至其它服务器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%90%AF%E5%8A%A8Hadoop"><span class="toc-text">4.4 启动Hadoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E9%80%9A%E8%BF%87%E6%9F%A5%E7%9C%8B%E8%BF%9B%E7%A8%8B%E7%9A%84%E6%96%B9%E5%BC%8F%E9%AA%8C%E8%AF%81"><span class="toc-text">4.5 通过查看进程的方式验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-HDFS%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E6%A0%BC%E5%BC%8F%E5%A6%82%E4%B8%8B"><span class="toc-text">4.6 HDFS基本命令格式如下</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9AYARN%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E4%B8%8E%E4%BB%8B%E7%BB%8D"><span class="toc-text">8.Hadoop实验：YARN集群部署与介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-7"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-7"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-7"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-MapReduce-%E6%A6%82%E8%BF%B0"><span class="toc-text">3.1 MapReduce 概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1%E7%AE%80%E4%BB%8B"><span class="toc-text">3.1.1简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2MapReduce%E5%88%86%E8%80%8C%E6%B2%BB%E4%B9%8B%E6%80%9D%E6%83%B3"><span class="toc-text">3.1.2MapReduce分而治之思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3MapReduce%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%C2%B7%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-text">3.1.3MapReduce计算框架·执行流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4%E4%B8%A4%E4%B8%AA%E9%87%8D%E8%A6%81%E7%9A%84%E8%BF%9B%E7%A8%8B"><span class="toc-text">3.1.4两个重要的进程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-5MapReduce%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B1%9E%E6%80%A7%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.1.5MapReduce配置文件属性介绍</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-YARN%E6%A6%82%E8%BF%B0"><span class="toc-text">3.2 YARN概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-YARN%E8%BF%90%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-text">3.2.1 YARN运行流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-Yarn%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%B1%9E%E6%80%A7%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.2.2 Yarn配置文件属性介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%9C%A8master%E6%9C%BA%E4%B8%8A%E9%85%8D%E7%BD%AEYARN"><span class="toc-text">4.1 在master机上配置YARN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BB%9F%E4%B8%80%E5%90%AF%E5%8A%A8YARN"><span class="toc-text">4.2 统一启动YARN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E9%AA%8C%E8%AF%81YARN%E5%90%AF%E5%8A%A8%E6%88%90%E5%8A%9F"><span class="toc-text">4.3 验证YARN启动成功</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%9C%A8master%E6%9C%BA%E4%B8%8A%E6%8F%90%E4%BA%A4DistributedShell%E4%BB%BB%E5%8A%A1"><span class="toc-text">4.4 在master机上提交DistributedShell任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%9C%A8master%E6%9C%BA%E4%B8%8A%E6%8F%90%E4%BA%A4MapReduce%E5%9E%8B%E4%BB%BB%E5%8A%A1"><span class="toc-text">4.5 在master机上提交MapReduce型任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-1-%E6%8C%87%E5%AE%9A%E5%9C%A8YARN%E4%B8%8A%E8%BF%90%E8%A1%8CMapReduce%E4%BB%BB%E5%8A%A1"><span class="toc-text">4.5.1 指定在YARN上运行MapReduce任务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-2-%E5%9C%A8master%E7%AB%AF%E6%8F%90%E4%BA%A4PI-Estimator%E4%BB%BB%E5%8A%A1"><span class="toc-text">4.5.2 在master端提交PI Estimator任务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-Hadoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%9F%BA%E4%BA%8EMapReduce%E5%AE%9E%E7%8E%B0%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F"><span class="toc-text">9.Hadoop实验：基于MapReduce实现二次排序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-8"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-8"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-8"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-text">4.1准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F-1"><span class="toc-text">4.2编写程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E6%89%93%E5%8C%85%E6%8F%90%E4%BA%A4"><span class="toc-text">4.3打包提交</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-Hive%E5%AE%9E%E9%AA%8C%EF%BC%9AHive%E7%BB%84%E4%BB%B6%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%83%A8%E7%BD%B2"><span class="toc-text">10.Hive实验：Hive组件介绍与部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-9"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-9"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-9"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1Hive%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.1Hive介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2Hive%E4%B8%8E%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">3.2Hive与关系数据库的区别</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3Hive%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-text">3.3Hive的优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1%E4%BC%98%E7%82%B9"><span class="toc-text">3.3.1优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2%E7%BC%BA%E7%82%B9"><span class="toc-text">3.3.2缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4Hive-%E6%9E%B6%E6%9E%84"><span class="toc-text">3.4Hive 架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%BB%84%E4%BB%B6"><span class="toc-text">3.4.1服务端组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%BB%84%E4%BB%B6"><span class="toc-text">3.4.2客户端组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E8%AE%BE%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-text">4.1设置环境变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E9%85%8D%E7%BD%AEhive%E6%96%87%E4%BB%B6"><span class="toc-text">4.2配置hive文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93%E9%85%8D%E7%BD%AE"><span class="toc-text">4.3元数据库配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E8%AE%BE%E7%BD%AE-Hive-%E4%BD%93%E7%B3%BB%E5%8F%82%E6%95%B0"><span class="toc-text">4.4设置 Hive 体系参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E5%88%9D%E5%A7%8B%E5%8C%96%E5%85%83%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">4.5初始化元数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6%E5%90%AF%E5%8A%A8Hive"><span class="toc-text">4.6启动Hive</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-Hive%E5%AE%9E%E9%AA%8C%EF%BC%9AHive%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C"><span class="toc-text">11.Hive实验：Hive基础操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-10"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-10"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-10"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8hive"><span class="toc-text">4.1启动hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%BB%BA%E8%A1%A8%EF%BC%88CREATE%EF%BC%89%E7%9A%84%E8%AF%AD%E6%B3%95%E5%A6%82%E4%B8%8B"><span class="toc-text">4.2建表（CREATE）的语法如下</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%BB%BA%E8%A1%A8%EF%BC%88CREATE%EF%BC%89"><span class="toc-text">4.3建表（CREATE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E5%88%9B%E5%BB%BA%E6%99%AE%E9%80%9A%E8%A1%A8"><span class="toc-text">4.3.1创建普通表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E5%88%9B%E5%BB%BA%E5%A4%96%E9%83%A8%E8%A1%A8"><span class="toc-text">4.3.2创建外部表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-Hive%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="toc-text">4.3.3 Hive分区表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-%E5%88%9B%E5%BB%BA-Bucket-%E8%A1%A8"><span class="toc-text">4.3.4 创建 Bucket 表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E4%BF%AE%E6%94%B9%E8%A1%A8%E7%BB%93%E6%9E%84"><span class="toc-text">4.4修改表结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E7%AE%97%E6%9C%AF%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-text">4.5算术运算符</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6%E5%B8%B8%E7%94%A8%E7%9A%84%E5%9F%BA%E7%A1%80%E5%87%BD%E6%95%B0"><span class="toc-text">4.6常用的基础函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95"><span class="toc-text">4.7常用语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-1LIMIT%E8%AF%AD%E6%B3%95"><span class="toc-text">4.7.1LIMIT语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-2WHERE%E8%AF%AD%E5%8F%A5"><span class="toc-text">4.7.2WHERE语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-3GROUP-BY-%E8%AF%AD%E5%8F%A5"><span class="toc-text">4.7.3GROUP BY 语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-4HANING%E5%AD%90%E5%8F%A5"><span class="toc-text">4.7.4HANING子句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-5JOIN%E8%AF%AD%E5%8F%A5"><span class="toc-text">4.7.5JOIN语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8%E6%8E%92%E5%BA%8F%E8%AF%AD%E5%8F%A5"><span class="toc-text">4.8排序语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-1Order-by%E8%AF%AD%E5%8F%A5"><span class="toc-text">4.8.1Order by语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-2-Distribute-By%E8%AF%AD%E5%8F%A5"><span class="toc-text">4.8.2 Distribute By语句</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-3-Cluster-By%E8%AF%AD%E5%8F%A5"><span class="toc-text">4.8.3 Cluster By语句</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-Hive%E5%AE%9E%E9%AA%8C%EF%BC%9Abeeline%E5%85%A5%E9%97%A8"><span class="toc-text">12.Hive实验：beeline入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-11"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-11"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-11"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8hive-1"><span class="toc-text">4.1启动hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">4.2创建数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-text">4.3创建表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87"><span class="toc-text">4.3.1导入数据前的准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.2导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.3查询数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E5%90%AF%E5%8A%A8hiveserver2"><span class="toc-text">4.4启动hiveserver2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1%E5%90%AF%E5%8A%A8hiveserver2"><span class="toc-text">4.4.1启动hiveserver2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">4.4.2连接数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-3%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">4.4.3查询数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E4%BD%BF%E7%94%A8java-api%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">4.5使用java api查询数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#13-Hive%E6%8C%91%E6%88%98%EF%BC%9AHive%E5%AE%9E%E7%8E%B0%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1"><span class="toc-text">13.Hive挑战：Hive实现单词统计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-12"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-12"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-12"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE-1"><span class="toc-text">4.1准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%8D%95%E8%AF%8D%E7%BB%9F%E8%AE%A1"><span class="toc-text">4.2单词统计</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#14-Hive%E5%AE%9E%E9%AA%8C%EF%BC%9A%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E5%92%8C%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="toc-text">14.Hive实验：自定义函数和窗口函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-13"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-13"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-13"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-text">3.1 内置函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E5%88%86%E7%B1%BB"><span class="toc-text">3.1.1自定义函数分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E6%B5%81%E7%A8%8B"><span class="toc-text">3.1.2 自定义函数流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="toc-text">3.2 窗口函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-Hive%E7%9A%84%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0%E5%8A%9F%E8%83%BD"><span class="toc-text">3.2.1 Hive的窗口函数功能</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1Hive%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-text">4.1Hive的内置函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-text">4.2 Hive自定义函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1%E4%BD%BF%E7%94%A8eclipse%E6%89%93jar%E5%8C%85"><span class="toc-text">4.2.1使用eclipse打jar包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%A0%BC%E5%BC%8F%E4%B8%B4%E6%97%B6%E5%87%BD%E6%95%B0%EF%BC%8C%E6%B0%B8%E4%B9%85%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-text">4.2.2创建自定义函数有两种格式临时函数，永久函数：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3Hive-%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0"><span class="toc-text">4.3Hive 窗口函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-Hive%E4%B8%AD%E5%88%9B%E5%BB%BA%E8%A1%A8"><span class="toc-text">4.3.1.Hive中创建表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-%E7%AA%97%E5%8F%A3%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-text">4.3.2.窗口聚合函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#count%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">count开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sum%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">sum开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#avg%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">avg开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#min%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">min开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#max%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">max开窗函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3%E7%AA%97%E5%8F%A3%E5%88%86%E6%9E%90%E5%87%BD%E6%95%B0"><span class="toc-text">4.3.3窗口分析函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#first-value%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">first_value开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#last-value%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">last_value开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lag%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">lag开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lead%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">lead开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cume-dist%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">cume_dist开窗函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4%E7%AA%97%E5%8F%A3%E6%8E%92%E5%BA%8F%E5%87%BD%E6%95%B0"><span class="toc-text">4.3.4窗口排序函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#rank%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">rank开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dense-rank%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">dense_rank开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ntile%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">ntile开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#row-number%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">row_number开窗函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#percent-rank%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0"><span class="toc-text">percent_rank开窗函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#15-Hive%E6%8C%91%E6%88%98%EF%BC%9AHQL%E5%9F%BA%E7%A1%80%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5"><span class="toc-text">15.Hive挑战：HQL基础查询语句</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-14"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-14"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-14"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE-2"><span class="toc-text">4.1准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-student-txt"><span class="toc-text">1.student.txt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-course-txt"><span class="toc-text">2.course.txt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-teacher-txt"><span class="toc-text">3.teacher.txt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-score-txt"><span class="toc-text">4.score.txt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1%E5%BB%BA%E8%A1%A8"><span class="toc-text">4.2.1建表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2%E5%8A%A0%E8%BD%BD%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%E5%88%B0hive%E8%A1%A8"><span class="toc-text">4.2.2加载本地数据到hive表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3HQL%E8%AF%AD%E5%8F%A5%E7%BB%83%E4%B9%A0"><span class="toc-text">4.3HQL语句练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#16-HBase%E5%AE%9E%E9%AA%8C%EF%BC%9AHBase%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE"><span class="toc-text">16.HBase实验：HBase简介与安装配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-15"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-15"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-15"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1HBase-%E6%A6%82%E8%BF%B0"><span class="toc-text">3.1HBase 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2HBase-%E5%8E%86%E5%8F%B2"><span class="toc-text">3.2HBase 历史</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3HBase-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.3HBase 数据模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4HBase-%E6%9E%B6%E6%9E%84"><span class="toc-text">3.4HBase 架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1Client"><span class="toc-text">3.4.1Client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2Zookeeper"><span class="toc-text">3.4.2Zookeeper</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3HMaster"><span class="toc-text">3.4.3HMaster</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-4HRegionServer"><span class="toc-text">3.4.4HRegionServer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5HBase-%E8%AE%BF%E9%97%AE%E6%8E%A5%E5%8F%A3"><span class="toc-text">3.5HBase 访问接口</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6HBase-%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F"><span class="toc-text">3.6HBase 存储格式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-1HFile"><span class="toc-text">3.6.1HFile</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7HBase-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">3.7HBase 应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-1HBase-%E7%9A%84%E4%BC%98%E5%8A%BF%E4%B8%BB%E8%A6%81%E5%9C%A8%E4%BB%A5%E4%B8%8B%E5%87%A0%E6%96%B9%E9%9D%A2%EF%BC%9A"><span class="toc-text">3.7.1HBase 的优势主要在以下几方面：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-2%E5%B8%B8%E8%A7%81%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9A"><span class="toc-text">3.7.2常见的应用场景：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E7%BC%96%E8%BE%91-hbase-env-sh"><span class="toc-text">4.1编辑 hbase-env.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E7%BC%96%E8%BE%91-hbase-site-xml"><span class="toc-text">4.2编辑 hbase-site.xml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%90%AF%E5%8A%A8-HBase"><span class="toc-text">4.3启动 HBase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E9%AA%8C%E8%AF%81%E5%90%AF%E5%8A%A8"><span class="toc-text">4.4验证启动</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#17-Hive%E6%8C%91%E6%88%98%EF%BC%9AHQL%E5%A4%8D%E6%9D%82%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5"><span class="toc-text">17.Hive挑战：HQL复杂查询语句</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-16"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-16"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-16"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE-3"><span class="toc-text">4.1准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-student-txt-1"><span class="toc-text">1.student.txt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-course-txt-1"><span class="toc-text">2.course.txt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-teacher-txt-1"><span class="toc-text">3.teacher.txt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-score-txt-1"><span class="toc-text">4.score.txt</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE-1"><span class="toc-text">4.2导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1%E5%BB%BA%E8%A1%A8-1"><span class="toc-text">4.2.1建表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2%E5%8A%A0%E8%BD%BD%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%E5%88%B0hive%E8%A1%A8-1"><span class="toc-text">4.2.2加载本地数据到hive表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3HQL%E8%AF%AD%E5%8F%A5%E7%BB%83%E4%B9%A0-1"><span class="toc-text">4.3HQL语句练习</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#18-HBase%E5%AE%9E%E9%AA%8C%EF%BC%9AHBase%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C"><span class="toc-text">18.HBase实验：HBase常用操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-17"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-17"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-17"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%87%86%E5%A4%87%E7%8E%AF%E5%A2%83"><span class="toc-text">4.1 准备环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%BF%AE%E6%94%B9mapred-site-xml%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">4.2 修改mapred-site.xml配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%BF%9E%E6%8E%A5%E5%88%B0-HBase"><span class="toc-text">4.3 连接到 HBase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-HBase-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%93%8D%E4%BD%9C"><span class="toc-text">4.4 HBase 数据模型和操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1-%E8%A1%A8"><span class="toc-text">4.4.1 表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2-%E8%A1%8C"><span class="toc-text">4.4.2 行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-3-%E5%88%97%E4%B8%8E%E5%88%97%E6%97%8F"><span class="toc-text">4.4.3 列与列族</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-4-%E6%9F%A5%E7%9C%8B%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-text">4.4.4 查看元数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-5-%E9%80%80%E5%87%BA-HBase-Shell"><span class="toc-text">4.4.5 退出 HBase Shell</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#19-HBase%E6%8C%91%E6%88%98%EF%BC%9AHive%E6%95%B4%E5%90%88HBase"><span class="toc-text">19.HBase挑战：Hive整合HBase</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-18"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-18"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-18"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%8F%82%E8%80%83%E4%B9%8B%E5%89%8D%E8%AF%BE%E7%A8%8B%E9%83%A8%E7%BD%B2%E5%A5%BDHive%E4%B8%8EHBase"><span class="toc-text">4.1参考之前课程部署好Hive与HBase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E8%BF%9E%E6%8E%A5hive%E4%B8%8Ehbase"><span class="toc-text">4.2连接hive与hbase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%90%AF%E5%8A%A8%E5%B9%B6%E8%BF%9B%E5%85%A5hive%E4%B8%8Ehbase"><span class="toc-text">4.3启动并进入hive与hbase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E6%B5%8B%E8%AF%95hive%E4%B8%8Ehbase%E6%98%AF%E5%90%A6%E6%88%90%E5%8A%9F%E9%80%9A%E4%BF%A1"><span class="toc-text">4.4测试hive与hbase是否成功通信</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20-HBase%E6%8C%91%E6%88%98%EF%BC%9AHBase%E5%AE%9E%E7%8E%B0Web%E6%97%A5%E5%BF%97%E5%9C%BA%E6%99%AF%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-text">20.HBase挑战：HBase实现Web日志场景数据处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-19"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-19"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-19"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%87%86%E5%A4%87%E7%8E%AF%E5%A2%83-1"><span class="toc-text">4.1 准备环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-text">4.2 数据处理</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%914/" title="Hadoop应用与开发4">Hadoop应用与开发4</a><time datetime="2025-06-16T04:00:00.000Z" title="发表于 2025-06-16 12:00:00">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/" title="Hadoop应用与开发3">Hadoop应用与开发3</a><time datetime="2025-05-16T04:00:00.000Z" title="发表于 2025-05-16 12:00:00">2025-05-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/" title="Hadoop应用与开发2">Hadoop应用与开发2</a><time datetime="2025-05-11T04:00:00.000Z" title="发表于 2025-05-11 12:00:00">2025-05-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/" title="Hadoop应用与开发1">Hadoop应用与开发1</a><time datetime="2025-04-20T04:00:00.000Z" title="发表于 2025-04-20 12:00:00">2025-04-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/12/Java/mooc%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E2%80%94%E2%80%94Java/" title="mooc面向对象程序设计——Java">mooc面向对象程序设计——Java</a><time datetime="2025-03-12T10:22:00.000Z" title="发表于 2025-03-12 18:22:00">2025-03-12</time></div></div></div></div></div></div></main><footer id="footer" style="background: linear-gradient(20deg, #ffd6e0, #f5f5f5, #c1f0c1);"><div id="footer-wrap"><div class="copyright">&copy;2025 By tong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>