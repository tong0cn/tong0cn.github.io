<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Hadoop应用与开发3 | Blog</title><meta name="author" content="tong"><meta name="copyright" content="tong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Hadoop应用与开发342.Flink实验：Flink简介与配置 目的1.了解Flink特性、功能模块、编程模型、构架模型并学会Flink各种模式的环境部署。 要求本次试验后，要求学生能：1.深入了解Flink的特性、功能模块、编程模型、构架模型；2.掌握Flink各种模式的环境部署 原理3.1Flink介绍Flink起源于一个名为Stratosphere的研究项目，目的是建立下一代大数据分析平">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop应用与开发3">
<meta property="og:url" content="http://example.com/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="Hadoop应用与开发342.Flink实验：Flink简介与配置 目的1.了解Flink特性、功能模块、编程模型、构架模型并学会Flink各种模式的环境部署。 要求本次试验后，要求学生能：1.深入了解Flink的特性、功能模块、编程模型、构架模型；2.掌握Flink各种模式的环境部署 原理3.1Flink介绍Flink起源于一个名为Stratosphere的研究项目，目的是建立下一代大数据分析平">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/touxiang.png">
<meta property="article:published_time" content="2025-05-16T04:00:00.000Z">
<meta property="article:modified_time" content="2025-10-31T08:02:09.190Z">
<meta property="article:author" content="tong">
<meta property="article:tag" content="hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/touxiang.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hadoop应用与开发3",
  "url": "http://example.com/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/",
  "image": "http://example.com/img/touxiang.png",
  "datePublished": "2025-05-16T04:00:00.000Z",
  "dateModified": "2025-10-31T08:02:09.190Z",
  "author": [
    {
      "@type": "Person",
      "name": "tong",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/./img/favicon1.ico"><link rel="canonical" href="http://example.com/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop应用与开发3',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(https://i.loli.net/2019/09/09/5oDRkWVKctx2b6A.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/./img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Hadoop应用与开发3</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Hadoop应用与开发3</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-16T04:00:00.000Z" title="发表于 2025-05-16 12:00:00">2025-05-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T08:02:09.190Z" title="更新于 2025-10-31 16:02:09">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="Hadoop应用与开发3"><a href="#Hadoop应用与开发3" class="headerlink" title="Hadoop应用与开发3"></a>Hadoop应用与开发3</h1><h2 id="42-Flink实验：Flink简介与配置"><a href="#42-Flink实验：Flink简介与配置" class="headerlink" title="42.Flink实验：Flink简介与配置"></a>42.Flink实验：Flink简介与配置</h2><blockquote>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>1.了解Flink特性、功能模块、编程模型、构架模型并学会Flink各种模式的环境部署。</p>
<h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能：<br>1.深入了解Flink的特性、功能模块、编程模型、构架模型；<br>2.掌握Flink各种模式的环境部署</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1Flink介绍"><a href="#3-1Flink介绍" class="headerlink" title="3.1Flink介绍"></a>3.1Flink介绍</h2><p>Flink起源于一个名为Stratosphere的研究项目，目的是建立下一代大数据分析平台，于2014年4月16日成为Apache孵化器项目。</p>
<p>Apache Flink是一个面向数据流处理和批量数据处理的可分布式的开源计算框架，它基于同一个Flink流式执行模型（streaming execution model），能够支持流处理和批处理两种应用类型。</p>
<p>由于流处理和批处理所提供的SLA(服务等级协议)是完全不相同。流处理一般需要支持低延迟、Exactly-once保证；而批处理需要支持高吞吐、高效处理。所以在实现的时候通常是分别给出两套实现方法，或者通过一个独立的开源框架来实现其中每一种处理方案。</p>
<p>比较典型的有：实现批处理的开源方案有MapReduce、Spark；实现流处理的开源方案有Storm；Spark的Streaming 其实本质上也是微批处理。</p>
<p>Flink在实现流处理和批处理时，与传统的一些方案完全不同，它从另一个视角看待流处理和批处理，将二者统一起来：Flink是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。</p>
<h2 id="3-2Flink特性"><a href="#3-2Flink特性" class="headerlink" title="3.2Flink特性"></a>3.2Flink特性</h2><p>(1)有状态计算的Exactly-once语义。状态是指flink能够维护数据在时序上的聚类和聚合，同时它的checkpoint机制。<br>(2)支持带有事件时间（event time）语义的流处理和窗口处理。事件时间的语义使流计算的结果更加精确，尤其在事件到达无序或者延迟的情况下。<br>(3)支持高度灵活的窗口（window）操作。支持基于time、count、session，以及data-driven的窗口操作，能很好的对现实环境中的创建的数据进行建模。<br>(4)轻量的容错处理（ fault tolerance）。 它使得系统既能保持高的吞吐率又能保证exactly-once的一致性。通过轻量的state snapshots实现。<br>(5)支持高吞吐、低延迟、高性能的流处理。<br>(6)支持savepoints 机制（一般手动触发）。即可以将应用的运行状态保存下来；在升级应用或者处理历史数据是能够做到无状态丢失和最小停机时间。<br>(7)支持大规模的集群模式，支持yarn、Mesos。可运行在成千上万的节点上。<br>(8)支持具有Backpressure功能的持续流模型。<br>(9)Flink在JVM内部实现了自己的内存管理。<br>(10)支持迭代计算。<br>(11)支持程序自动优化：避免特定情况下Shuffle、排序等昂贵操作，中间结果进行缓存。</p>
<h2 id="3-3Flink功能模块"><a href="#3-3Flink功能模块" class="headerlink" title="3.3Flink功能模块"></a>3.3Flink功能模块</h2><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797459489.png" alt="img"></p>
<p>(1)Deployment层： 该层主要涉及了Flink的部署模式，Flink支持多种部署模式：本地、集群（Standalone&#x2F;YARN），（GCE&#x2F;EC2）。<br>(2)Runtime层：Runtime层提供了支持Flink计算的全部核心实现，比如：支持分布式Stream处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务。<br>(3)API层： 主要实现了面向无界Stream的流处理和面向Batch的批处理API，其中面向流处理对应DataStream API，面向批处理对应DataSet API。<br>(4)Libraries层：该层也可以称为Flink应用框架层，根据API层的划分，在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类。面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作）；面向批处理支持：FlinkML（机器学习库）、Gelly（图处理）</p>
<h2 id="3-4Flink编程模型"><a href="#3-4Flink编程模型" class="headerlink" title="3.4Flink编程模型"></a>3.4Flink编程模型</h2><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797466777.png" alt="img"></p>
<p>(1)有状态的数据流处理层。最底层的抽象仅仅提供有状态的数据流，它通过处理函数（Process  Function）嵌入到数据流api(DataStream API).  用户可以通过它自由的处理单流或者多流，并保持一致性和容错。同时用户可以注册事件时间和处理时间的回调处理，以实现复杂的计算逻辑。<br>(2)核心API层。 它提供了数据处理的基础模块，像各种transformation, join,aggregations,windows,stat 以及数据类型等等。<br>(3)Table API层。 定了围绕关系表的DSL(领域描述语言)。Table  API遵循了关系模型的标准：Table类型关系型数据库中的表，API也提供了相应的操作，像select, project, join,  group-by, aggregate等。Table API声明式的定义了逻辑上的操作（logical operation）不是code for the operation；Flink会对Table API逻辑在执行前进行优化。同时代码上，Flink允许混合使用Table  API和DataStram&#x2F;DataSet API。<br>(4)SQL层。 它很类似Table API的语法和表达，也是定义与Table API层次之上的，但是提供的是纯SQL的查询表达式</p>
<h2 id="3-5Flink架构模型图"><a href="#3-5Flink架构模型图" class="headerlink" title="3.5Flink架构模型图"></a>3.5Flink架构模型图</h2><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797474482.png" alt="img"></p>
<p>(1)Client：Flink 作业在哪台机器上面提交，那么当前机器称之为Client。用户开发的Program代码，它会构建出DataFlow graph，然后通过Client提交给JobManager。<br>(2)JobManager：是主（master）节点，相当于YARN里面的ResourceManager，生成环境中一般可以做HA高可用。JobManager会将任务进行拆分，调度到TaskManager上面执行。<br>(3)TaskManager：是从节点（slave），TaskManager才是真正实现task的部分。Client提交作业到JobManager，就需要跟JobManager进行通信，它使用Akka框架或者库进行通信，另外Client与JobManager进行数据交互，使用的是Netty框架。Akka通信基于ActorSystem，Client可以向JobManager发送指令，比如Submit job或者Cancel &#x2F;update job。JobManager也可以反馈信息给Client，比如status  updates，Statistics和results。</p>
<p>Client提交给JobManager的是一个Job，然后JobManager将Job拆分成task，提交给TaskManager（worker）。JobManager与TaskManager也是基于Akka进行通信，JobManager发送指令，比如Deploy&#x2F;Stop&#x2F;Cancel Tasks或者触发Checkpoint，反过来TaskManager也会跟JobManager通信返回Task  Status，Heartbeat（心跳），Statistics等。另外TaskManager之间的数据通过网络进行传输，比如Data  Stream做一些算子的操作，数据往往需要在TaskManager之间做数据传输。</p>
<p>当Flink系统启动时，首先启动JobManager和一至多个TaskManager。JobManager负责协调Flink系统，TaskManager则是执行并行程序的worker。当系统以本地形式启动时，一个JobManager和一个TaskManager会启动在同一个JVM中。当一个程序被提交后，系统会创建一个Client来进行预处理，将程序转变成一个并行数据流的形式，交给JobManager和TaskManager执行。</p>
</blockquote>
<h3 id="4-1Flink的local模式部署"><a href="#4-1Flink的local模式部署" class="headerlink" title="4.1Flink的local模式部署"></a>4.1Flink的local模式部署</h3><p>local模式，适用于测试调试，不需要启动任何的进程，仅仅是使用本地线程来模拟flink的进程，适用于测试开发调试等，这种模式下，不用更改任何配置，只需要保证jdk8安装正常即可。flink在处于local模式下，不需要更改任何配置，直接解压之后启动即可执行以下命令</p>
<h3 id="4-1-1直接启动local模式"><a href="#4-1-1直接启动local模式" class="headerlink" title="4.1.1直接启动local模式"></a>4.1.1直接启动local模式</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink</span><br><span class="line"># bin/start-cluster.sh               # 启动 Flink</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797485952.png" alt="img"></p>
<h3 id="4-1-2查看进程jps"><a href="#4-1-2查看进程jps" class="headerlink" title="4.1.2查看进程jps"></a>4.1.2查看进程jps</h3><p>可以看到StandaloneSessionClusterEntrypoint进程即JobManager，TaskManagerRunner 即TaskManager</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797493519.png" alt="img"></p>
<p>访问master:8081，确认监控界面正常启动，Web监控界面会显示有一个可用的TaskManager实例。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797499400.png" alt="img"></p>
<h3 id="4-1-3验证"><a href="#4-1-3验证" class="headerlink" title="4.1.3验证"></a>4.1.3验证</h3><p>本文将验证官方给出的SocketWindowWordCount代码，代码将从socket监听中读取数据，并且每5秒打印一次前5秒内每个不同单词的出现次数，即使用windos的滚动时间窗口。</p>
<p>SocketWindowWordCount的scala源码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">object SocketWindowWordCount &#123;</span><br><span class="line">    def main(args: Array[String]) : Unit = &#123;</span><br><span class="line">        // the port to connect to</span><br><span class="line">        val port: Int = try &#123;</span><br><span class="line">            ParameterTool.fromArgs(args).getInt(&#x27;port&#x27;)</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">            case e: Exception =&gt; &#123;</span><br><span class="line">                System.err.println(&#x27;No port specified. Please run &#x27;SocketWindowWordCount --port &lt;port&gt;&#x27;&#x27;)</span><br><span class="line">                return</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        // get the execution environment</span><br><span class="line">        val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">        // get input data by connecting to the socket</span><br><span class="line">        val text = env.socketTextStream(&#x27;localhost&#x27;, port, &#x27;\n&#x27;)</span><br><span class="line">        // parse the data, group it, window it, and aggregate the counts</span><br><span class="line">        val windowCounts = text</span><br><span class="line">            .flatMap &#123; w =&gt; w.split(&#x27;\\s&#x27;) &#125;</span><br><span class="line">            .map &#123; w =&gt; WordWithCount(w, 1) &#125;</span><br><span class="line">            .keyBy(&#x27;word&#x27;)</span><br><span class="line">            .timeWindow(Time.seconds(5), Time.seconds(1))</span><br><span class="line">            .sum(&#x27;count&#x27;)</span><br><span class="line">        // print the results with a single thread, rather than in parallel</span><br><span class="line">        windowCounts.print().setParallelism(1) </span><br><span class="line">        env.execute(&#x27;Socket Window WordCount&#x27;)</span><br><span class="line">    &#125;</span><br><span class="line">    // Data type for words with count</span><br><span class="line">    case class WordWithCount(word: String, count: Long)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>( 1 )netcat来启动本地服务器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># nc -lk 9000</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797510600.png" alt="img"></p>
<p>( 2 )提交Flink计划：</p>
<p>再打开一个master终端，提交Flink计划</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink</span><br><span class="line"># bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797517854.png" alt="img"></p>
<p>( 3 )查看计算结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tail -f log/flink-*-taskexecutor-*.out</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797523868.png" alt="img"></p>
<h3 id="4-1-4关闭单节点flink"><a href="#4-1-4关闭单节点flink" class="headerlink" title="4.1.4关闭单节点flink"></a>4.1.4关闭单节点flink</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./bin/stop-cluster.sh                  # 关闭 Flink</span><br></pre></td></tr></table></figure>

<h3 id="4-2Flink的Standalone模式部署"><a href="#4-2Flink的Standalone模式部署" class="headerlink" title="4.2Flink的Standalone模式部署"></a>4.2Flink的Standalone模式部署</h3><p>Flink自带了集群模式Standalone，主要是将资源调度管理交给flink集群自己来处理，standAlone是一种集群模式，可以有一个或者多个主节点JobManager(HA模式)，用于资源管理调度，任务管理，任务划分等工作，多个从节点taskManager，主要用于执行JobManager分解出来的任务，使用standalone模式，需要启动flink的主节点JobManager以及从节点taskManager。</p>
<h3 id="4-2-1更改配置文件"><a href="#4-2-1更改配置文件" class="headerlink" title="4.2.1更改配置文件"></a>4.2.1更改配置文件</h3><p>master服务器更改flink-conf.yaml配置文件文件</p>
<h4 id="修改flink-conf-yaml"><a href="#修改flink-conf-yaml" class="headerlink" title="修改flink-conf.yaml"></a>修改flink-conf.yaml</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink/conf</span><br><span class="line"># vim flink-conf.yaml</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797532727.png" alt="img"></p>
<p>更改这个配置，指定jobmanager所在的服务器为master</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jobmanager.rpc.address: master</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797560639.png" alt="img"></p>
<h4 id="修改masters配置文件"><a href="#修改masters配置文件" class="headerlink" title="修改masters配置文件"></a>修改masters配置文件</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim masters</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797583576.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797566578.png" alt="img"></p>
<h4 id="修改slaves配置文件"><a href="#修改slaves配置文件" class="headerlink" title="修改slaves配置文件"></a>修改slaves配置文件</h4><p>master执行以下命令更改从节点slaves配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim workers</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797602270.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681797607520.png" alt="img"></p>
<h3 id="4-2-2安装包分发"><a href="#4-2-2安装包分发" class="headerlink" title="4.2.2安装包分发"></a>4.2.2安装包分发</h3><p>将master服务器的flink安装包分发到其他机器上面去<br>master服务器执行以下命令分发安装包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/</span><br><span class="line"># scp -r flink/ slave1：/usr/cstor</span><br><span class="line"># scp -r flink/ slave2：/usr/cstor</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798123597.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798128998.png" alt="img"></p>
<h3 id="4-2-3启动flink集群"><a href="#4-2-3启动flink集群" class="headerlink" title="4.2.3启动flink集群"></a>4.2.3启动flink集群</h3><p>master执行以下命令启动flink集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink</span><br><span class="line"># bin/start-cluster.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798137062.png" alt="img"></p>
<h3 id="4-2-4页面访问"><a href="#4-2-4页面访问" class="headerlink" title="4.2.4页面访问"></a>4.2.4页面访问</h3><p>打开浏览器输入<a target="_blank" rel="noopener" href="http://master(ip地址):8081/">http://master(IP地址):8081</a></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798144714.png" alt="img"></p>
<h3 id="4-2-5运行flink自带的测试用例"><a href="#4-2-5运行flink自带的测试用例" class="headerlink" title="4.2.5运行flink自带的测试用例"></a>4.2.5运行flink自带的测试用例</h3><p>master执行以下命令启动socket服务，输入单词</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># nc -lk 9000</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798150251.png" alt="img"></p>
<p>再打开个master终端启动flink的自带的单词统计程序，接受输入的socket数据并进行统计</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink</span><br><span class="line"># bin/flink run examples/streaming/SocketWindowWordCount.jar  --port 9000</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798156706.png" alt="img"></p>
<p>在新打开一个master终端执行以下命令查看统计结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink/log</span><br><span class="line"># ls</span><br><span class="line"># tail -f flink-root-taskexecutor-*-master.out</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798163204.png" alt="img"></p>
<p>查看web页面，可以看到我们提交的任务</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798170126.png" alt="img"></p>
<h3 id="4-3Flink的Standalone模式的HA环境部署"><a href="#4-3Flink的Standalone模式的HA环境部署" class="headerlink" title="4.3Flink的Standalone模式的HA环境部署"></a>4.3Flink的Standalone模式的HA环境部署</h3><p>我们实现了flink的standAlone模式的环境安装，并且能够正常提交任务到集群上面去，我们的主节点是jobManager，但是唯一的问题是jobmanager是单节点的，必然会有单节点故障问题的产生，所以我们也可以在standAlone模式下，借助于zookeeper，将我们的jobManager实现成为高可用的模式。</p>
<h3 id="4-3-1首先启动Hadoop集群"><a href="#4-3-1首先启动Hadoop集群" class="headerlink" title="4.3.1首先启动Hadoop集群"></a>4.3.1首先启动Hadoop集群</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop</span><br><span class="line"># sbin/start-dfs.sh</span><br><span class="line"># sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798176563.png" alt="img"></p>
<h3 id="4-3-2修改配置文件"><a href="#4-3-2修改配置文件" class="headerlink" title="4.3.2修改配置文件"></a>4.3.2修改配置文件</h3><p>( 1 ) 修改flink-conf.yaml配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink/conf</span><br><span class="line"># vim flink-conf.yaml</span><br><span class="line"></span><br><span class="line">jobmanager.rpc.address: master</span><br><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.storageDir: hdfs://master:8020/flink/ha</span><br><span class="line">high-availability.zookeeper.quorum: master:2181,slave1:2181,slave2:2181</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798185645.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798191230.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798249100.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798196376.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798201777.png" alt="img"></p>
<p>( 2 ) 修改masters配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vim masters</span><br><span class="line"></span><br><span class="line">master:8081</span><br><span class="line">slave1:8081</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798266587.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798271561.png" alt="img"></p>
<p>( 3 ) 修改slaves配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># vim workers</span><br><span class="line"></span><br><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798280263.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798305882.png" alt="img"></p>
<p>( 4 ) 配置ZooKeeper集群</p>
<p>配置ZooKeeper，具体步骤请参考ZooKeeper实验：安装于部署。</p>
<p>( 5 ) 导入Hadoop依赖</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://repo.maven.apache.org/maven2/org/apache/flink/flink-shaded-hadoop-2-uber/2.6.5-10.0/ 下载依赖</span><br></pre></td></tr></table></figure>

<p>下载后再通过WinSCP放到&#x2F;usr&#x2F;cstor&#x2F;flink&#x2F;lib目录下</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798392710.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798399081.png" alt="img"></p>
<h3 id="4-3-3hdfs上面创建flink对应的文件夹"><a href="#4-3-3hdfs上面创建flink对应的文件夹" class="headerlink" title="4.3.3hdfs上面创建flink对应的文件夹"></a>4.3.3hdfs上面创建flink对应的文件夹</h3><p>master执行以下命令，在hdfs上面创建文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir -p /flink/ha</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798405301.png" alt="img"></p>
<h3 id="4-3-4拷贝配置文件"><a href="#4-3-4拷贝配置文件" class="headerlink" title="4.3.4拷贝配置文件"></a>4.3.4拷贝配置文件</h3><p>将master服务器修改后的配置文件拷贝到其他服务器上面去<br>master执行以下命令拷贝配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#cd ../../</span><br><span class="line"># scp –r flink/ slave1:/usr/cstor/</span><br><span class="line"># scp –r flink/ slave2:/usr/cstor/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798411272.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798416064.png" alt="img"></p>
<h3 id="4-3-5启动zookeeper和flink集群"><a href="#4-3-5启动zookeeper和flink集群" class="headerlink" title="4.3.5启动zookeeper和flink集群"></a>4.3.5启动zookeeper和flink集群</h3><p>master执行以下命令启动flink集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/zookeeper              master、slave1、slave2 依次启动</span><br><span class="line"># bin/ zkServer.sh start                启动ZooKeeper</span><br><span class="line"></span><br><span class="line"># cd /usr/cstor/flink/</span><br><span class="line"># bin/start-cluster.sh                  启动集群</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798426687.png" alt="img"></p>
<h3 id="4-4flink-on-yarn模式"><a href="#4-4flink-on-yarn模式" class="headerlink" title="4.4flink on yarn模式"></a>4.4flink on yarn模式</h3><p>这种方式需要先启动集群，然后在提交作业，接着会向yarn申请一块资源空间后，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成后，释放了资源，那下一个作业才会正常提交，实际工作当中一般不会使用这种模式。<br>这种模式，直接将任务提交到yarn集群上面去，我们需要提前启动hdfs以及yarn集群即可。</p>
<h3 id="4-4-1修改yarn-site-xml文件"><a href="#4-4-1修改yarn-site-xml文件" class="headerlink" title="4.4.1修改yarn-site.xml文件"></a>4.4.1修改yarn-site.xml文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/etc/hadoop/</span><br><span class="line"># vim yarn-site.xml</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798438654.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">&lt;value&gt;master&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">&lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;3.1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;2048&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798446559.png" alt="img"></p>
<h3 id="4-4-2启动Hadoop集群（HDFS和Yarn）"><a href="#4-4-2启动Hadoop集群（HDFS和Yarn）" class="headerlink" title="4.4.2启动Hadoop集群（HDFS和Yarn）"></a>4.4.2启动Hadoop集群（HDFS和Yarn）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop</span><br><span class="line"># sbin/start-dfs.sh</span><br><span class="line"># sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798453139.png" alt="img"></p>
<h3 id="4-4-3向yarn集群申请资源"><a href="#4-4-3向yarn集群申请资源" class="headerlink" title="4.4.3向yarn集群申请资源"></a>4.4.3向yarn集群申请资源</h3><p>执行以下命令向yarn集群申请资源</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bin/yarn-session.sh -n 10 -tm 1024 -s 4 -nm FlinkOnYarnSession –d</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798458922.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798464525.png" alt="img"></p>
<p>可以使用以下参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-D &lt;arg&gt;                        Dynamic properties</span><br><span class="line">     -d,--detached                     Start detached</span><br><span class="line">     -jm,--jobManagerMemory &lt;arg&gt;     Memory for JobManager Container with optional unit (default: MB)</span><br><span class="line">     -nm,--name                      Set a custom name for the application on YARN</span><br><span class="line">     -at,--applicationType               Set a custom application type on YARN</span><br><span class="line">     -q,--query                        Display available YARN resources (memory, cores)</span><br><span class="line">     -qu,--queue &lt;arg&gt;                 Specify YARN queue.</span><br><span class="line">     -s,--slots &lt;arg&gt;                   Number of slots per TaskManager</span><br><span class="line">     -tm,--taskManagerMemory &lt;arg&gt;    Memory per TaskManager Container with optional unit (default: MB)</span><br><span class="line">     -z,--zookeeperNamespace &lt;arg&gt;     Namespace to create the Zookeeper sub-paths for HA mode</span><br></pre></td></tr></table></figure>

<h3 id="4-4-4提交任务"><a href="#4-4-4提交任务" class="headerlink" title="4.4.4提交任务"></a>4.4.4提交任务</h3><p>通过命令行提交需要执行以下命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bin/flink run examples/batch/WordCount.jar</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798594804.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798600053.png" alt="img"></p>
<p>第二种方法(创建的临时集群)</p>
<p>这种模式在你提交JOB的时候再去申请资源，可以理解每个JOB都有自己的job manager, task manager,，一旦JOB完成，进程也就退出了，不会占用任何资源了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bin/flink run -m yarn-cluster ./examples/batch/WordCount.ja</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798607567.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798612589.png" alt="img"></p>
<p>与yarn-session不同的是，需要指定 -m yarn-cluster。如下图，我们实际已经有了yarn-session后台进程，但是通过single job提交方式会生成一个新的application master</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798582748.png" alt="img"></p>
<p>可选参数如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">-c,--class &lt;classname&gt;           Class with the program entry point (&#x27;main&#x27;</span><br><span class="line">                                      method or &#x27;getPlan()&#x27; method. Only needed</span><br><span class="line">                                      if the JAR file does not specify the class</span><br><span class="line">                                      in its manifest.</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;      Address of the JobManager to</span><br><span class="line">                                      which to connect. Use this flag to connect</span><br><span class="line">                                      to a different JobManager than the one</span><br><span class="line">                                      specified in the configuration.</span><br><span class="line">     -p,--parallelism &lt;parallelism&gt;   The parallelism with which to run the</span><br><span class="line">                                      program. Optional flag to override the</span><br><span class="line">                                      default value specified in the</span><br><span class="line">                                      configuration</span><br></pre></td></tr></table></figure>

<h2 id="43-综合实战：环境大数据"><a href="#43-综合实战：环境大数据" class="headerlink" title="43.综合实战：环境大数据"></a>43.综合实战：环境大数据</h2><blockquote>
<h3 id="目的-1"><a href="#目的-1" class="headerlink" title="目的"></a>目的</h3><p>1.学会分析环境数据文件；<br>2.学会编写解析环境数据文件并进行统计的代码；<br>3.学会进行递归MapReduce。</p>
<h3 id="要求-1"><a href="#要求-1" class="headerlink" title="要求"></a>要求</h3><p>要求实验结束时，每位学生均已在master服务器上运行从北京2016年1月到6月这半年间的历史天气和空气质量数据文件中分析出的环境统计结果，包含月平均气温、空气质量分布情况等。</p>
<h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>近年来，由于雾霾问题的持续发酵，越来越多的人开始关注城市相关的环境数据，包括空气质量数据、天气数据等等。</p>
<p>如果每小时记录一次城市的天气实况和空气质量实况信息，则每个城市每天都会产生24条环境数据，全国所有2500多个城市如果均如此进行记录，那每天产生的数据量将达到6万多条，每年则会产生2190万条记录，已经可以称得上环境大数据。</p>
<p>对于这些原始监测数据，我们可以根据时间的维度来进行统计，从而得出与该城市相关的日度及月度平均气温、空气质量优良及污染天数等等，从而为研究空气污染物扩散条件提供有力的数据支持。</p>
<p>本实验中选取了北京2016年1月到6月这半年间的每小时天气和空气质量数据（未取到数据的字段填充“N&#x2F;A”），利用MapReduce来统计月度平均气温和半年内空气质量为优、良、轻度污染、中度污染、重度污染和严重污染的天数。</p>
</blockquote>
<h3 id="4-1-分析数据文件"><a href="#4-1-分析数据文件" class="headerlink" title="4.1 分析数据文件"></a>4.1 分析数据文件</h3><p>在master服务器上执行下列命令，查看环境数据文件beijing.txt，路径在&#x2F;root&#x2F;data&#x2F;hadoop&#x2F;MRevmt目录下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# more /root/data/hadoop/MRevmt/beijing.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1677824963983.png" alt="img"><br>可以看到，我们需要关心的数据有第一列DATE、第二列HOUR、第六列TMP和第七列AQI。</p>
<h3 id="4-2-将数据文件上传至HDFS"><a href="#4-2-将数据文件上传至HDFS" class="headerlink" title="4.2 将数据文件上传至HDFS"></a>4.2 将数据文件上传至HDFS</h3><p>在master上传beijing.txt到HDFS的&#x2F;input目录上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop fs -mkdir /input</span><br><span class="line">[root@master ~]# hadoop fs -put /root/data/hadoop/MRevmt/beijing.txt /input</span><br></pre></td></tr></table></figure>

<h3 id="4-3-编写月平均气温统计程序"><a href="#4-3-编写月平均气温统计程序" class="headerlink" title="4.3 编写月平均气温统计程序"></a>4.3 编写月平均气温统计程序</h3><p>使用Eclipse编写月平均气温统计程序，打包为tmpstat.jar，并上传至master服务器上</p>
<p>在Eclipse上新建MapReduce项目，命名为TmpStat，在src目录下新建文件 TmpStat.java，并键入如下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;</span><br><span class="line"></span><br><span class="line">public class TmpStat</span><br><span class="line">&#123;</span><br><span class="line">  public static class StatMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;</span><br><span class="line">  &#123;</span><br><span class="line">    private IntWritable intValue = new IntWritable();</span><br><span class="line">    private Text dateKey = new Text();</span><br><span class="line"></span><br><span class="line">    public void map(Object key, Text value, Context context)</span><br><span class="line">          throws IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      String[] items = value.toString().split(&#x27;,&#x27;);</span><br><span class="line"></span><br><span class="line">      String date = items[0];</span><br><span class="line">      String tmp = items[5];</span><br><span class="line"></span><br><span class="line">      if(!&#x27;DATE&#x27;.equals(date) &amp;&amp; !&#x27;N/A&#x27;.equals(tmp))</span><br><span class="line">      &#123;//排除第一行说明以及未取到数据的行</span><br><span class="line">        dateKey.set(date.substring(0, 6));</span><br><span class="line">        intValue.set(Integer.parseInt(tmp));</span><br><span class="line">        context.write(dateKey, intValue);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static class StatReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;</span><br><span class="line">    &#123;</span><br><span class="line">    private IntWritable result = new IntWritable();</span><br><span class="line">    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span><br><span class="line">          throws IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      int tmp_sum = 0;</span><br><span class="line">      int count = 0;</span><br><span class="line"></span><br><span class="line">      for(IntWritable val : values)</span><br><span class="line">      &#123;</span><br><span class="line">        tmp_sum += val.get();</span><br><span class="line">        count++;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      int tmp_avg = tmp_sum/count;</span><br><span class="line">      result.set(tmp_avg);</span><br><span class="line">      context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static void main(String args[])</span><br><span class="line">        throws IOException, ClassNotFoundException, InterruptedException</span><br><span class="line">  &#123;</span><br><span class="line">      Configuration conf = new Configuration();</span><br><span class="line">    Job job = new Job(conf, &#x27;MonthlyAvgTmpStat&#x27;);</span><br><span class="line">    job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">    TextInputFormat.setInputPaths(job, args[0]);</span><br><span class="line">    job.setJarByClass(TmpStat.class);</span><br><span class="line">    job.setMapperClass(StatMapper.class);</span><br><span class="line">    job.setMapOutputKeyClass(Text.class);</span><br><span class="line">    job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">    job.setPartitionerClass(HashPartitioner.class);</span><br><span class="line">    job.setReducerClass(StatReducer.class);</span><br><span class="line">    job.setNumReduceTasks(Integer.parseInt(args[2]));</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">    TextOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="line">    System.exit(job.waitForCompletion(true) ? 0 : 1);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用Eclipse软件将TmpStat项目导出成Jar文件，指定主类为TmpStat，命名为tmpstat.jar，并上传至master服务器上。</p>
<h3 id="4-4-查看月平均气温统计结果"><a href="#4-4-查看月平均气温统计结果" class="headerlink" title="4.4 查看月平均气温统计结果"></a>4.4 查看月平均气温统计结果</h3><p>在master上执行tmpstat.jar，指定输出目录为&#x2F;monthlyavgtmp，reducer数量为1。如图所示：</p>
<p><code>[root@master ~]# hadoop jar tmpstat.jar /input /monthlyavgtmp 1</code><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1677824990184.png" alt="img"><br>图40-2 运行tmpstat.jar<br>在master上查看统计结果。如图40-3所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop fs -ls /monthlyavgtmp</span><br><span class="line">[root@master ~]# hadoop fs -cat /monthlyavgtmp/part-r-00000</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1677825048683.png" alt="img"><br>图40-3 查看月平均气温统计结果</p>
<h3 id="4-5-编写每日空气质量统计程序"><a href="#4-5-编写每日空气质量统计程序" class="headerlink" title="4.5 编写每日空气质量统计程序"></a>4.5 编写每日空气质量统计程序</h3><p>在Eclipse上新建MapReduce项目，命名为AqiStatDaily，在src目录下新建文件 AqiStatDaily.java，并键入如下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;</span><br><span class="line"></span><br><span class="line">public class AqiStatDaily</span><br><span class="line">&#123;</span><br><span class="line">  public static class StatMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;</span><br><span class="line">  &#123;</span><br><span class="line">    private IntWritable intValue = new IntWritable();</span><br><span class="line">    private Text dateKey = new Text();</span><br><span class="line"></span><br><span class="line">    public void map(Object key, Text value, Context context)</span><br><span class="line">          throws IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      String[] items = value.toString().split(&#x27;,&#x27;);</span><br><span class="line"></span><br><span class="line">      String date = items[0];</span><br><span class="line">      String aqi = items[6];</span><br><span class="line"></span><br><span class="line">      if(!&#x27;DATE&#x27;.equals(date) &amp;&amp; !&#x27;N/A&#x27;.equals(aqi))</span><br><span class="line">      &#123;</span><br><span class="line">        dateKey.set(date);</span><br><span class="line">        intValue.set(Integer.parseInt(aqi));</span><br><span class="line">        context.write(dateKey, intValue);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static class StatReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;</span><br><span class="line">  &#123;</span><br><span class="line">    private IntWritable result = new IntWritable();</span><br><span class="line">    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span><br><span class="line">            throws IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      int aqi_sum = 0;</span><br><span class="line">      int count = 0;</span><br><span class="line"></span><br><span class="line">      for(IntWritable val : values)</span><br><span class="line">      &#123;</span><br><span class="line">        aqi_sum += val.get();</span><br><span class="line">        count++;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      int aqi_avg = aqi_sum/count;</span><br><span class="line">      result.set(aqi_avg);</span><br><span class="line">      context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static void main(String args[])</span><br><span class="line">        throws IOException, ClassNotFoundException, InterruptedException</span><br><span class="line">  &#123;</span><br><span class="line">    Configuration conf = new Configuration();</span><br><span class="line">    Job job = new Job(conf, &#x27;AqiStatDaily&#x27;);</span><br><span class="line">    job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">    TextInputFormat.setInputPaths(job, args[0]);</span><br><span class="line">    job.setJarByClass(AqiStatDaily.class);</span><br><span class="line">    job.setMapperClass(StatMapper.class);</span><br><span class="line">    job.setMapOutputKeyClass(Text.class);</span><br><span class="line">    job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">    job.setPartitionerClass(HashPartitioner.class);</span><br><span class="line">    job.setReducerClass(StatReducer.class);</span><br><span class="line">    job.setNumReduceTasks(Integer.parseInt(args[2]));</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">    TextOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="line">    System.exit(job.waitForCompletion(true) ? 0 : 1);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用Eclipse软件将AqiStatDaily项目导出成Jar文件，指定主类为AqiStatDaily，命名为aqistatdaily.jar，并上传至master服务器上。</p>
<h3 id="4-6-查看每日空气质量统计结果"><a href="#4-6-查看每日空气质量统计结果" class="headerlink" title="4.6 查看每日空气质量统计结果"></a>4.6 查看每日空气质量统计结果</h3><p>在master上执行aqistatdaily.jar，指定输出目录为&#x2F;aqidaily，reducer数量为3。如图所示：</p>
<p><code>[root@master ~]# hadoop jar aqistatdaily.jar /input /aqidaily 3</code><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1677825076820.png" alt="img"></p>
<p>在master上查看统计结果文件。如图40-5所示：</p>
<p><code>[root@master ~]# hadoop fs -ls /aqidaily</code><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1677825084110.png" alt="img"></p>
<p>可以看到，结果文件被分成了3个部分，依次查看这3个文件的内容，即可看到每天的空气质量统计结果数据。如图所示：</p>
<p><code>[root@master ~]# hadoop fs -cat /aqidaily/p*</code><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1677825092776.png" alt="img"></p>
<h3 id="4-7-将每日空气质量统计文件进行整合"><a href="#4-7-将每日空气质量统计文件进行整合" class="headerlink" title="4.7 将每日空气质量统计文件进行整合"></a>4.7 将每日空气质量统计文件进行整合</h3><p>在master服务器上将每日空气质量统计结果保存到aqidaily.txt。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop fs -cat /aqidaily/part-r-00000 &gt; aqidaily.txt</span><br><span class="line">[root@master ~]# hadoop fs -cat /aqidaily/part-r-00001 &gt; aqidaily.txt</span><br><span class="line">[root@master ~]# hadoop fs -cat /aqidaily/part-r-00002 &gt; aqidaily.txt</span><br><span class="line">[root@master ~]# cat aqidaily.txt |wc -l</span><br><span class="line">182</span><br><span class="line">[root@master ~]#</span><br></pre></td></tr></table></figure>

<p>在HDFS上创建&#x2F;aqiinput目录，并将aqidaily.txt上传至该目录下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop fs -mkdir /aqiinput</span><br><span class="line">[root@master ~]# hadoop fs -put aqidaily.txt /aqiinput</span><br></pre></td></tr></table></figure>

<h3 id="4-8-编写各空气质量天数统计程序"><a href="#4-8-编写各空气质量天数统计程序" class="headerlink" title="4.8 编写各空气质量天数统计程序"></a>4.8 编写各空气质量天数统计程序</h3><p>在Eclipse上新建MapReduce项目，命名为AqiStat，在src目录下新建文件 AqiStat.java，并键入如下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapreduce.Job;</span><br><span class="line">import org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line">import org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line">import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;</span><br><span class="line"></span><br><span class="line">public class AqiStat</span><br><span class="line">&#123;</span><br><span class="line">  public static final String GOOD = &#x27;优&#x27;;</span><br><span class="line">  public static final String MODERATE = &#x27;良&#x27;;</span><br><span class="line">  public static final String LIGHTLY_POLLUTED = &#x27;轻度污染&#x27;;</span><br><span class="line">  public static final String MODERATELY_POLLUTED = &#x27;中度污染&#x27;;</span><br><span class="line">  public static final String HEAVILY_POLLUTED = &#x27;重度污染&#x27;;</span><br><span class="line">  public static final String SEVERELY_POLLUTED = &#x27;严重污染&#x27;;</span><br><span class="line"></span><br><span class="line">  public static class StatMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;</span><br><span class="line">  &#123;</span><br><span class="line">    private final static IntWritable one = new IntWritable(1);</span><br><span class="line">    private Text cond = new Text();</span><br><span class="line">    // map方法，根据AQI值，将对应空气质量的天数加1</span><br><span class="line">    public void map(Object key, Text value, Context context)</span><br><span class="line">            throws IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      String[] items = value.toString().split(&#x27;\t&#x27;);</span><br><span class="line">      int aqi = Integer.parseInt(items[1]);</span><br><span class="line"></span><br><span class="line">      if(aqi &lt;= 50)</span><br><span class="line">      &#123;</span><br><span class="line">        // 优</span><br><span class="line">        cond.set(GOOD);</span><br><span class="line">      &#125;</span><br><span class="line">      else if(aqi &lt;= 100)</span><br><span class="line">      &#123;</span><br><span class="line">        // 良</span><br><span class="line">        cond.set(MODERATE);</span><br><span class="line">      &#125;</span><br><span class="line">      else if(aqi &lt;= 150)</span><br><span class="line">      &#123;</span><br><span class="line">        // 轻度污染</span><br><span class="line">        cond.set(LIGHTLY_POLLUTED);</span><br><span class="line">      &#125;</span><br><span class="line">      else if(aqi &lt;= 200)</span><br><span class="line">      &#123;</span><br><span class="line">        // 中度污染</span><br><span class="line">        cond.set(MODERATELY_POLLUTED);</span><br><span class="line">      &#125;</span><br><span class="line">      else if(aqi &lt;= 300)</span><br><span class="line">      &#123;</span><br><span class="line">        // 重度污染</span><br><span class="line">        cond.set(HEAVILY_POLLUTED);</span><br><span class="line">      &#125;</span><br><span class="line">      else</span><br><span class="line">      &#123;</span><br><span class="line">        // 严重污染</span><br><span class="line">        cond.set(SEVERELY_POLLUTED);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      context.write(cond, one);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  // 定义reduce类，对相同的空气质量状况，把它们&lt;K,VList&gt;中VList值全部相加</span><br><span class="line">  public static class StatReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;</span><br><span class="line">  &#123;</span><br><span class="line">    private IntWritable result = new IntWritable();</span><br><span class="line">    public void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context)</span><br><span class="line">            throws IOException, InterruptedException</span><br><span class="line">    &#123;</span><br><span class="line">      int sum = 0;</span><br><span class="line">      for (IntWritable val : values)</span><br><span class="line">      &#123;</span><br><span class="line">        sum += val.get();</span><br><span class="line">      &#125;</span><br><span class="line">      result.set(sum);</span><br><span class="line">      context.write(key, result);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  public static void main(String args[])</span><br><span class="line">          throws IOException, ClassNotFoundException, InterruptedException</span><br><span class="line">  &#123;</span><br><span class="line">    Configuration conf = new Configuration();</span><br><span class="line">    Job job = new Job(conf, &#x27;AqiStat&#x27;);</span><br><span class="line">    job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">    TextInputFormat.setInputPaths(job, args[0]);</span><br><span class="line">    job.setJarByClass(AqiStat.class);</span><br><span class="line">    job.setMapperClass(StatMapper.class);</span><br><span class="line">    job.setCombinerClass(StatReducer.class);</span><br><span class="line">    job.setMapOutputKeyClass(Text.class);</span><br><span class="line">    job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">    job.setPartitionerClass(HashPartitioner.class);</span><br><span class="line">    job.setReducerClass(StatReducer.class);</span><br><span class="line">    job.setNumReduceTasks(Integer.parseInt(args[2]));</span><br><span class="line">    job.setOutputKeyClass(Text.class);</span><br><span class="line">    job.setOutputValueClass(IntWritable.class);</span><br><span class="line">    job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">    TextOutputFormat.setOutputPath(job, new Path(args[1]));</span><br><span class="line">    System.exit(job.waitForCompletion(true) ? 0 : 1);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用Eclipse软件将AqiStat项目导出成Jar文件，指定主类为AqiStat，命名为aqistat.jar，并上传至master服务器上。</p>
<h3 id="4-9-查看各空气质量天数统计结果"><a href="#4-9-查看各空气质量天数统计结果" class="headerlink" title="4.9 查看各空气质量天数统计结果"></a>4.9 查看各空气质量天数统计结果</h3><p>在master上执行aqistat.jar，指定输出目录为&#x2F;aqioutput，reducer数量为1。如图所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop jar aqistat.jar /aqiinput /aqioutput 1</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1677825123248.png" alt="img"></p>
<p>在master上查看统计结果。如图40-8所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop fs -ls /aqioutput</span><br><span class="line">[root@master ~]# hadoop fs -cat /aqioutput/p*</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1677825135711.png" alt="img"></p>
<h2 id="44-Flink实验：使用内置Scala-Shell"><a href="#44-Flink实验：使用内置Scala-Shell" class="headerlink" title="44.Flink实验：使用内置Scala-Shell"></a>44.Flink实验：使用内置Scala-Shell</h2><blockquote>
<h3 id="目的-2"><a href="#目的-2" class="headerlink" title="目的"></a>目的</h3><p>1.学会使用flink的scala-shell终端程序，通过该脚本可以快速上手flink，并可以对简单的flink任务进行调试和测试。</p>
<h3 id="要求-2"><a href="#要求-2" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能：<br>1.会启动各种模式的Scala-shell。</p>
<h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><p>Flink的交互式编程环境只支持Scala语言，程序员可以基于Scala语言调用DataStream&#x2F;DataSet API、Table API &amp;  SQL，不支持Java。另外，Flink提供了Python版本的REPL环境，不过目前Flink（1.9）的Python API只支持Table API调用。本文主要展示Scala的REPL的使用方法。</p>
</blockquote>
<h3 id="4-1启动flink的scala-shell"><a href="#4-1启动flink的scala-shell" class="headerlink" title="4.1启动flink的scala-shell"></a>4.1启动flink的scala-shell</h3><p>Flink附带了一个集成的交互式Scala Shell。它可以在本地模式和群集模式中使用。要将shell与集成的Flink集群一起使用，只需执行(注意：该命令集成了flink的执行环境，所以不需要启动flink集群)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink</span><br><span class="line"># bin/start-scala-shell.sh local</span><br></pre></td></tr></table></figure>

<p>启动后，命令行中会反馈一些注意信息：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798654690.png" alt="img"></p>
<h3 id="4-2验证一下Scala的Hello-World"><a href="#4-2验证一下Scala的Hello-World" class="headerlink" title="4.2验证一下Scala的Hello World"></a>4.2验证一下Scala的Hello World</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; print(&#x27;Hello World!&#x27;)</span><br><span class="line">Hello World!</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798663052.png" alt="img"></p>
<h3 id="4-3使用Flink集群"><a href="#4-3使用Flink集群" class="headerlink" title="4.3使用Flink集群"></a>4.3使用Flink集群</h3><p>Flink Scala Shell也支持集群模式，包括独立的Flink集群和与其他应用共享的Yarn集群。</p>
<h3 id="4-3-1远程Flink集群"><a href="#4-3-1远程Flink集群" class="headerlink" title="4.3.1远程Flink集群"></a>4.3.1远程Flink集群</h3><p>使用remote模式，指定JobManager的机器名（IP）和端口号：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-scala-shell.sh remote &lt;hostname&gt; &lt;portnumber&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798669885.png" alt="img"></p>
<h3 id="4-3-2集群模式-yarn-scala-shell-cluster"><a href="#4-3-2集群模式-yarn-scala-shell-cluster" class="headerlink" title="4.3.2集群模式 yarn scala shell cluster"></a>4.3.2集群模式 yarn scala shell cluster</h3><p>可以通过scala shell在yarn上启动一个专有的flink集群，yarn 可以通过参数-jm -tm  指定JobManager  container 的内存、TaskManager  container的内存。shell在yarn上部署了一个新的集群并且连接到这个集群。你也可以指定集群的参数，例如：指定TaskManager的slot数量，yarn application的名称等等。</p>
<p>例如：针对scala shell启动一个JobManager container、TaskManager container内存各为 1GB 的Yarn集群，使用下面的参数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bin/start-scala-shell.sh yarn  -jm 1024 -tm 1024</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798677561.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798682642.png" alt="img"></p>
<h3 id="4-3-3yarn-session模式"><a href="#4-3-3yarn-session模式" class="headerlink" title="4.3.3yarn session模式"></a>4.3.3yarn session模式</h3><p>如果你之前已经使用flink yarn session模式启动了一个flink集群，scala shell可以使用下面的命令进行连接：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bin/start-scala-shell.sh yarn</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681798689118.png" alt="img"></p>
<h3 id="4-4完整的参数选项"><a href="#4-4完整的参数选项" class="headerlink" title="4.4完整的参数选项"></a>4.4完整的参数选项</h3><h3 id="4-4-1flink-run参数"><a href="#4-4-1flink-run参数" class="headerlink" title="4.4.1flink run参数"></a>4.4.1flink run参数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">flink run命令执行模板：flink run [option]</span><br><span class="line"></span><br><span class="line">-c,–class : 需要指定的main方法的类</span><br><span class="line"></span><br><span class="line">-C,–classpath : 向每个用户代码添加url，他是通过UrlClassLoader加载。url需要指定文件的schema如（file://）</span><br><span class="line"></span><br><span class="line">-d,–detached : 在后台运行</span><br><span class="line"></span><br><span class="line">-p,–parallelism : job需要指定env的并行度，这个一般都需要设置。</span><br><span class="line"></span><br><span class="line">-q,–sysoutLogging : 禁止logging输出作为标准输出。</span><br><span class="line"></span><br><span class="line">-s,–fromSavepoint : 基于savepoint保存下来的路径，进行恢复。</span><br><span class="line"></span><br><span class="line">-sae,–shutdownOnAttachedExit : 如果是前台的方式提交，当客户端中断，集群执行的job任务也会shutdown。</span><br></pre></td></tr></table></figure>

<h3 id="4-4-2flink-run-m-yarn-cluster参数"><a href="#4-4-2flink-run-m-yarn-cluster参数" class="headerlink" title="4.4.2flink run -m yarn-cluster参数"></a>4.4.2flink run -m yarn-cluster参数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-m,–jobmanager : yarn-cluster集群</span><br><span class="line">-d,–detached : 后台</span><br><span class="line">-jm,–jobManager : jobmanager的内存</span><br><span class="line">-tm,–taskManager : taskmanager的内存</span><br><span class="line">-n,–container : TaskManager的个数</span><br><span class="line">-id,–applicationId : job依附的applicationId</span><br><span class="line">-nm,–name : application的名称</span><br><span class="line">-s,–slots : 分配的slots个数</span><br></pre></td></tr></table></figure>

<h3 id="4-4-3flink-list"><a href="#4-4-3flink-list" class="headerlink" title="4.4.3flink-list"></a>4.4.3flink-list</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">flink list：列出flink的job列表。</span><br><span class="line"></span><br><span class="line">flink list -r/–runing :列出正在运行的job</span><br><span class="line"></span><br><span class="line">flink list -s/–scheduled :列出已调度完成的job</span><br></pre></td></tr></table></figure>

<h3 id="4-4-4flink-cancel"><a href="#4-4-4flink-cancel" class="headerlink" title="4.4.4flink cancel"></a>4.4.4flink cancel</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">flink cancel [options] &lt;job_id&gt; : 取消正在运行的job id</span><br><span class="line"></span><br><span class="line">flink cancel -s/–withSavepoint &lt;job_id&gt; ： 取消正在运行的job，并保存到相应的保存点</span><br><span class="line"></span><br><span class="line">通过 -m 来指定要停止的 JobManager 的主机地址和端口</span><br><span class="line"></span><br><span class="line">例： bin/flink cancel -m 127.0.0.1:8081 5e20cb6b0f357591171dfcca2eea09de</span><br></pre></td></tr></table></figure>

<h3 id="4-4-5flink-stop-仅仅针对Streaming-job"><a href="#4-4-5flink-stop-仅仅针对Streaming-job" class="headerlink" title="4.4.5flink stop :仅仅针对Streaming job"></a>4.4.5flink stop :仅仅针对Streaming job</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">flink stop [options] &lt;job_id&gt;</span><br><span class="line"></span><br><span class="line">flink stop &lt;job_id&gt;：停止对应的job</span><br><span class="line"></span><br><span class="line">通过 -m 来指定要停止的 JobManager 的主机地址和端口</span><br><span class="line"></span><br><span class="line">例： bin/flink stop -m 127.0.0.1:8081 d67420e52bd051fae2fddbaa79e046bb</span><br><span class="line"></span><br><span class="line">取消和停止（流作业）的区别如下：</span><br><span class="line"></span><br><span class="line">cancel() 调用，立即调用作业算子的 cancel() 方法，以尽快取消它们。如果算子在接到 cancel() 调用后没有停止，Flink 将开始定期中断算子线程的执行，直到所有算子停止为止。</span><br><span class="line">stop() 调用，是更优雅的停止正在运行流作业的方式。stop() 仅适用于 Source 实现了 StoppableFunction 接口的作业。当用户请求停止作业时，作业的所有 Source 都将接收 stop() 方法调用。直到所有 Source 正常关闭时，作业才会正常结束。这种方式，使作业正常处理完所有作业。</span><br></pre></td></tr></table></figure>

<h3 id="4-4-6Flink-Scala-Shell"><a href="#4-4-6Flink-Scala-Shell" class="headerlink" title="4.4.6Flink Scala Shell"></a>4.4.6Flink Scala Shell</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">-jm arg | --jobManagerMemory arg</span><br><span class="line">        JobManager container 的内存[in MB]</span><br><span class="line">  -nm &lt;value&gt; | --name &lt;value&gt;</span><br><span class="line">        在YARN上给应用设置一个名字</span><br><span class="line">  -qu &lt;arg&gt; | --queue &lt;arg&gt;</span><br><span class="line">        指定YARN队列</span><br><span class="line">  -s &lt;arg&gt; | --slots &lt;arg&gt;</span><br><span class="line">        指定每个TaskManager的slot数量</span><br><span class="line">  -tm &lt;arg&gt; | --taskManagerMemory &lt;arg&gt;</span><br><span class="line">        TaskManager container的内存 [in MB]</span><br><span class="line">  -a &lt;path/to/jar&gt; | --addclasspath &lt;path/to/jar&gt;</span><br><span class="line">        指定flink使用的第三方jar</span><br><span class="line">  --configDir &lt;value&gt;</span><br><span class="line">        配置文件目录.</span><br><span class="line">  -h | --help</span><br><span class="line">        打印帮助信息</span><br></pre></td></tr></table></figure>

<h2 id="45-Flink实验：算子实例解析"><a href="#45-Flink实验：算子实例解析" class="headerlink" title="45.Flink实验：算子实例解析"></a>45.Flink实验：算子实例解析</h2><blockquote>
<h3 id="目的-3"><a href="#目的-3" class="headerlink" title="目的"></a>目的</h3><p>1.学会使用Flink中的基础算子，来对数据流进行处理。</p>
<h3 id="要求-3"><a href="#要求-3" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能：<br>1.单数据流基本转换<br>2.基于Key的分组转换<br>3.多数据流转换<br>4.数据重分布转换</p>
<h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>Flink的Transformation转换主要包括四种：单数据流基本转换、基于Key的分组转换、多数据流转换和数据重分布转换。</p>
</blockquote>
<h3 id="4-1启动flink-shell"><a href="#4-1启动flink-shell" class="headerlink" title="4.1启动flink shell"></a>4.1启动flink shell</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/flink/bin</span><br><span class="line"># ./start-scala-shell.sh local</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681799917393.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681799922184.png" alt="img"></p>
<h3 id="4-2数据流进行处理和转化"><a href="#4-2数据流进行处理和转化" class="headerlink" title="4.2数据流进行处理和转化"></a>4.2数据流进行处理和转化</h3><p>在使用这些算子时，需要在算子上进行用户自定义操作，一般使用Lambda表达式或者继承模板类并重写函数两种方式完成这个用户自定义的过程。下文将用map算子来演示如何使用Lambda表达式或者重写函数的方式实现对算子的自定义。</p>
<h3 id="4-2-1map"><a href="#4-2-1map" class="headerlink" title="4.2.1map"></a>4.2.1map</h3><p>map算子对一个DataStream中的每个元素使用用户自定义的map函数进行处理，每个输入元素对应一个输出元素，最终整个数据流被转换成一个新的DataStream。输出的数据流DataStream[OUT]类型可能和输入的数据流DataStream[IN]不同。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681799932060.png" alt="img"></p>
<p>我们可以重写MapFunction或RichMapFunction来自定义map函数，RichMapFunction的定义为：RichMapFunction[IN,  OUT]，其内部有一个map虚函数，我们需要对这个虚函数重写(注意，在流处理模式下，print不会自动触发，必须调用execute才能触发执行前面的程序)。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val dataStream: DataStream[Int] = senv.fromElements(1, 2, -3, 0, 5, -9, 8)</span><br><span class="line">// 继承RichMapFunction</span><br><span class="line">// 第一个泛型是输入类型，第二个参数是泛型类型</span><br><span class="line">scala&gt; class DoubleMapFunction extends RichMapFunction[Int, String] &#123;</span><br><span class="line">  override def map(input: Int): String =</span><br><span class="line">  (&quot;overide map Input : &quot; + input.toString + &quot;, Output : &quot; + (input * 2).toString)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; val richFunctionDataStream = dataStream.map &#123;new DoubleMapFunction()&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; richFunctionDataStream.print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681799939088.png" alt="img"></p>
<p>上面的代码清单重写了RichMapFunction中的map函数，将输入结果乘以2，转化为字符串后输出。我们也可以不用显示定义DoubleMapFunction这个类，而是使用匿名类：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 匿名类</span><br><span class="line">scala&gt; val dataStream: DataStream[Int] = senv.fromElements(1, 2, -3, 0, 5, -9, 8)</span><br><span class="line"></span><br><span class="line">scala&gt;val anonymousDataStream = dataStream.map &#123;new RichMapFunction[Int, String] &#123;</span><br><span class="line">  override def map(input: Int): String = &#123;</span><br><span class="line">    (&quot;overide mapInput : &quot; + input.toString + &quot;, Output : &quot; + (input * 2).toString)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;&#125;</span><br><span class="line"></span><br><span class="line">scala&gt;anonymousDataStream.print()</span><br><span class="line"></span><br><span class="line">scala&gt;senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801535004.png" alt="img"></p>
<p>自定义map函数最简便的操作是使用Lambda表达式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 使用=&gt;构造Lambda表达式</span><br><span class="line"></span><br><span class="line">scala&gt; val dataStream: DataStream[Int] = senv.fromElements(1, 2, -3, 0, 5, -9, 8)</span><br><span class="line"></span><br><span class="line">scala&gt; val lambda = dataStream.map ( input =&gt; input * 2 ).print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801543097.png" alt="img"></p>
<p>上面的代码清单中，我们对某整数数据流进行操作，输入元素均为Int，输出元素均为Double。<br>注意，使用Scala进行Flink编程，自定义算子时可以使用圆括号()，也可以使用花括号&#x2F;&#x2F; 使用 _ 构造Lambda表达式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 使用 _ 构造Lambda表达式</span><br><span class="line"></span><br><span class="line">scala&gt; val dataStream: DataStream[Int] = senv.fromElements(1, 2, -3, 0, 5, -9, 8)</span><br><span class="line"></span><br><span class="line">scala&gt; val lambda2 = dataStream.map &#123; _.toDouble * 2 &#125;.print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801552055.png" alt="img"></p>
<p>对上面的几种方式比较可见，Lambda表达式更为简洁，但是可读性差，其他人不容易读懂代码逻辑。重写函数的方式代码更为臃肿，但定义更清晰。</p>
<h3 id="4-2-2filter"><a href="#4-2-2filter" class="headerlink" title="4.2.2filter"></a>4.2.2filter</h3><p>filter算子对每个元素进行过滤，过滤的过程使用一个filter函数进行逻辑判断。对于输入的每个元素，如果filter函数返回True，则保留，如果返回False，则丢弃。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801559824.png" alt="img"></p>
<p>我们可以使用Lambda表达式过滤掉小于等于0的元素：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val dataStream: DataStream[Int] = senv.fromElements(1, 2, -3, 0, 5, -9, 8)</span><br><span class="line"></span><br><span class="line">// 使用 =&gt; 构造Lambda表达式</span><br><span class="line">scala&gt; val lambda = dataStream.filter ( input =&gt; input &gt; 0 ).print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scala&gt; val dataStream: DataStream[Int] = senv.fromElements(1, 2, -3, 0, 5, -9, 8)</span><br><span class="line"></span><br><span class="line">// 使用 _ 构造Lambda表达式</span><br><span class="line">scala&gt; val lambda2 = dataStream.map &#123; _ &gt; 0 &#125;.print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801567650.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801573506.png" alt="img"></p>
<p>也可以继承FilterFunction或RichFilterFunction，然后重写filter方法，我们还可以将参数传递给继承后的类。比如，MyFilterFunction增加一个构造函数参数limit，并在filter方法中使用这个参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">// 继承RichFilterFunction</span><br><span class="line">// limit参数可以从外部传入</span><br><span class="line">scala&gt; class MyFilterFunction(limit: Int) extends RichFilterFunction[Int] &#123;</span><br><span class="line"></span><br><span class="line">  override def filter(input: Int): Boolean = &#123;</span><br><span class="line">    if (input &gt; limit) &#123;</span><br><span class="line">      true</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      false</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; val richFunctionDataStream = dataStream.filter(new MyFilterFunction(2))</span><br><span class="line"></span><br><span class="line">scala&gt; richFunctionDataStream.print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801581520.png" alt="img"></p>
<h3 id="4-2-3flatMap"><a href="#4-2-3flatMap" class="headerlink" title="4.2.3flatMap"></a>4.2.3flatMap</h3><p>flatMap算子和map有些相似，输入都是数据流中的每个元素，与之不同的是，flatMap的输出可以是零个、一个或多个元素，当输出元素是一个列表时，flatMap会将列表展平。如下图所示，输入是包含圆形或正方形的列表，flatMap过滤掉圆形，正方形列表被展平，以单个元素的形式输出。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801589487.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val dataStream: DataStream[String] = senv.fromElements(&quot;Hello World&quot;, &quot;Hello this is Flink&quot;)</span><br><span class="line"></span><br><span class="line">// split函数的输入为 &quot;Hello World&quot; 输出为 &quot;Hello&quot; 和 &quot;World&quot; 组成的列表 [&quot;Hello&quot;, &quot;World&quot;]</span><br><span class="line">// flatMap将列表中每个元素提取出来</span><br><span class="line">// 最后输出为 [&quot;Hello&quot;, &quot;World&quot;, &quot;Hello&quot;, &quot;this&quot;, &quot;is&quot;, &quot;Flink&quot;]</span><br><span class="line">scala&gt; val words = dataStream.flatMap ( input =&gt; input.split(&quot; &quot;) )</span><br><span class="line"></span><br><span class="line">scala&gt; val words2 = dataStream.map &#123; _.split(&quot; &quot;) &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801596220.png" alt="img"></p>
<p>因为flatMap可以输出零到多个元素，我们可以将其看做是map和filter更一般的形式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val dataStream: DataStream[String] = senv.fromElements(&quot;Hello World&quot;, &quot;Hello this is Flink&quot;)</span><br><span class="line"></span><br><span class="line">// 只对字符串数量大于15的句子进行处理</span><br><span class="line">scala&gt; val longSentenceWords = dataStream.flatMap &#123;</span><br><span class="line">  input =&gt; &#123;</span><br><span class="line">    if (input.size &gt; 15) &#123;</span><br><span class="line">      input.split(&quot; &quot;)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      Seq.empty</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; longSentenceWords.print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801604105.png" alt="img"></p>
<p>如果我们只想对长句子进行处理，则可以在flatMap中使用判断语句，对于不需要的部分，我们直接返回空结果Seq.empty。<br>注意，虽然flatMap可以完全替代map和filter，但Flink仍然保留了这三个API，主要因为map和filter的语义更明确，更明确的语义有助于提高代码的可读性。map可以表示一对一的转换，代码阅读者能够确认对于一个输入，肯定能得到一个输出；filter则明确表示发生了过滤操作。</p>
<h3 id="4-3Key的分组转换"><a href="#4-3Key的分组转换" class="headerlink" title="4.3Key的分组转换"></a>4.3Key的分组转换</h3><p>对数据分组主要是为了进行后续的聚合操作，即对同组数据进行聚合分析。keyBy会将一个DataStream转化为一个KeyedStream，聚合操作会将KeyedStream转化为DataStream。如果聚合前每个元素数据类型是T，聚合后的数据类型仍为T。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801611106.png" alt="img"></p>
<h3 id="4-3-1keyBy"><a href="#4-3-1keyBy" class="headerlink" title="4.3.1keyBy"></a>4.3.1keyBy</h3><p>绝大多数情况，我们要根据事件的某种属性或数据的某个字段进行分组，对一个分组内的数据进行处理。如下图所示，keyBy算子根据元素的形状对数据进行分组，相同形状的元素被分到了一起，可被后续算子统一处理。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801616268.png" alt="img"></p>
<p>keyBy算子将DataStream转换成一个KeyedStream。KeyedStream是一种特殊的DataStream，事实上，KeyedStream继承了DataStream，DataStream的各元素随机分布在各Task Slot中，KeyedStream的各元素按照Key分组，分配到各Task  Slot中。我们需要向keyBy算子传递一个参数，以告知Flink以什么字段作为Key进行分组。<br>我们可以使用数字位置来指定Key：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val dataStream: DataStream[(Int, Double)] = senv.fromElements((1, 1.0), (2, 3.2), (1, 5.5), (3, 10.0), (3, 12.5))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 使用数字位置定义Key 按照第一个字段进行分组</span><br><span class="line">scala&gt; val keyedStream = dataStream.keyBy(0).print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801623494.png" alt="img"></p>
<p>一旦按照Key分组后，我们后续可以按照Key进行时间窗口的处理和状态的创建和更新。数据流里包含相同Key的数据都可以访问和修改相同的状态。</p>
<h3 id="4-3-2aggregation"><a href="#4-3-2aggregation" class="headerlink" title="4.3.2aggregation"></a>4.3.2aggregation</h3><p>常见的聚合操作有sum、max、min等，这些聚合操作统称为aggregation。aggregation需要一个参数来指定按照哪个字段进行聚合。跟keyBy相似，我们可以使用数字位置来指定对哪个字段进行聚合，也可以使用字段名。<br>与批处理不同，这些聚合函数是对流数据进行数据，流数据是依次进入Flink的，聚合操作是对之前流入的数据进行统计聚合。sum算子的功能对该字段进行加和，并将结果保存在该字段上。min操作无法确定其他字段的数值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val tupleStream = senv.fromElements(</span><br><span class="line">      (0, 0, 0), (0, 1, 1), (0, 2, 2),</span><br><span class="line">      (1, 0, 6), (1, 1, 7), (1, 2, 8)</span><br><span class="line">)</span><br><span class="line">scala&gt; val sumStream = tupleStream.keyBy(0).sum(1).print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 按第一个字段分组，对第二个字段求和，打印出来的结果如下：</span><br><span class="line">//  (0,0,0)</span><br><span class="line">//  (0,1,0)</span><br><span class="line">//  (0,3,0)</span><br><span class="line">//  (1,0,6)</span><br><span class="line">//  (1,1,6)</span><br><span class="line">//  (1,3,6)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801631252.png" alt="img"></p>
<p>max算子对该字段求最大值，并将结果保存在该字段上。对于其他字段，该操作并不能保证其数值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val maxStream = tupleStream.keyBy(0).max(2).print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 按第一个字段分组，对第三个字段求最大值max，打印出来的结果如下：</span><br><span class="line">//  (0,0,0)</span><br><span class="line">//  (0,0,1)</span><br><span class="line">//  (0,0,2)</span><br><span class="line">//  (1,0,6)</span><br><span class="line">//  (1,0,7)</span><br><span class="line">//  (1,0,8)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801638187.png" alt="img"></p>
<p>maxBy算子对该字段求最大值，maxBy与max的区别在于，maxBy同时保留其他字段的数值，即maxBy可以得到数据流中最大的元素。maxBy算子对该字段求最大值，maxBy与max的区别在于，maxBy同时保留其他字段的数值，即maxBy可以得到数据流中最大的元素。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val maxByStream = tupleStream.keyBy(0).maxBy(2).print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br><span class="line"></span><br><span class="line">// 按第一个字段分组，对第三个字段求最大值maxBy，打印出来的结果如下：</span><br><span class="line">//  (0,0,0)</span><br><span class="line">//  (0,1,1)</span><br><span class="line">//  (0,2,2)</span><br><span class="line">//  (1,0,6)</span><br><span class="line">//  (1,1,7)</span><br><span class="line">//  (1,2,8)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801644945.png" alt="img"></p>
<p>同样，min和minBy的区别在于，min算子对某字段求最小值，minBy返回具有最小值的元素。<br>其实，这些aggregation操作里已经封装了状态数据，比如，sum算子内部记录了当前的和，max算子内部记录了当前的最大值。由于内部封装了状态数据，而且状态数据并不会被清理，因此一定要避免在一个无限数据流上使用aggregation。注意：对于一个KeyedStream,一次只能使用一个aggregation操作，无法链式使用多个。</p>
<h3 id="4-2-3reduce"><a href="#4-2-3reduce" class="headerlink" title="4.2.3reduce"></a>4.2.3reduce</h3><p>前面几个aggregation是几个较为特殊的操作，对分组数据进行处理更为通用的方法是使用reduce算子。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801651018.png" alt="img"></p>
<p>上图展示了reduce算子的原理：reduce在按照同一个Key分组的数据流上生效，它接受两个输入，生成一个输出，即两两合一地进行汇总操作，生成一个同类型的新元素。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class Score(name: String, course: String, score: Int)</span><br><span class="line"></span><br><span class="line">scala&gt; val dataStream: DataStream[Score] = senv.fromElements(</span><br><span class="line">Score(&quot;Li&quot;, &quot;English&quot;, 90), Score(&quot;Wang&quot;, &quot;English&quot;, 88), Score(&quot;Li&quot;, &quot;Math&quot;, 85),</span><br><span class="line">Score(&quot;Wang&quot;, &quot;Math&quot;, 92), Score(&quot;Liu&quot;, &quot;Math&quot;, 91), Score(&quot;Liu&quot;, &quot;English&quot;, 87))</span><br><span class="line"></span><br><span class="line">scala&gt; class MyReduceFunction() extends ReduceFunction[Score] &#123;</span><br><span class="line">  // reduce 接受两个输入，生成一个同类型的新的输出</span><br><span class="line">override def reduce(s1: Score, s2: Score): Score = &#123;</span><br><span class="line">Score(s1.name, &quot;Sum&quot;, s1.score + s2.score)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">scala&gt; val sumReduceFunctionStream = dataStream.keyBy(&quot;name&quot;).reduce(new MyReduceFunction)</span><br><span class="line"></span><br><span class="line">scala&gt; sumReduceFunctionStream.print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801659811.png" alt="img"></p>
<p>使用Lambda表达式更简洁一些：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; case class Score(name: String, course: String, score: Int)</span><br><span class="line">scala&gt; val dataStream: DataStream[Score] = senv.fromElements(</span><br><span class="line">Score(&quot;Li&quot;, &quot;English&quot;, 90), Score(&quot;Wang&quot;, &quot;English&quot;, 88), Score(&quot;Li&quot;, &quot;Math&quot;, 85),</span><br><span class="line">Score(&quot;Wang&quot;, &quot;Math&quot;, 92), Score(&quot;Liu&quot;, &quot;Math&quot;, 91), Score(&quot;Liu&quot;, &quot;English&quot;, 87))</span><br><span class="line"></span><br><span class="line">scala&gt; val sumLambdaStream = dataStream.keyBy(&quot;name&quot;).reduce((s1, s2) =&gt; Score(s1.name, &quot;Sum&quot;, s1.score + s2.score))</span><br><span class="line"></span><br><span class="line">scala&gt; sumLambdaStream.print()</span><br><span class="line"></span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801670176.png" alt="img"></p>
<h3 id="4-4多数据流转换"><a href="#4-4多数据流转换" class="headerlink" title="4.4多数据流转换"></a>4.4多数据流转换</h3><h3 id="4-4-1union"><a href="#4-4-1union" class="headerlink" title="4.4.1union"></a>4.4.1union</h3><p>在DataStream上使用union算子可以合并多个同类型的数据流，并生成同类型的数据流，即可以将多个DataStream[T]合并为一个新的DataStream[T]。数据将按照先进先出（First In First Out）的模式合并，且不去重。下图union对白色和深色两个数据流进行合并，生成一个数据流。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801685673.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val data1Stream: DataStream[String] = senv.fromElements(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;,&quot;8&quot;)</span><br><span class="line">scala&gt; val data2Stream: DataStream[String] = senv.fromElements(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;,&quot;h&quot;)</span><br><span class="line">scala&gt; val data3Stream: DataStream[String] = senv.fromElements(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;,&quot;H&quot;)</span><br><span class="line">scala&gt; val unionStream: DataStream[String]= data1Stream.union(data2Stream,data3Stream)</span><br><span class="line">scala&gt; unionStream.print()</span><br><span class="line">scala&gt; senv.execute()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801693767.png" alt="img"></p>
<h3 id="4-4-2connect"><a href="#4-4-2connect" class="headerlink" title="4.4.2connect"></a>4.4.2connect</h3><p>union虽然可以合并多个数据流，但有一个限制，即多个数据流的数据类型必须相同。connect提供了和union类似的功能，用来连接两个数据流，它与union的区别在于：</p>
<p>1、connect只能连接两个数据流，union可以连接多个数据流。<br>2、connect所连接的两个数据流的数据类型可以不一致，union所连接的两个数据流的数据类型必须一致。<br>3、两个DataStream经过connect之后被转化为ConnectedStreams，ConnectedStreams会对两个流的数据应用不同的处理方法，且双流之间可以共享状态。<br>connect经常被应用在对一个数据流使用另外一个流进行控制处理的场景上，如下图所示。控制流可以是阈值、规则、机器学习模型或其他参数。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801709875.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val intStream: DataStream[Int] = senv.fromElements(1, 0, 9, 2, 3, 6)</span><br><span class="line"></span><br><span class="line">scala&gt; val stringStream: DataStream[String] = senv.fromElements(&quot;LOW&quot;, &quot;HIGH&quot;, &quot;LOW&quot;, &quot;LOW&quot;)</span><br><span class="line"></span><br><span class="line">scala&gt; val connectedStream: ConnectedStreams[Int, String] = intStream.connect(stringStream)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801716810.png" alt="img"></p>
<h3 id="4-5并行度和数据重分布"><a href="#4-5并行度和数据重分布" class="headerlink" title="4.5并行度和数据重分布"></a>4.5并行度和数据重分布</h3><h3 id="4-5-1并行度"><a href="#4-5-1并行度" class="headerlink" title="4.5.1并行度"></a>4.5.1并行度</h3><p>Flink使用并行度来定义某个算子被切分为多少个算子子任务。我们编写的大部分Transformation转换操作能够形成一个逻辑视图，当实际运行时，逻辑视图中的算子会被并行切分为一到多个算子子任务，每个算子子任务处理一部分数据。如下图所示，各个算子并行地在多个子任务上执行，假如算子的并行度为2，那么它有两个实例。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801723781.png" alt="img"></p>
<p>并行度可以在一个Flink作业的执行环境层面统一设置，这样将设置该作业所有算子并行度，也可以对某个算子单独设置其并行度。如果不进行任何设置，默认情况下，一个作业所有算子的并行度会依赖于这个作业的执行环境。如果一个作业在本地执行，那么并行度默认是本机CPU核心数。当我们将作业提交到Flink集群时，需要使用提交作业的客户端，并指定一系列参数，其中一个参数就是并行度。</p>
<p>下面的代码展示了如何获取执行环境的默认并行度，如何更改执行环境的并行度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 获取当前执行环境的默认并行度</span><br><span class="line">val defaultParallelism = senv.getParallelism</span><br><span class="line"></span><br><span class="line">// 设置所有算子的并行度为4，表示所有算子的并行执行的实例数为4</span><br><span class="line">senv.setParallelism(4)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801731164.png" alt="img"></p>
<p>也可以对某个算子设置并行度：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.map(new MyMapper).setParallelism(defaultParallelism * 2)</span><br></pre></td></tr></table></figure>

<h3 id="4-5-2数据重分布"><a href="#4-5-2数据重分布" class="headerlink" title="4.5.2数据重分布"></a>4.5.2数据重分布</h3><p>默认情况下，数据是自动分配到多个实例上的。有的时候，我们需要手动对数据在多个实例上进行分配，例如，我们知道某个实例上的数据过多，其他实例上的数据稀疏，产生了数据倾斜，这时我们需要将数据均匀分布到各个实例上，以避免部分实例负载过重。数据倾斜问题会导致整个作业的计算时间过长或者内存不足等问题。<br>下文涉及到的各个数据重分布算子的输入是DataStream，输出也是DataStream。keyBy也有对数据进行分组和数据重分布的功能，但keyBy输出的是KeyedStream。</p>
<h4 id="（1）shuffle"><a href="#（1）shuffle" class="headerlink" title="（1）shuffle"></a>（1）shuffle</h4><p>shuffle基于正态分布，将数据随机分配到下游各算子实例上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.shuffle()</span><br></pre></td></tr></table></figure>

<h4 id="（2）rebalance与rescale"><a href="#（2）rebalance与rescale" class="headerlink" title="（2）rebalance与rescale"></a>（2）rebalance与rescale</h4><p>rebalance使用Round-ribon思想将数据均匀分配到各实例上。Round-ribon是负载均衡领域经常使用的均匀分配的方法，上游的数据会轮询式地分配到下游的所有的实例上。如下图所示，上游的算子会将数据依次发送给下游所有算子实例。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801740556.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.rebalance()</span><br></pre></td></tr></table></figure>

<p>rescale与rebalance很像，也是将数据均匀分布到各下游各实例上，但它的传输开销更小，因为rescale并不是将每个数据轮询地发送给下游每个实例，而是就近发送给下游实例。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.rescale()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801759780.png" alt="img"></p>
<p>如上图所示，当上游有两个实例时，上游第一个实例将数据发送给下游第一个和第二个实例，上游第二个实例将数据发送给下游第三个和第四个实例，相比rebalance将数据发送给下游每个实例，rescale的传输开销更小。下图则展示了当上游有四个实例，上游前两个实例将数据发送给下游第一个实例，上游后两个实例将数据发送给下游第二个实例。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801765834.png" alt="img"></p>
<h4 id="（3）broadcast"><a href="#（3）broadcast" class="headerlink" title="（3）broadcast"></a>（3）broadcast</h4><p>英文单词’broadcast’翻译过来为广播，在Flink里，数据会被复制并广播发送给下游的所有实例上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.broadcast()</span><br></pre></td></tr></table></figure>

<h4 id="（4）global"><a href="#（4）global" class="headerlink" title="（4）global"></a>（4）global</h4><p>global会所有数据发送给下游算子的第一个实例上，使用这个算子时要小心，以免造成严重的性能问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataStream.global()</span><br></pre></td></tr></table></figure>

<h4 id="（5）partitionCustom"><a href="#（5）partitionCustom" class="headerlink" title="（5）partitionCustom"></a>（5）partitionCustom</h4><p>我们也可以使用partitionCustom来自定义数据重分布逻辑。partitionCustom有两个参数：第一个参数是自定义的Partitioner，我们需要重写里面的partition函数；第二个参数是对数据流哪个字段使用partiton逻辑。partition函数的返回一个整数，表示该元素将被路由到下游第几个实例。<br>Partitioner[T]中泛型T为指定的字段类型，比如我们要对case class (id: Long, name: String, score:  Double)这个数据结构按照id均匀分配到下游各实例，那么泛型T就为id的数据类型Long。同时，泛型T也是partition(key,  numPartitions)函数的第一个参数的数据类型。在调用partitionCustom(partitioner，  field)时，第一个参数是我们重写的Partitioner，第二个参数表示按照id字段进行处理。</p>
<h2 id="46-Flink挑战：用Flink实现Wordcount"><a href="#46-Flink挑战：用Flink实现Wordcount" class="headerlink" title="46.Flink挑战：用Flink实现Wordcount"></a>46.Flink挑战：用Flink实现Wordcount</h2><blockquote>
<h3 id="目的-4"><a href="#目的-4" class="headerlink" title="目的"></a>目的</h3><p>1.学会Flink Scala Shell的Streaming环境、Batch环境、其他依赖并会使用基础算子。</p>
<h3 id="要求-4"><a href="#要求-4" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能：<br>1.学会使用Streaming环境；<br>2.学会使用Batch环境；<br>3.学会使用其他依赖；<br>4.学会使用基础算子。</p>
<h3 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h3><p>scala-shell集成环境说明<br>shell支持Streaming（是流处理）和Batch（是批处理）。启动后会自动预先绑定两个不同的执行环境。可以使用”benv”和”senv”变量(类似于spark-shell中sc变量)来分别访问Batch和Streaming环境。</p>
</blockquote>
<h3 id="4-1使用Streaming环境"><a href="#4-1使用Streaming环境" class="headerlink" title="4.1使用Streaming环境"></a>4.1使用Streaming环境</h3><p>在scala shell中通过DataStream API来计算wordcount</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># scala&gt; val textStreaming = senv.fromElements(</span><br><span class="line">     |   &#x27;To be, or not to be,--that is the question:--&#x27;,</span><br><span class="line">     |   &#x27;Whether &#x27;tis nobler in the mind to suffer&#x27;,</span><br><span class="line">     |   &#x27;The slings and arrows of outrageous fortune&#x27;,</span><br><span class="line">     |   &#x27;Or to take arms against a sea of troubles,&#x27;)</span><br><span class="line">textStreaming: org.apache.flink.streaming.api.scala.DataStream[String] = org.apache.flink.streaming.api.scala.DataStream@22717282</span><br><span class="line"></span><br><span class="line"># scala&gt; val countsStreaming = textStreaming .flatMap &#123; _.toLowerCase.split(&#x27;\\W+&#x27;) &#125; .map &#123; (_, 1) &#125;.keyBy(0).sum(1)</span><br><span class="line">countsStreaming: org.apache.flink.streaming.api.scala.DataStream[(String, Int)] = org.apache.flink.streaming.api.scala.DataStream@4daa4a5a</span><br><span class="line"></span><br><span class="line"># scala&gt; countsStreaming.print()</span><br><span class="line">res7: org.apache.flink.streaming.api.datastream.DataStreamSink[(String, Int)] = org.apache.flink.streaming.api.datastream.DataStreamSink@7d957c96</span><br><span class="line"></span><br><span class="line"># scala&gt; senv.execute(&#x27;Streaming Wordcount&#x27;)</span><br><span class="line"></span><br><span class="line">注意，在流处理模式下，print不会自动触发，必须调用execute才能触发执行前面的程序。</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801894906.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801899870.png" alt="img"></p>
<h3 id="4-2使用Batch环境"><a href="#4-2使用Batch环境" class="headerlink" title="4.2使用Batch环境"></a>4.2使用Batch环境</h3><p>在scala shell中执行wordcount<br>使用Batch环境<br>在scala shell中执行wordcount</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val text = benv.fromElements(</span><br><span class="line">     |   &#x27;To be, or not to be,--that is the question:--&#x27;,</span><br><span class="line">     |   &#x27;Whether &#x27;tis nobler in the mind to suffer&#x27;,</span><br><span class="line">     |   &#x27;The slings and arrows of outrageous fortune&#x27;,</span><br><span class="line">     |   &#x27;Or to take arms against a sea of troubles,&#x27;)</span><br><span class="line">text: org.apache.flink.api.scala.DataSet[String] = org.apache.flink.api.scala.DataSet@479f738a</span><br><span class="line"></span><br><span class="line">scala&gt; val counts = text</span><br><span class="line">counts: org.apache.flink.api.scala.DataSet[String] = org.apache.flink.api.scala.DataSet@479f738a</span><br><span class="line"></span><br><span class="line">scala&gt; val counts = text.flatMap &#123; _.toLowerCase.split(&#x27;\\W+&#x27;) &#125;.map &#123; (_, 1) &#125;.groupBy(0).sum(1)</span><br><span class="line">counts: org.apache.flink.api.scala.AggregateDataSet[(String, Int)] = org.apache.flink.api.scala.AggregateDataSet@44f4c619</span><br><span class="line"></span><br><span class="line">scala&gt; counts.print()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801907718.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681801912394.png" alt="img"></p>
<h3 id="4-3使用其他依赖"><a href="#4-3使用其他依赖" class="headerlink" title="4.3使用其他依赖"></a>4.3使用其他依赖</h3><p>如果程序依赖了其他包，可以在启动Flink Scala Shell时，加上参数-a &lt;path&#x2F;to&#x2F;jar&gt;或–addclasspath &lt;path&#x2F;to&#x2F;jar&gt;。<br>例如，我想使用Gson来解析json数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bin/start-scala-shell.sh local -a /gson//gson-*.jar</span><br></pre></td></tr></table></figure>

<p>这样我就能在交互式环境中使用这个包下的各种类和方法了。绝大多数情况下，我们可能要依赖多个不同的包，这时候需要使用maven-shade-plugin工具将所依赖包合并到一起，打成一个超级包（uber-jar），超级包内包含了这个程序所有必备的依赖。</p>
<h2 id="47-Sqoop实验：基础与搭建"><a href="#47-Sqoop实验：基础与搭建" class="headerlink" title="47.Sqoop实验：基础与搭建"></a>47.Sqoop实验：基础与搭建</h2><blockquote>
<h3 id="目的-5"><a href="#目的-5" class="headerlink" title="目的"></a>目的</h3><p>1.熟悉sqoop的数据传递原理</p>
<h3 id="要求-5"><a href="#要求-5" class="headerlink" title="要求"></a>要求</h3><p>1.掌握sqoop的安装</p>
<h3 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h3><p>数据传递工具Sqoop分为了Sqoop的搭建，常见Sqoop命令的详解，数据的导入&#x2F;导出几部分，本次实验主要是Sqoop的搭建部分。</p>
<h2 id="3-1Sqoop基本原理"><a href="#3-1Sqoop基本原理" class="headerlink" title="3.1Sqoop基本原理"></a>3.1Sqoop基本原理</h2><h3 id="3-1-何为Sqoop？"><a href="#3-1-何为Sqoop？" class="headerlink" title="3.1 何为Sqoop？"></a>3.1 何为Sqoop？</h3><p>Sqoop(SQL-to-Hadoop)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导出到关系型数据库中。</p>
<h3 id="3-1-2-为什么需要用Sqoop？"><a href="#3-1-2-为什么需要用Sqoop？" class="headerlink" title="3.1.2 为什么需要用Sqoop？"></a>3.1.2 为什么需要用Sqoop？</h3><p>我们通常把有价值的数据存储在关系型数据库系统中，以行和列的形式存储数据，以便于用户读取和查询。但是当遇到海量数据时，我们需要把数据提取出来，通过MapReduce对数据进行加工，获得更符合我们需求的数据。数据的导入和导出本质上是Mapreduce程序，充分利用了MR的并行化和容错性。为了能够和HDFS系统之外的数据库系统进行数据交互，MapReduce程序需要使用外部API来访问数据，因此我们需要用到Sqoop。</p>
<h3 id="3-1-3-关系图"><a href="#3-1-3-关系图" class="headerlink" title="3.1.3 关系图"></a>3.1.3 关系图</h3><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681789883156.png" alt="img"></p>
<h3 id="3-1-4-架构图"><a href="#3-1-4-架构图" class="headerlink" title="3.1.4 架构图"></a>3.1.4 架构图</h3><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681789889466.png" alt="img"></p>
<p>在 mapreduce 中主要是对 inputformat 和 outputformat 进行定制。<br>Sqoop工具接收到客户端的shell命令或者Java api命令后，通过Sqoop中的任务翻译器(Task Translator)将命令转换为对应的MapReduce任务，而后将关系型数据库和Hadoop中的数据进行相互转移，进而完成数据的拷贝。</p>
</blockquote>
<p>安装Sqoop的前提是已经具备Java和Hadoop的环境。</p>
<h3 id="4-1-下载并解压"><a href="#4-1-下载并解压" class="headerlink" title="4.1 下载并解压"></a>4.1 下载并解压</h3><ol>
<li>下载地址：<a target="_blank" rel="noopener" href="http://archive.apache.org/dist/sqoop/1.4.7/">http://archive.apache.org/dist/sqoop/1.4.7/</a></li>
<li>上传安装包sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz到环境中</li>
<li>解压sqoop安装包到指定目录，如：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz -C /usr/cstor/sqoop/</span><br></pre></td></tr></table></figure>

<h3 id="4-2-修改配置文件"><a href="#4-2-修改配置文件" class="headerlink" title="4.2 修改配置文件"></a>4.2 修改配置文件</h3><p>Sqoop的配置文件与大多数大数据框架类似，在sqoop根目录下的conf目录中。</p>
<p>1 重命名配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br></pre></td></tr></table></figure>

<p>2 修改配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim sqoop-env.sh</span><br><span class="line"></span><br><span class="line">export HADOOP_COMMON_HOME=/usr/cstor/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=/usr/cstor/hadoop</span><br><span class="line">export HBASE_HOME=/usr/cstor/hbase</span><br><span class="line">export HIVE_HOME=/usr/cstor/hive/bin</span><br><span class="line">export ZOOCFGDIR=/usr/cstor/zookeeper/conf</span><br></pre></td></tr></table></figure>

<p>3 修改configure-sqoop文件</p>
<p>修改sqoop安装目录bin文件夹下的configure-sqoop文件，找到如下位置并注释。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/cstor/sqoop/bin/</span><br><span class="line">vim configure-sqoop</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681789925168.png" alt="img"></p>
<p>4 输入如下命令验证是否正确安装 sqoop，如果正确安装则出现 sqoop 提示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop help</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681789931021.png" alt="img"></p>
<h2 id="48-Sqoop实验：数据导入"><a href="#48-Sqoop实验：数据导入" class="headerlink" title="48.Sqoop实验：数据导入"></a>48.Sqoop实验：数据导入</h2><blockquote>
<h3 id="目的-6"><a href="#目的-6" class="headerlink" title="目的"></a>目的</h3><p>1.熟悉mysql数据导入至HDFS&#x2F;Hive&#x2F;Hbase的过程</p>
<h3 id="要求-6"><a href="#要求-6" class="headerlink" title="要求"></a>要求</h3><p>2.掌握如何将mysql数据导入至HDFS&#x2F;Hive&#x2F;Hbase</p>
<h3 id="原理-6"><a href="#原理-6" class="headerlink" title="原理"></a>原理</h3><p>数据传递工具Sqoop分为了Sqoop的搭建，常见Sqoop命令的详解，数据的导入&#x2F;导出几部分，本次实验主要是Sqoop的数据导入。</p>
<p>Sqoop是关系数据库与Hadoop 之间的数据桥梁，这个桥梁的重要组件是Sgoop连接器，它用于实现与各种关系数据库的连接，从而实现数据的导人和导出操作。</p>
<p>Sqoop连接器能够支持大多数常用的关系数据库，如MySQL、Oracle、DB2和SQL Server等，同时它还有一个通用的JDBC连接器，用于连接支持JDBC协议的数据库。</p>
<p>在Sqoop中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做：导入，即使用import关键字。</p>
<h2 id="3-1导入原理"><a href="#3-1导入原理" class="headerlink" title="3.1导入原理"></a>3.1导入原理</h2><p>在导人数据之前，Sqoop使用JDBC检查导人的数据表，检索出表中的所有列以及列的SQL数据类型，并将这些SQL类型映射为Java数据类型，在转换后的MapReduce应用中使用这些对应的Java类型来保存字段的值，Sqoop的代码生成器使用这些信息来创建对应表的类，用于保存从表中抽取的记录。</p>
</blockquote>
<h3 id="4-1验证sqoop是否安装成功"><a href="#4-1验证sqoop是否安装成功" class="headerlink" title="4.1验证sqoop是否安装成功"></a>4.1验证sqoop是否安装成功</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop help</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790108807.png" alt="img"></p>
<h3 id="4-2-初始化mysql"><a href="#4-2-初始化mysql" class="headerlink" title="4.2 初始化mysql"></a>4.2 初始化mysql</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysqladmin -u root password &#x27;123456&#x27;</span><br><span class="line">mysql -u root -p</span><br><span class="line">输入刚刚设置的密码</span><br><span class="line">MariaDB [(none)]&gt; GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27;</span><br><span class="line">IDENTIFIED BY &#x27;123456&#x27; WITH GRANT OPTION;</span><br><span class="line">MariaDB [(none)]&gt; use mysql;</span><br><span class="line">Database changed</span><br><span class="line">MariaDB [mysql]&gt; update user set</span><br><span class="line">password=&#x27;*6BB4837EB74329105EE4568DDA7DC67ED2CA2AD9&#x27; where</span><br><span class="line">host=&#x27;master&#x27; and user=&#x27;root&#x27;;</span><br><span class="line">MariaDB [mysql]&gt; flush privileges;</span><br><span class="line">把 MySql 数据导入到 HDFS 中</span><br><span class="line">使用如下命令列出 MySql 中所有数据库：</span><br><span class="line">sqoop list-databases --connect jdbc:mysql://master:3306/test --username root --password 123456</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790118648.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [mysql]&gt; create database company;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [mysql]&gt; use company;</span><br><span class="line">Database changed</span><br><span class="line">MariaDB [company]&gt; create table company.staff(id int(4) primary key not null auto_increment, name varchar(255), sex varchar(255));</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line">MariaDB [company]&gt; insert into company.staff(name, sex) values(&#x27;Thomas&#x27;, &#x27;Male&#x27;);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [company]&gt; insert into company.staff(name, sex) values(&#x27;Catalina&#x27;, &#x27;FeMale&#x27;);</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [company]&gt; select * from staff;</span><br><span class="line">+----+----------+--------+</span><br><span class="line">| id | name     | sex    |</span><br><span class="line">+----+----------+--------+</span><br><span class="line">|  1 | Thomas   | Male   |</span><br><span class="line">|  2 | Catalina | FeMale |</span><br><span class="line">+----+----------+--------+</span><br><span class="line">2 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790128290.png" alt="img"></p>
<h3 id="4-3导入数据"><a href="#4-3导入数据" class="headerlink" title="4.3导入数据"></a>4.3导入数据</h3><h3 id="4-3-1全部导入"><a href="#4-3-1全部导入" class="headerlink" title="4.3.1全部导入"></a>4.3.1全部导入</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790138538.png" alt="img"></p>
<h3 id="4-3-2查询导入"><a href="#4-3-2查询导入" class="headerlink" title="4.3.2查询导入"></a>4.3.2查询导入</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--target-dir /user/company \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--query &#x27;select name,sex from staff where id &lt;=1 and $CONDITIONS;&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790145462.png" alt="img"></p>
<h3 id="4-3-3结果查询"><a href="#4-3-3结果查询" class="headerlink" title="4.3.3结果查询"></a>4.3.3结果查询</h3><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790158405.png" alt="img"></p>
<h3 id="4-4-数据导入至Hive-Hbase"><a href="#4-4-数据导入至Hive-Hbase" class="headerlink" title="4.4 数据导入至Hive&#x2F;Hbase"></a>4.4 数据导入至Hive&#x2F;Hbase</h3><p>注意：环境内并未搭建好Hive和Hbase，可手动搭建后自行尝试</p>
<h3 id="4-4-1-导入到Hive命令参考"><a href="#4-4-1-导入到Hive命令参考" class="headerlink" title="4.4.1 导入到Hive命令参考"></a>4.4.1 导入到Hive命令参考</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--hive-import \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--hive-overwrite \</span><br><span class="line">--hive-table staff_hive</span><br></pre></td></tr></table></figure>

<p>提示：该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库，第一步默认的临时目录是&#x2F;user&#x2F;atguigu&#x2F;表名</p>
<h3 id="4-4-2导入到Hbase命令参考"><a href="#4-4-2导入到Hbase命令参考" class="headerlink" title="4.4.2导入到Hbase命令参考"></a>4.4.2导入到Hbase命令参考</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table company \</span><br><span class="line">--columns &#x27;id,name,sex&#x27; \</span><br><span class="line">--column-family &#x27;info&#x27; \</span><br><span class="line">--hbase-create-table \</span><br><span class="line">--hbase-row-key &#x27;id&#x27; \</span><br><span class="line">--hbase-table &#x27;hbase_company&#x27; \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--split-by id</span><br></pre></td></tr></table></figure>

<p>提示：sqoop1.4.6只支持HBase1.0.1之前的版本的自动创建HBase表的功能<br>解决方案：手动创建HBase表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; create &#x27;hbase_company,&#x27;info&#x27;</span><br></pre></td></tr></table></figure>

<p>在HBase中scan这张表得到如下内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; scan ‘hbase_company’</span><br></pre></td></tr></table></figure>

<h2 id="49-Sqoop实验：数据导出"><a href="#49-Sqoop实验：数据导出" class="headerlink" title="49.Sqoop实验：数据导出"></a>49.Sqoop实验：数据导出</h2><blockquote>
<h3 id="目的-7"><a href="#目的-7" class="headerlink" title="目的"></a>目的</h3><p>1熟悉sqoop数据导出的流畅</p>
<h3 id="要求-7"><a href="#要求-7" class="headerlink" title="要求"></a>要求</h3><p>2掌握sqoop数据导出的操作</p>
<h3 id="原理-7"><a href="#原理-7" class="headerlink" title="原理"></a>原理</h3><p>数据传递工具Sqoop分为了Sqoop的搭建，常见Sqoop命令的详解，数据的导入&#x2F;导出几部分，本次实验主要是Sqoop的数据导出。<br>在Sqoop中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群（RDBMS）中传输数据，叫做：导出，即使用export关键字。</p>
<h2 id="3-1导出原理"><a href="#3-1导出原理" class="headerlink" title="3.1导出原理"></a>3.1导出原理</h2><p>在导出数据之前，Sqoop会根据数据库连接字符串来选择一个导出方法，对于大部分系统来说，Sqoop会选择JDBC。Sqoop会根据目标表的定义生成一个Java类，这个生成的类能够从文本中解析出记录数据，并能够向表中插人类型合适的值，然后启动一个MapReduce作业，从HDFS中读取源数据文件，使用生成的类解析出记录，并且执行选定的导出方法。</p>
</blockquote>
<h3 id="4-1-数据准备"><a href="#4-1-数据准备" class="headerlink" title="4.1 数据准备"></a>4.1 数据准备</h3><p>sqoop从MySQL中导出的数据</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790062972.png" alt="img"></p>
<h3 id="4-2-编写脚本"><a href="#4-2-编写脚本" class="headerlink" title="4.2 编写脚本"></a>4.2 编写脚本</h3><p>编写脚本并保存为HDFSToMySQL.conf<br>&amp;characterEncoding&#x3D;utf8如果HDFS上的数据导入MySQL中乱码在jdbc后添加字符集</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">–columns 指定列名</span><br><span class="line">–export-dir 指定HDFS上的（需要导出的位置）存储位置</span><br><span class="line"></span><br><span class="line">export</span><br><span class="line">--connect</span><br><span class="line">jdbc:mysql://master:3306/company?useSSL=false&amp;characterEncoding=utf8</span><br><span class="line">--username</span><br><span class="line">root</span><br><span class="line">--password</span><br><span class="line">123456</span><br><span class="line">--table</span><br><span class="line">staff1</span><br><span class="line">-m</span><br><span class="line">1</span><br><span class="line">--columns</span><br><span class="line">name,sex</span><br><span class="line">--export-dir</span><br><span class="line">/user/company/</span><br><span class="line">--fields-terminated-by </span><br><span class="line">&#x27;,&#x27;</span><br></pre></td></tr></table></figure>

<h3 id="4-3-执行脚本"><a href="#4-3-执行脚本" class="headerlink" title="4.3 执行脚本"></a>4.3 执行脚本</h3><h3 id="4-3-1-创建新表staff1"><a href="#4-3-1-创建新表staff1" class="headerlink" title="4.3.1 创建新表staff1"></a>4.3.1 创建新表staff1</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MariaDB [company]&gt; create table company.staff1(id int(4) primary key not null auto_increment, name varchar(255), sex varchar(255));</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790081901.png" alt="img"></p>
<h3 id="4-3-2-执行脚本"><a href="#4-3-2-执行脚本" class="headerlink" title="4.3.2 执行脚本"></a>4.3.2 执行脚本</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop --options-file HDFSToMySQL.conf</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790087608.png" alt="img"></p>
<h3 id="4-3-3-查看表staff1"><a href="#4-3-3-查看表staff1" class="headerlink" title="4.3.3 查看表staff1"></a>4.3.3 查看表staff1</h3><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681790096041.png" alt="img"></p>
<h2 id="50-Sqoop实验：常用命令"><a href="#50-Sqoop实验：常用命令" class="headerlink" title="50.Sqoop实验：常用命令"></a>50.Sqoop实验：常用命令</h2><blockquote>
<h3 id="目的-8"><a href="#目的-8" class="headerlink" title="目的"></a>目的</h3><p>1.熟悉sqoop的基本命令以及使用场景</p>
<h3 id="要求-8"><a href="#要求-8" class="headerlink" title="要求"></a>要求</h3><p>1.掌握sqoop常用命令的使用方法</p>
<h3 id="原理-8"><a href="#原理-8" class="headerlink" title="原理"></a>原理</h3><p>数据传递工具Sqoop分为了Sqoop的搭建，常见Sqoop命令的详解，数据的导入&#x2F;导出几部分，本次实验主要是Sqoop的命令详解。</p>
<h2 id="3-1-常用命令列举"><a href="#3-1-常用命令列举" class="headerlink" title="3.1 常用命令列举"></a>3.1 常用命令列举</h2><p>这里给大家列出来了一部分Sqoop操作时的常用参数，以供参考，需要深入学习的可以参看对应类的源代码。</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>命令</th>
<th>类</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>import</td>
<td>ImportTool</td>
<td>将数据导入到集群</td>
</tr>
<tr>
<td>2</td>
<td>export</td>
<td>ExportTool</td>
<td>将集群数据导出</td>
</tr>
<tr>
<td>3</td>
<td>codegen</td>
<td>CodeGenTool</td>
<td>获取数据库中某张表数据生成Java并打包Jar</td>
</tr>
<tr>
<td>4</td>
<td>create-hive-table</td>
<td>CreateHiveTableTool</td>
<td>创建Hive表</td>
</tr>
<tr>
<td>5</td>
<td>eval</td>
<td>EvalSqlTool</td>
<td>查看SQL执行结果</td>
</tr>
<tr>
<td>6</td>
<td>import-all-tables</td>
<td>ImportAllTablesTool</td>
<td>导入某个数据库下所有表到HDFS中</td>
</tr>
<tr>
<td>7</td>
<td>job</td>
<td>JobTool</td>
<td>用来生成一个sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务。</td>
</tr>
<tr>
<td>8</td>
<td>list-databases</td>
<td>ListDatabasesTool</td>
<td>列出所有数据库名</td>
</tr>
<tr>
<td>9</td>
<td>list-tables</td>
<td>ListTablesTool</td>
<td>列出某个数据库下所有表</td>
</tr>
<tr>
<td>10</td>
<td>merge</td>
<td>MergeTool</td>
<td>将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中</td>
</tr>
<tr>
<td>11</td>
<td>metastore</td>
<td>MetastoreTool</td>
<td>记录sqoop job的元数据信息，如果不启动metastore实例，则默认的元数据存储目录为：~&#x2F;.sqoop，如果要更改存储目录，可以在配置文件sqoop-site.xml中进行更改。</td>
</tr>
<tr>
<td>12</td>
<td>help</td>
<td>HelpTool</td>
<td>打印sqoop帮助信息</td>
</tr>
<tr>
<td>13</td>
<td>version</td>
<td>VersionTool</td>
<td>打印sqoop版本信息</td>
</tr>
</tbody></table>
</blockquote>
<p>刚才列举了一些Sqoop的常用命令，对于不同的命令，有不同的参数，让我们来一一列举说明。<br>首先来我们来介绍一下公用的参数，所谓公用参数，就是大多数命令都支持的参数。</p>
<h3 id="4-1-公用参数：数据库连接"><a href="#4-1-公用参数：数据库连接" class="headerlink" title="4.1 公用参数：数据库连接"></a>4.1 公用参数：数据库连接</h3><table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–connect</td>
<td>连接关系型数据库的URL</td>
</tr>
<tr>
<td>2</td>
<td>–connection-manager</td>
<td>指定要使用的连接管理类</td>
</tr>
<tr>
<td>3</td>
<td>–driver</td>
<td>Hadoop根目录</td>
</tr>
<tr>
<td>4</td>
<td>–help</td>
<td>打印帮助信息</td>
</tr>
<tr>
<td>5</td>
<td>–password</td>
<td>连接数据库的密码</td>
</tr>
<tr>
<td>6</td>
<td>–username</td>
<td>连接数据库的用户名</td>
</tr>
<tr>
<td>7</td>
<td>–verbose</td>
<td>在控制台打印出详细信息</td>
</tr>
</tbody></table>
<h3 id="4-2-公用参数：import"><a href="#4-2-公用参数：import" class="headerlink" title="4.2 公用参数：import"></a>4.2 公用参数：import</h3><table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–enclosed-by</td>
<td>给字段值前加上指定的字符</td>
</tr>
<tr>
<td>2</td>
<td>–escaped-by</td>
<td>对字段中的双引号加转义符</td>
</tr>
<tr>
<td>3</td>
<td>–fields-terminated-by</td>
<td>设定每个字段是以什么符号作为结束，默认为逗号</td>
</tr>
<tr>
<td>4</td>
<td>–lines-terminated-by</td>
<td>设定每行记录之间的分隔符，默认是\n</td>
</tr>
<tr>
<td>5</td>
<td>–mysql-delimiters</td>
<td>Mysql默认的分隔符设置，字段之间以逗号分隔，行之间以\n分隔，默认转义符是\，字段值以单引号包裹。</td>
</tr>
<tr>
<td>6</td>
<td>–optionally-enclosed-by</td>
<td>给带有双引号或单引号的字段值前后加上指定字符。</td>
</tr>
</tbody></table>
<h3 id="4-3-公用参数：export"><a href="#4-3-公用参数：export" class="headerlink" title="4.3 公用参数：export"></a>4.3 公用参数：export</h3><table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–input-enclosed-by</td>
<td>对字段值前后加上指定字符</td>
</tr>
<tr>
<td>2</td>
<td>–input-escaped-by</td>
<td>对含有转移符的字段做转义处理</td>
</tr>
<tr>
<td>3</td>
<td>–input-fields-terminated-by</td>
<td>字段之间的分隔符</td>
</tr>
<tr>
<td>4</td>
<td>–input-lines-terminated-by</td>
<td>行之间的分隔符</td>
</tr>
<tr>
<td>5</td>
<td>–input-optionally-enclosed-by</td>
<td>给带有双引号或单引号的字段前后加上指定字符</td>
</tr>
</tbody></table>
<h3 id="4-4-公用参数：hive"><a href="#4-4-公用参数：hive" class="headerlink" title="4.4 公用参数：hive"></a>4.4 公用参数：hive</h3><table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–hive-delims-replacement</td>
<td>用自定义的字符串替换掉数据中的\r\n和\013 \010等字符</td>
</tr>
<tr>
<td>2</td>
<td>–hive-drop-import-delims</td>
<td>在导入数据到hive时，去掉数据中的\r\n\013\010这样的字符</td>
</tr>
<tr>
<td>3</td>
<td>–map-column-hive</td>
<td>生成hive表时，可以更改生成字段的数据类型</td>
</tr>
<tr>
<td>4</td>
<td>–hive-partition-key</td>
<td>创建分区，后面直接跟分区名，分区字段的默认类型为string</td>
</tr>
<tr>
<td>5</td>
<td>–hive-partition-value</td>
<td>导入数据时，指定某个分区的值</td>
</tr>
<tr>
<td>6</td>
<td>–hive-home</td>
<td>hive的安装目录，可以通过该参数覆盖之前默认配置的目录</td>
</tr>
<tr>
<td>7</td>
<td>–hive-import</td>
<td>将数据从关系数据库中导入到hive表中</td>
</tr>
<tr>
<td>8</td>
<td>–hive-overwrite</td>
<td>覆盖掉在hive表中已经存在的数据</td>
</tr>
<tr>
<td>9</td>
<td>–create-hive-table</td>
<td>默认是false，即，如果目标表已经存在了，那么创建任务失败</td>
</tr>
<tr>
<td>10</td>
<td>–hive-table</td>
<td>后面接要创建的hive表,默认使用MySQL的表名</td>
</tr>
<tr>
<td>11</td>
<td>–table</td>
<td>指定关系数据库的表名</td>
</tr>
</tbody></table>
<p>公用参数介绍完之后，我们来按照命令介绍命令对应的特有参数。</p>
<h3 id="4-5-命令-参数：import"><a href="#4-5-命令-参数：import" class="headerlink" title="4.5 命令&amp;参数：import"></a>4.5 命令&amp;参数：import</h3><p>将关系型数据库中的数据导入到HDFS（包括Hive，HBase）中，如果导入的是Hive，那么当Hive中没有对应表时，则自动创建。</p>
<ol>
<li>命令：</li>
</ol>
<p>如：导入数据到hive中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff \</span><br><span class="line">--hive-import</span><br></pre></td></tr></table></figure>

<p>如：增量导入数据到hive中，mode&#x3D;append</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">append导入：</span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff \</span><br><span class="line">--num-mappers 1 \</span><br><span class="line">--fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--target-dir /user/hive/warehouse/staff_hive \</span><br><span class="line">--check-column id \</span><br><span class="line">--incremental append \</span><br><span class="line">--last-value 3</span><br></pre></td></tr></table></figure>

<p>注意：append不能与–hive-等参数同时使用（Append mode for hive imports is not yet supported. Please remove the parameter –append-mode）</p>
<p>如：增量导入数据到hdfs中，mode&#x3D;lastmodified</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">先在mysql中建表并插入几条数据：</span><br><span class="line">mysql&gt; create table company.staff_timestamp(id int(4), name varchar(255), sex varchar(255), last_modified timestamp DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP);</span><br><span class="line">mysql&gt; insert into company.staff_timestamp (id, name, sex) values(1, &#x27;AAA&#x27;, &#x27;female&#x27;);</span><br><span class="line">mysql&gt; insert into company.staff_timestamp (id, name, sex) values(2, &#x27;BBB&#x27;, &#x27;female&#x27;);</span><br><span class="line">先导入一部分数据：</span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff_timestamp \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--m 1</span><br><span class="line">再增量导入一部分数据：</span><br><span class="line">mysql&gt; insert into company.staff_timestamp (id, name, sex) values(3, &#x27;CCC&#x27;, &#x27;female&#x27;);</span><br><span class="line">sqoop import \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff_timestamp \</span><br><span class="line">--check-column last_modified \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value &quot;2017-09-28 22:20:38&quot; \</span><br><span class="line">--m 1 \</span><br><span class="line">--append</span><br></pre></td></tr></table></figure>

<p>注意：使用lastmodified方式导入数据要指定增量数据是要–append（追加）还是要–merge-key（合并）</p>
<p>注意：last-value指定的值是会包含于增量导入的数据中</p>
<ol>
<li>参数：</li>
</ol>
<p>|column1|column2|column3|</p>
<p>|content1|content2|content3|</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–append</td>
<td>将数据追加到HDFS中已经存在的DataSet中，如果使用该参数，sqoop会把数据先导入到临时文件目录，再合并</td>
</tr>
<tr>
<td>2</td>
<td>–as-avrodatafile</td>
<td>将数据导入到一个Avro数据文件中</td>
</tr>
<tr>
<td>3</td>
<td>–as-sequencefile</td>
<td>将数据导入到一个sequence文件中</td>
</tr>
<tr>
<td>4</td>
<td>–as-textfile</td>
<td>将数据导入到一个普通文本文件中</td>
</tr>
<tr>
<td>5</td>
<td>–boundary-query</td>
<td>边界查询，导入的数据为该参数的值（一条sql语句）所执行的结果区间内的数据</td>
</tr>
<tr>
<td>6</td>
<td>–columns &lt;col1, col2, col3&gt;</td>
<td>指定要导入的字段</td>
</tr>
<tr>
<td>7</td>
<td>–direct</td>
<td>直接导入模式，使用的是关系数据库自带的导入导出工具，以便加快导入导出过程</td>
</tr>
<tr>
<td>8</td>
<td>–direct-split-size</td>
<td>在使用上面direct直接导入的基础上，对导入的流按字节分块，即达到该阈值就产生一个新的文件</td>
</tr>
<tr>
<td>9</td>
<td>–inline-lob-limit</td>
<td>设定大对象数据类型的最大值</td>
</tr>
<tr>
<td>10</td>
<td>–m或–num-mappers</td>
<td>启动N个map来并行导入数据，默认4个</td>
</tr>
<tr>
<td>11</td>
<td>–query或–e</td>
<td>将查询结果的数据导入，使用时必须伴随参–target-dir，–hive-table，如果查询中有where条件，则条件后必须加上$CONDITIONS关键字</td>
</tr>
<tr>
<td>12</td>
<td>–split-by</td>
<td>按照某一列来切分表的工作单元，不能与–autoreset-to-one-mapper连用（请参考官方文档）</td>
</tr>
<tr>
<td>13</td>
<td>–table</td>
<td>关系数据库的表名</td>
</tr>
<tr>
<td>14</td>
<td>–target-dir</td>
<td>指定HDFS路径</td>
</tr>
<tr>
<td>15</td>
<td>–warehouse-dir</td>
<td>与14参数不能同时使用，导入数据到HDFS时指定的目录</td>
</tr>
<tr>
<td>16</td>
<td>–where</td>
<td>从关系数据库导入数据时的查询条件</td>
</tr>
<tr>
<td>17</td>
<td>–z或–compress</td>
<td>允许压缩</td>
</tr>
<tr>
<td>18</td>
<td>–compression-codec</td>
<td>指定hadoop压缩编码类，默认为gzip(Use Hadoop codec default gzip)</td>
</tr>
<tr>
<td>19</td>
<td>–null-string</td>
<td>string类型的列如果null，替换为指定字符串</td>
</tr>
<tr>
<td>20</td>
<td>–null-non-string</td>
<td>非string类型的列如果null，替换为指定字符串</td>
</tr>
<tr>
<td>21</td>
<td>–check-column</td>
<td>作为增量导入判断的列名</td>
</tr>
<tr>
<td>22</td>
<td>–incremental</td>
<td>mode：append或lastmodified</td>
</tr>
<tr>
<td>23</td>
<td>–last-value</td>
<td>指定某一个值，用于标记增量导入的位置</td>
</tr>
</tbody></table>
<h3 id="4-6-命令-参数：export"><a href="#4-6-命令-参数：export" class="headerlink" title="4.6 命令&amp;参数：export"></a>4.6 命令&amp;参数：export</h3><p>从HDFS（包括Hive和HBase）中奖数据导出到关系型数据库中。</p>
<ol>
<li>命令：<br>如：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop export \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff \</span><br><span class="line">--export-dir /user/company \</span><br><span class="line">--input-fields-terminated-by &quot;\t&quot; \</span><br><span class="line">--num-mappers 1</span><br></pre></td></tr></table></figure>

<ol>
<li>参数：</li>
</ol>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–direct</td>
<td>利用数据库自带的导入导出工具，以便于提高效率</td>
</tr>
<tr>
<td>2</td>
<td>–export-dir</td>
<td>存放数据的HDFS的源目录</td>
</tr>
<tr>
<td>3</td>
<td>-m或–num-mappers</td>
<td>启动N个map来并行导入数据，默认4个</td>
</tr>
<tr>
<td>4</td>
<td>–table</td>
<td>指定导出到哪个RDBMS中的表</td>
</tr>
<tr>
<td>5</td>
<td>–update-key</td>
<td>对某一列的字段进行更新操作</td>
</tr>
<tr>
<td>6</td>
<td>–update-mode</td>
<td>updateonly  allowinsert(默认)</td>
</tr>
<tr>
<td>7</td>
<td>–input-null-string</td>
<td>请参考import该类似参数说明</td>
</tr>
<tr>
<td>8</td>
<td>–input-null-non-string</td>
<td>请参考import该类似参数说明</td>
</tr>
<tr>
<td>9</td>
<td>–staging-table</td>
<td>创建一张临时表，用于存放所有事务的结果，然后将所有事务结果一次性导入到目标表中，防止错误</td>
</tr>
<tr>
<td>10</td>
<td>–clear-staging-table</td>
<td>如果第9个参数非空，则可以在导出操作执行前，清空临时事务结果表</td>
</tr>
</tbody></table>
<h3 id="4-7-命令-参数：codegen"><a href="#4-7-命令-参数：codegen" class="headerlink" title="4.7 命令&amp;参数：codegen"></a>4.7 命令&amp;参数：codegen</h3><p>将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段。<br>如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sqoop codegen \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff \</span><br><span class="line">--bindir /home/admin/Desktop/staff \</span><br><span class="line">--class-name Staff \</span><br><span class="line">--fields-terminated-by &quot;\t&quot;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–bindir</td>
<td>指定生成的Java文件、编译成的class文件及将生成文件打包为jar的文件输出路径</td>
</tr>
<tr>
<td>2</td>
<td>–class-name</td>
<td>设定生成的Java文件指定的名称</td>
</tr>
<tr>
<td>3</td>
<td>–outdir</td>
<td>生成Java文件存放的路径</td>
</tr>
<tr>
<td>4</td>
<td>–package-name</td>
<td>包名，如com.z，就会生成com和z两级目录</td>
</tr>
<tr>
<td>5</td>
<td>–input-null-non-string</td>
<td>在生成的Java文件中，可以将null字符串或者不存在的字符串设置为想要设定的值（例如空字符串）</td>
</tr>
<tr>
<td>6</td>
<td>–input-null-string</td>
<td>将null字符串替换成想要替换的值（一般与5同时使用）</td>
</tr>
<tr>
<td>7</td>
<td>–map-column-java</td>
<td>数据库字段在生成的Java文件中会映射成各种属性，且默认的数据类型与数据库类型保持对应关系。该参数可以改变默认类型，例如：–map-column-java id&#x3D;long, name&#x3D;String</td>
</tr>
<tr>
<td>8</td>
<td>–null-non-string</td>
<td>在生成Java文件时，可以将不存在或者null的字符串设置为其他值</td>
</tr>
<tr>
<td>9</td>
<td>–null-string</td>
<td>在生成Java文件时，将null字符串设置为其他值（一般与8同时使用）</td>
</tr>
<tr>
<td>10</td>
<td>–table</td>
<td>对应关系数据库中的表名，生成的Java文件中的各个属性与该表的各个字段一一对应</td>
</tr>
</tbody></table>
<h3 id="4-8-命令-参数：create-hive-table"><a href="#4-8-命令-参数：create-hive-table" class="headerlink" title="4.8 命令&amp;参数：create-hive-table"></a>4.8 命令&amp;参数：create-hive-table</h3><p>生成与关系数据库表结构对应的hive表结构。</p>
<p>命令：<br>如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sqoop create-hive-table \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff \</span><br><span class="line">--hive-table hive_staff</span><br></pre></td></tr></table></figure>

<p>参数：</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–hive-home</td>
<td>Hive的安装目录，可以通过该参数覆盖掉默认的Hive目录</td>
</tr>
<tr>
<td>2</td>
<td>–hive-overwrite</td>
<td>覆盖掉在Hive表中已经存在的数据</td>
</tr>
<tr>
<td>3</td>
<td>–create-hive-table</td>
<td>默认是false，如果目标表已经存在了，那么创建任务会失败</td>
</tr>
<tr>
<td>4</td>
<td>–hive-table</td>
<td>后面接要创建的hive表</td>
</tr>
<tr>
<td>5</td>
<td>–table</td>
<td>指定关系数据库的表名</td>
</tr>
</tbody></table>
<h3 id="4-9-命令-参数：eval"><a href="#4-9-命令-参数：eval" class="headerlink" title="4.9 命令&amp;参数：eval"></a>4.9 命令&amp;参数：eval</h3><p>可以快速的使用SQL语句对关系型数据库进行操作，经常用于在import数据之前，了解一下SQL语句是否正确，数据是否正常，并可以将结果显示在控制台。</p>
<p>命令：<br>如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop eval \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--query &quot;SELECT * FROM staff&quot;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–query或–e</td>
<td>后跟查询的SQL语句</td>
</tr>
</tbody></table>
<h3 id="4-10-命令-参数：import-all-tables"><a href="#4-10-命令-参数：import-all-tables" class="headerlink" title="4.10 命令&amp;参数：import-all-tables"></a>4.10 命令&amp;参数：import-all-tables</h3><p>可以将RDBMS中的所有表导入到HDFS中，每一个表都对应一个HDFS目录</p>
<p>命令：<br>如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop import-all-tables \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--warehouse-dir /all_tables</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–as-avrodatafile</td>
<td>这些参数的含义均和import对应的含义一致</td>
</tr>
<tr>
<td>2</td>
<td>–as-sequencefile</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>–as-textfile</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>–direct</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>–direct-split-size</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>–inline-lob-limit</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>–m或—num-mappers</td>
<td></td>
</tr>
<tr>
<td>8</td>
<td>–warehouse-dir</td>
<td></td>
</tr>
<tr>
<td>9</td>
<td>-z或–compress</td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>–compression-codec</td>
<td></td>
</tr>
</tbody></table>
<h3 id="4-11-命令-参数：job"><a href="#4-11-命令-参数：job" class="headerlink" title="4.11 命令&amp;参数：job"></a>4.11 命令&amp;参数：job</h3><p>用来生成一个sqoop任务，生成后不会立即执行，需要手动执行。</p>
<p>命令：<br>如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqoop job \</span><br><span class="line"> --create myjob -- import-all-tables \</span><br><span class="line"> --connect jdbc:mysql://master:3306/company \</span><br><span class="line"> --username root \</span><br><span class="line"> --password 123456</span><br><span class="line">sqoop job \</span><br><span class="line">--list</span><br><span class="line">sqoop job \</span><br><span class="line">--exec myjob</span><br></pre></td></tr></table></figure>

<p>注意：注意import-all-tables和它左边的–之间有一个空格<br>注意：如果需要连接metastore，则–meta-connect jdbc:hsqldb:hsql:&#x2F;&#x2F;master:16000&#x2F;sqoop<br>参数：</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–create</td>
<td>创建job参数</td>
</tr>
<tr>
<td>2</td>
<td>–delete</td>
<td>删除一个job</td>
</tr>
<tr>
<td>3</td>
<td>–exec</td>
<td>执行一个job</td>
</tr>
<tr>
<td>4</td>
<td>–help</td>
<td>显示job帮助</td>
</tr>
<tr>
<td>5</td>
<td>–list</td>
<td>显示job列表</td>
</tr>
<tr>
<td>6</td>
<td>–meta-connect</td>
<td>用来连接metastore服务</td>
</tr>
<tr>
<td>7</td>
<td>–show</td>
<td>显示一个job的信息</td>
</tr>
<tr>
<td>8</td>
<td>–verbose</td>
<td>打印命令运行时的详细信息</td>
</tr>
</tbody></table>
<p>尖叫提示：在执行一个job时，如果需要手动输入数据库密码，可以做如下优化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;sqoop.metastore.client.record.password&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;If true, allow saved passwords in the metastore.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="4-12-命令-参数：list-databases"><a href="#4-12-命令-参数：list-databases" class="headerlink" title="4.12 命令&amp;参数：list-databases"></a>4.12 命令&amp;参数：list-databases</h3><p>命令：<br>如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-databases \</span><br><span class="line">--connect jdbc:mysql://master:3306/ \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456</span><br></pre></td></tr></table></figure>

<p>参数：与公用参数一样</p>
<h3 id="4-13-命令-参数：list-tables"><a href="#4-13-命令-参数：list-tables" class="headerlink" title="4.13 命令&amp;参数：list-tables"></a>4.13 命令&amp;参数：list-tables</h3><p>命令：<br>如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sqoop list-tables \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456</span><br></pre></td></tr></table></figure>

<p>参数：与公用参数一样</p>
<h3 id="4-14-命令-参数：merge"><a href="#4-14-命令-参数：merge" class="headerlink" title="4.14 命令&amp;参数：merge"></a>4.14 命令&amp;参数：merge</h3><p>将HDFS中不同目录下面的数据合并在一起并放入指定目录中<br>数据环境：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">new_staff</span><br><span class="line">1       AAA     male</span><br><span class="line">2       BBB     male</span><br><span class="line">3       CCC     male</span><br><span class="line">4       DDD     male</span><br><span class="line">old_staff</span><br><span class="line">1       AAA     female</span><br><span class="line">2       CCC     female</span><br><span class="line">3       BBB     female</span><br><span class="line">6       DDD     female</span><br></pre></td></tr></table></figure>

<p>注意：上边数据的列之间的分隔符应该为\t，行与行之间的分割符为\n，如果直接复制，请检查之。</p>
<p>命令：</p>
<p>如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">创建JavaBean：</span><br><span class="line">sqoop codegen \</span><br><span class="line">--connect jdbc:mysql://master:3306/company \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table staff \</span><br><span class="line">--bindir /home/admin/Desktop/staff \</span><br><span class="line">--class-name Staff \</span><br><span class="line">--fields-terminated-by &quot;\t&quot;</span><br><span class="line"></span><br><span class="line">开始合并：</span><br><span class="line">sqoop merge \</span><br><span class="line">--new-data /test/new/ \</span><br><span class="line">--onto /test/old/ \</span><br><span class="line">--target-dir /test/merged \</span><br><span class="line">--jar-file /home/admin/Desktop/staff/Staff.jar \</span><br><span class="line">--class-name Staff \</span><br><span class="line">--merge-key id</span><br><span class="line">结果：</span><br><span class="line">1	AAA	MALE</span><br><span class="line">2	BBB	MALE</span><br><span class="line">3	CCC	MALE</span><br><span class="line">4	DDD	MALE</span><br><span class="line">6	DDD	FEMALE</span><br></pre></td></tr></table></figure>

<p>参数：</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–new-data</td>
<td>HDFS 待合并的数据目录，合并后在新的数据集中保留</td>
</tr>
<tr>
<td>2</td>
<td>–onto</td>
<td>HDFS合并后，重复的部分在新的数据集中被覆盖</td>
</tr>
<tr>
<td>3</td>
<td>–merge-key</td>
<td>合并键，一般是主键ID</td>
</tr>
<tr>
<td>4</td>
<td>–jar-file</td>
<td>合并时引入的jar包，该jar包是通过Codegen工具生成的jar包</td>
</tr>
<tr>
<td>5</td>
<td>–class-name</td>
<td>对应的表名或对象名，该class类是包含在jar包中的</td>
</tr>
<tr>
<td>6</td>
<td>–target-dir</td>
<td>合并后的数据在HDFS里存放的目录</td>
</tr>
</tbody></table>
<h3 id="4-15-命令-参数：metastore"><a href="#4-15-命令-参数：metastore" class="headerlink" title="4.15 命令&amp;参数：metastore"></a>4.15 命令&amp;参数：metastore</h3><p>记录了Sqoop job的元数据信息，如果不启动该服务，那么默认job元数据的存储目录为~&#x2F;.sqoop，可在sqoop-site.xml中修改。</p>
<p>命令：</p>
<p>如：启动sqoop的metastore服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqoop metastore</span><br></pre></td></tr></table></figure>

<p>参数：</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>–shutdown</td>
<td>关闭metastore</td>
</tr>
</tbody></table>
<h2 id="51-ZooKeeper实验：安装与部署"><a href="#51-ZooKeeper实验：安装与部署" class="headerlink" title="51.ZooKeeper实验：安装与部署"></a>51.ZooKeeper实验：安装与部署</h2><blockquote>
<h3 id="目的-9"><a href="#目的-9" class="headerlink" title="目的"></a>目的</h3><p>学会Zookeeper的相关部署，了解Zookeeper的相关概念。</p>
<h3 id="要求-9"><a href="#要求-9" class="headerlink" title="要求"></a>要求</h3><p>1.掌握ZooKeeper集群安装部署；<br>2.加深对ZooKeeper相关概念的理解；</p>
<h3 id="原理-9"><a href="#原理-9" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1认识Zookeeper"><a href="#3-1认识Zookeeper" class="headerlink" title="3.1认识Zookeeper"></a>3.1认识Zookeeper</h2><p>介绍Zookeeper前，看下面这个图：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868887354.png" alt="img"></p>
<p>在这张图片里，我们可以看到Hadoop是一只大象，HIVE是一只蜜蜂，Bigtop是马戏团，Pig是一只猪，HAMA是一只河马…只有Zookeeper是一个拿着铁锹的人，我们可以将Zookeeper理解为动物园的管理员…它负责管理动物园中的动物，如果某个动物出现了问题，它就会第一时间知道并进行处理。此时再用官方的解释来说，Zookeeper是一个开源的分布式的、为分布式应用提供服务的Apache项目。好了，这就是Zookeeper，是不是很好理解。</p>
<h2 id="3-2-zookeeper工作机制"><a href="#3-2-zookeeper工作机制" class="headerlink" title="3.2 zookeeper工作机制"></a>3.2 zookeeper工作机制</h2><p>它既然是一个管理员，那么它就需要知道自己管辖范围内的各种情况，如果有问题就及时处理；我们从设计模式的角度来理解：zookeeper是一个基于观察者模式设计的分布式服务管理框架；它负责存储和管理大家都关心的数据，然后接收观察者的注册、一旦这些数据发送了变化，zookeeper就将负责通知已经在zookeeper上注册的那些观察者做出相应的反应。</p>
<h2 id="3-3集群结构"><a href="#3-3集群结构" class="headerlink" title="3.3集群结构"></a>3.3集群结构</h2><p>类似其他分布式工具，Zookeeper是支持复制的，由一系列的Server组成，Client与Server相连以获取相关的服务。<br>在这里插入图片描述</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868898607.png" alt="img"></p>
<p>集群结构说明：<br>集群数量为奇数，并且超过N+1台，只要集群中大多数节点都处于可用状态，那么集群就是可用的，集群中的服务器角色有：<br>(1）领导者：复杂投票的发起和决议，并更新系统状态<br>(2）学习者：包括跟随者和观察者，从Leader处获取最新的系统状态<br>(3）Follower用于接受客户端请求，并向客户端返回结果，在选举过程中参与投票<br>(4）Observer观察者可以接受客户端请求，将写请求转发给Leader，但是不参与投票，只同步状态。它的目的是为了扩展系统，以提高读取速度。</p>
<p>集群中由一个Leader节点和N个Follower节点组成，所有的Server之间能彼此通信，并且在各自内存中维护了状态图，与之一致的事务日志和快照位于持久化存储中。这一点类似Redis的Sentinel模式，它们都具有高度的自治能力，一旦Leader节点不可达，则自动完成新Leader的选举并恢复服务。</p>
<h2 id="3-4-Zookeeper的数据结构"><a href="#3-4-Zookeeper的数据结构" class="headerlink" title="3.4 Zookeeper的数据结构"></a>3.4 Zookeeper的数据结构</h2><p>Zookeeper的数据模型结构与我们学习的Unix文件系统类似，只不过它是由节点组成，Unix是由文件组成，Zookeeper的每一个节点称作ZNode,每个节点可以存储1M的数据，看起来它很小，其实很大了，因为它只会存储关键的数据，通常情况下，每个节点ZNode中只存储的xxkb的数据。每个ZNode都可以通过路径做唯一的标识</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868922481.png" alt="img"></p>
</blockquote>
<h3 id="4-1安装Zookeeper"><a href="#4-1安装Zookeeper" class="headerlink" title="4.1安装Zookeeper"></a>4.1安装Zookeeper</h3><p>在 Apache 下载 Zookeeper 软件包，（我们的平台已经安装了Zookeeper组件，我们可以跳过这步，想自己安装的小伙伴可以输入以下命令尝试一下)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</span><br><span class="line"># tar -zxvf zookeeper-3.4.6.tar.gz</span><br></pre></td></tr></table></figure>

<h3 id="4-2本地模式部署"><a href="#4-2本地模式部署" class="headerlink" title="4.2本地模式部署"></a>4.2本地模式部署</h3><h3 id="4-2-1-配置环境变量"><a href="#4-2-1-配置环境变量" class="headerlink" title="4.2.1 配置环境变量"></a>4.2.1 配置环境变量</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/profile   打开环境变量配置文件</span><br><span class="line"></span><br><span class="line">export ZOOKEEPER_HOME=/usr/cstor/zookeeper </span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868940142.png" alt="img"></p>
<p>修改完成后，我们输入以下命令激活我们修改的环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># source /etc/profile</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868948028.png" alt="img"></p>
<p>4.2.2配置修改</p>
<p>进入到Zookeeper根目录下的下conf文件中复制zoo_sample.cfg并命名为zoo.cfg</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/zookeeper/</span><br><span class="line"># cd conf/</span><br><span class="line"># ls</span><br><span class="line"># cp zoo_sample.cfg zoo.cfg </span><br><span class="line"># ls</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868958502.png" alt="img"></p>
<p>修改zoo.cfg文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vim zoo.cfg</span><br><span class="line"></span><br><span class="line">dataDir=/usr/cstor/zookeeper/data</span><br><span class="line">dataLogDir=/usr/cstor/zookeeper/log</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868967800.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868974262.png" alt="img"></p>
<p>Zookeeper中的配置文件zoo.cfg中参数含义解读如下：</p>
<p>tickTime &#x3D;2000：通信心跳数，Zookeeper服务器与客户端心跳时间，单位毫秒<br>Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个tickTime时间就会发送一个心跳，时间单位为毫秒。它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime)<br>initLimit &#x3D;10：LF初始通信时限<br>集群中的Follower跟随者服务器与Leader领导者服务器之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。<br>syncLimit &#x3D;5：LF同步通信时限<br>集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。<br>dataDir：数据文件目录+数据持久化路径<br>主要用于保存Zookeeper中的数据。<br>clientPort &#x3D;2181：客户端连接端口<br>监听客户端连接的端口</p>
<p>回到zookeeper目录下创建data、log(因为datadir、dataLogDir指向它)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd ../</span><br><span class="line"># mkdir data</span><br><span class="line"># mkdir log</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868986142.png" alt="img"></p>
<h3 id="4-2-3测试是否安装成功"><a href="#4-2-3测试是否安装成功" class="headerlink" title="4.2.3测试是否安装成功"></a>4.2.3测试是否安装成功</h3><p>启动zookeeper服务 ：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bin/zkServer.sh start|stop|status|restart</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681868998239.png" alt="img"></p>
<p>启动成功。</p>
<h3 id="4-2-4-测试Zookeeper客户端"><a href="#4-2-4-测试Zookeeper客户端" class="headerlink" title="4.2.4 测试Zookeeper客户端"></a>4.2.4 测试Zookeeper客户端</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># bin/zkCli.sh          #启动客户端</span><br><span class="line"># quit                 #退出客户端</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869009441.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869019820.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869027689.png" alt="img"></p>
<p>本地模式部署完成。</p>
<h3 id="4-3-集群模式部署"><a href="#4-3-集群模式部署" class="headerlink" title="4.3 集群模式部署"></a>4.3 集群模式部署</h3><h3 id="4-3-1部署前的准备"><a href="#4-3-1部署前的准备" class="headerlink" title="4.3.1部署前的准备"></a>4.3.1部署前的准备</h3><p>（1）首先，集群模式要在前面本地模式没有问题并且三个节点都成功的情况下进行</p>
<p>（2）然后我们需要先把先把zookeeper服务停掉</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># bin/zkServer.sh stop</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869039830.png" alt="img"></p>
<p>（3）log和data中的数据全部删除</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cd data/</span><br><span class="line"># rm -r verstion-2/</span><br><span class="line"># cd ../</span><br><span class="line"># cd log/</span><br><span class="line"># rm -r version-2/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869048919.png" alt="img"></p>
<h3 id="4-3-2创建myid文件"><a href="#4-3-2创建myid文件" class="headerlink" title="4.3.2创建myid文件"></a>4.3.2创建myid文件</h3><p>分别在三个节点（master、slave1、slave2）在zookeeper&#x2F;data中创建myid文件。并在文件中写一个数值，类似我们Hadoop集群时的hostname。该值代表zookeeper节点Id。注意：该值在整个集群中是唯一的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># cd ../</span><br><span class="line"># cd data/</span><br><span class="line"># vim myid</span><br><span class="line"></span><br><span class="line">master 的myid内容为1</span><br><span class="line">slave1 的myid内容为2</span><br><span class="line">slave2 的myid内容为3</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869063081.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869070277.png" alt="img"></p>
<p>slave1、slave2操作如上。</p>
<h3 id="4-3-3修改配置文件zoo-cfg"><a href="#4-3-3修改配置文件zoo-cfg" class="headerlink" title="4.3.3修改配置文件zoo.cfg"></a>4.3.3修改配置文件zoo.cfg</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># vim conf/zoo.cfg</span><br><span class="line"></span><br><span class="line"># The number of milliseconds of each tick</span><br><span class="line">tickTime=2000</span><br><span class="line"># The number of ticks that the initial </span><br><span class="line"># synchronization phase can take</span><br><span class="line">initLimit=10</span><br><span class="line"># The number of ticks that can pass between </span><br><span class="line"># sending a request and getting an acknowledgement</span><br><span class="line">syncLimit=5</span><br><span class="line"># the port at which the clients will connect</span><br><span class="line">clientPort=2181</span><br><span class="line"># the directory where the snapshot is stored.</span><br><span class="line">dataDir=/usr/cstor/zookeeper/data</span><br><span class="line">dataLogDir=/usr/cstor/zookeeper/log</span><br><span class="line">server.1=master:2888:3888</span><br><span class="line">server.2=slave1:2888:3888</span><br><span class="line">server.3=slave2:2888:3888</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869080630.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869091286.png" alt="img"></p>
<p>将修改好的zoo.cfg文件分发到slave1和slave2节点上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># scp -r conf/zoo.cfg slave1:/usr/cstor/zookeeper/conf/</span><br><span class="line"></span><br><span class="line"># scp -r conf/zoo.cfg slave2:/usr/cstor/zookeeper/conf/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869105880.png" alt="img"></p>
<h3 id="4-3-4启动Zookeeper集群"><a href="#4-3-4启动Zookeeper集群" class="headerlink" title="4.3.4启动Zookeeper集群"></a>4.3.4启动Zookeeper集群</h3><p>分别在三个节点进入bin目录，启动ZooKeeper服务进程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/zookeeper/bin</span><br><span class="line"></span><br><span class="line"># ./zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>master:</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869120205.png" alt="img"></p>
<p>slave1:</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869128050.png" alt="img"></p>
<p>slave2:</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869135575.png" alt="img"></p>
<h3 id="4-3-5查看状态信息"><a href="#4-3-5查看状态信息" class="headerlink" title="4.3.5查看状态信息"></a>4.3.5查看状态信息</h3><p>在各机器上依次执行脚本，查看ZooKeeper状态信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./zkServer.sh status</span><br></pre></td></tr></table></figure>

<p>master：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869147906.png" alt="img"></p>
<p>slave1:</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869155979.png" alt="img"></p>
<p>slave2:</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869165948.png" alt="img"></p>
<h3 id="4-3-6启动zkCli客户端"><a href="#4-3-6启动zkCli客户端" class="headerlink" title="4.3.6启动zkCli客户端"></a>4.3.6启动zkCli客户端</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./zkCli.sh -server master:2181,slave1:2181,slave2:2181</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869175875.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869188424.png" alt="img"></p>
<h2 id="52-Zookeeper实验：zkCli客户端"><a href="#52-Zookeeper实验：zkCli客户端" class="headerlink" title="52.Zookeeper实验：zkCli客户端"></a>52.Zookeeper实验：zkCli客户端</h2><blockquote>
<h3 id="目的-10"><a href="#目的-10" class="headerlink" title="目的"></a>目的</h3><p>熟悉ZooKeeper相关命令操作。</p>
<h3 id="要求-10"><a href="#要求-10" class="headerlink" title="要求"></a>要求</h3><p>1、理解session基本原理<br>2、理解watch机制<br>3、学会zkCli的常用命令</p>
<h3 id="原理-10"><a href="#原理-10" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1、session基本原理："><a href="#3-1、session基本原理：" class="headerlink" title="3.1、session基本原理："></a>3.1、session基本原理：</h2><p>客户端与服务端之间的连接存在会话，每个会话都会可以设置一个超时时间，心跳结束，session则过期，Session过期，则临时节点znode会被抛弃。心跳机制:客户端向服务端的ping包请求。</p>
<h2 id="3-2、watch机制"><a href="#3-2、watch机制" class="headerlink" title="3.2、watch机制"></a>3.2、watch机制</h2><p>针对每个节点的操作，都会有一个监督者→wathcer，当监控的某个对象( znode )发生了变化，则触发watcher事件，zk中的watcher是一次性的，触发后立即销毁（可以设置为永久型）。父节点，子节点增删改都能够触发其watcher。<br>针对不同类型的操作，触发的watcher事件也不同︰<br>1.(子)节点创建事件<br>2.(子)节点删除事件<br>3.(子)节点数据变化事件</p>
</blockquote>
<h3 id="4-1启动客户端"><a href="#4-1启动客户端" class="headerlink" title="4.1启动客户端"></a>4.1启动客户端</h3><p>注意，一定先启动了ZooKeeper才可以开启客户端（启动ZooKeeper集群，具体步骤可以参考ZooKeeper实验：安装与部署。）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># jps</span><br><span class="line"># cd /usr/cstor/zookeeper</span><br><span class="line"># bin/zkCli.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869218516.png" alt="img"></p>
<h3 id="4-2查询命令"><a href="#4-2查询命令" class="headerlink" title="4.2查询命令"></a>4.2查询命令</h3><p>（1）显示所有操作命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># help</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869243032.png" alt="img"></p>
<p>（2） 查看当前znode总所包含的内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ls /</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869252722.png" alt="img"></p>
<p>（3）查看节点信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># stat /</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869262396.png" alt="img"></p>
<p>（4）除了能查看目录下的节点，还能返回节点的信息（例如：ls2 &#x2F;zookeeper。另外 stat &#x2F;zookeeper 只能返回节点信息，ls2相当于 ls + stat 命令。）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ls2 /</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869294388.png" alt="img"></p>
<p>（5）详细信息详解</p>
<p>cZxid:代表zookeeper创建后为这个节点所分配的ID号。<br>ctime:代表该节点创建时间。<br>mZxid:代表修改节点后分配的一个新ID。节点每次修改mZxid都会以递增的形式增加。<br>mtime:代表最后一次被修改的时间。<br>pZxid:代表节点的子节点的最后被修改生成的ID号。<br>cversion:节点下所有子节点被修改的版本号，表示节点被修改的次数。<br>dataVersion:节点修改的版本号，代表节点被修改的次数，初始值为0，每一次修改一次值加1。<br>aclVersion:代表一个权限模型 acl代表权限 当节点权限发生变化的时候 权限的版本会自动累加1<br>ephemeralOwner：如果节点为临时节点，表示节点拥有者的会话ID，否则为0.<br>dataLength:代表的是元数据长度<br>numChildren:节点下子节点的数量。</p>
<h3 id="4-3维护命令"><a href="#4-3维护命令" class="headerlink" title="4.3维护命令"></a>4.3维护命令</h3><p>（1）创建节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># create /wangzhe &#x27;luban&#x27;     创建一个节点 wangzhe 并添加值 luban</span><br><span class="line"></span><br><span class="line"># create /wangzhe/sheshou &#x27;houyi&#x27;       在节点 wangzhe 下，创建一个sheshou并添加内容houyi</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869307675.png" alt="img"></p>
<p>获取节点的值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># ls / 	查看我们创建的wangzhe节点</span><br><span class="line"># get /wangzhe	获取wangzhe节点值</span><br><span class="line"># ls /wangzhe	查看wangzhe节点下的sheshou节点</span><br><span class="line"># get /wangzhe/sheshou	获取sheshou节点值</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869315430.png" alt="img"></p>
<p>（2）创建临时节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># create -e /wangzhe/fuzhu &#x27;daqiao&#x27;      	在wangzhe节点下，创建临时 fuzhu 节点并添加值 daqiao</span><br><span class="line"># ls /wangzhe	查看创建的 fuzhu 临时节点</span><br><span class="line"># get /wangzhe/fuzhu	获取 fuzhu 节点值</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869328817.png" alt="img"></p>
<p>退出当前客户端再进入查看临时节点是否存在</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># quit	退出客户端</span><br><span class="line"># bin/zkCli.sh	启动客户端</span><br><span class="line"># ls /	查看默认节点wangzh 发现存在</span><br><span class="line"># ls /wangzhe	查看wangzhe节点下的fuzhu临时节点消失</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869339581.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869347300.png" alt="img"></p>
<p>（3）创建顺序节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># create -s /wangzhe/fashi &#x27;zhouyu&#x27;  在wangzhe节点下创建顺序节点 fashe 并添加值 zhouyu</span><br><span class="line"># ls /wangzhe	查看节点</span><br><span class="line"># get /wangzhe/fashi0000000003       	获取fashi0000000003数据值</span><br></pre></td></tr></table></figure>

<p>注意：设置的节点路径会被重命名为序列数，如果客户端不与服务端连接，则节点的心跳机制也会停止（心跳机制也有时间段的，不是说客户端不与服务端连接就马上停止，得过了心跳机制设置的时间才会停止），ZooKeeper也会自动删除临时节点。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869358132.png" alt="img"></p>
<p>（4）修改节点数据值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># set /wangzhe/fashi0000000003 &#x27;daqiao&#x27;    修改fashi0000000003数据值为 daqiao</span><br><span class="line"># get /wangzge/fashi0000000003            获取fashi0000000003 数据值</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869369033.png" alt="img"></p>
<p>（5）删除节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># delete /wangzhe/fashi0000000003       删除fashi0000000003顺序节点</span><br><span class="line"># ls /wangzhe                         查看fashi0000000003 是否成功删除</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869378088.png" alt="img"></p>
<h3 id="4-4节点监听"><a href="#4-4节点监听" class="headerlink" title="4.4节点监听"></a>4.4节点监听</h3><p>Watch命令<br>（1）（子）节点值变化的监听<br>通过get path [watch]设置watcher<br>我们启动两台zkCli客户端就命名为A、B。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># get /wangzhe watch	在A上注册监听/wangzhe 节点数据变化</span><br><span class="line"># set /wangzhe &#x27;zheyun&#x27;       在B上修改/wangzge 节点的数据</span><br></pre></td></tr></table></figure>

<p>修改完成A就会收到数据变化的监听了。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869669399.png" alt="img"></p>
<p>（2）（子）节点变化的监听</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ls /wangzhe/sheshou watch    在A上注册监听/wangzhe/sheshou 节点的子节点变化</span><br><span class="line"># create /wangzhe/sheshou/jialuo &#x27;jialuo&#x27;  在B上/wangzhe/sheshou 节点上创建字节点</span><br></pre></td></tr></table></figure>

<p>创建完成后A就会收到子节点变化的监听了。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869681739.png" alt="img"></p>
<p>（3）（子）节点删除事件的监听</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ls /wangzhe/sheshou/jialuo watch    在A上注册监听/wangzhe/sheshou/jialuo 节点的子节点变化</span><br><span class="line"># delete /wangzhe/sheshou/jialuo      在B上删除 /wangzhe/sheshou/jialuo 子节点</span><br></pre></td></tr></table></figure>

<p>删除完成后A就会收到节点变化的监听了。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869694239.png" alt="img"></p>
<h2 id="53-ZooKeeper实验：ACL权限控制"><a href="#53-ZooKeeper实验：ACL权限控制" class="headerlink" title="53.ZooKeeper实验：ACL权限控制"></a>53.ZooKeeper实验：ACL权限控制</h2><blockquote>
<h3 id="目的-11"><a href="#目的-11" class="headerlink" title="目的"></a>目的</h3><p>ZooKeeper 类似文件系统，client 可以创建节点、更新节点、删除节点，那么如何做到节点的权限的控制呢？ZooKeeper的Access Control List 访问控制列表可以做到这一点。</p>
<h3 id="要求-11"><a href="#要求-11" class="headerlink" title="要求"></a>要求</h3><p>了解掌握ACL相关权限控制。</p>
<h3 id="原理-11"><a href="#原理-11" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1、ACL权限控制："><a href="#3-1、ACL权限控制：" class="headerlink" title="3.1、ACL权限控制："></a>3.1、ACL权限控制：</h2><p>使用scheme、id、permission 来标识，主要涵盖 3 个方面：<br>权限模式（scheme）：授权的策略<br>授权对象（id）：授权的对象<br>权限（permission）：授予的权限</p>
<h2 id="3-2、Zookeeper-ACL的特性："><a href="#3-2、Zookeeper-ACL的特性：" class="headerlink" title="3.2、Zookeeper ACL的特性："></a>3.2、Zookeeper ACL的特性：</h2><p>Zookeeper的权限控制是基于znode节点的，需要对每个节点设置权限。<br>每个znode支持设置多种权限控制方案和多个权限。<br>子节点不会继承父节点的权限。客户端无法访问某个节点，但是可以访问他的子节点</p>
<h2 id="3-3、权限模式"><a href="#3-3、权限模式" class="headerlink" title="3.3、权限模式"></a>3.3、权限模式</h2><table>
<thead>
<tr>
<th>方案</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>world</td>
<td>只有一个用户：anyone，代表登录zokeeper所有人（默认）</td>
</tr>
<tr>
<td>ip</td>
<td>对客户端使用IP地址认证</td>
</tr>
<tr>
<td>auth</td>
<td>使用已添加认证的用户认证</td>
</tr>
<tr>
<td>digest</td>
<td>使用“用户名:密码”方式认证</td>
</tr>
</tbody></table>
<h2 id="3-4、授予权限"><a href="#3-4、授予权限" class="headerlink" title="3.4、授予权限"></a>3.4、授予权限</h2><p>create、delete、read、writer、admin也就是 增、删、改、查、管理权限，这5种权限简写为cdrwa，注意:这5种权限中，delete是指对子节点的删除权限，其它4种权限指对自身节点的操作权限。</p>
<table>
<thead>
<tr>
<th>权限</th>
<th>ACL简写</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>create</td>
<td>c</td>
<td>可以创建子节点</td>
</tr>
<tr>
<td>delete</td>
<td>d</td>
<td>可以删除子节点（仅下一级节点）</td>
</tr>
<tr>
<td>read</td>
<td>r</td>
<td>可以读取节点数据及显示子节点列表</td>
</tr>
<tr>
<td>write</td>
<td>w</td>
<td>可以设置节点数据</td>
</tr>
<tr>
<td>admin</td>
<td>a</td>
<td>可以设置节点访问控制列表权限</td>
</tr>
</tbody></table>
</blockquote>
<h3 id="4-1、启动ZooKeeper集群"><a href="#4-1、启动ZooKeeper集群" class="headerlink" title="4.1、启动ZooKeeper集群"></a>4.1、启动ZooKeeper集群</h3><p>启动ZooKeeper集群，具体步骤可以参考ZooKeeper实验：安装与部署。</p>
<h3 id="4-2、启动zkCli客户端"><a href="#4-2、启动zkCli客户端" class="headerlink" title="4.2、启动zkCli客户端"></a>4.2、启动zkCli客户端</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/zookeeper</span><br><span class="line"># bin/zkClin.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869757752.png" alt="img"></p>
<h3 id="4-3、查看节点ACL"><a href="#4-3、查看节点ACL" class="headerlink" title="4.3、查看节点ACL"></a>4.3、查看节点ACL</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># create /node1 “node1”</span><br><span class="line"># getAcl /node1</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869766843.png" alt="img"></p>
<p>这是创建节点时的默认acl ，表示任何连接到Zookeeper的客户端都能对该节点进行cdrwa操作。权限设置要使用模式 acl的模式为 scheme: id:permission 比如 world:anyone:cdwa。</p>
<h3 id="4-4、world权限"><a href="#4-4、world权限" class="headerlink" title="4.4、world权限"></a>4.4、world权限</h3><p>world权限模式只有一种设置模式。就是 setAcl world:anyone:[r][w][c][d][a]</p>
<p>（1）取消节点读取数据权限：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># setAcl /node1 world:anyone:cdwa     取消节点读取数据权限</span><br><span class="line"># getAcl /node1                     查看节点权限</span><br><span class="line"># get /node1                        获取node1节点值</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869779517.png" alt="img"></p>
<p>设置以后尝试读取&#x2F;node1节点将是没有权限 Authentication is not valid。</p>
<p>（2）取消节点设置数据的权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># setAcl /node1 world:anyone:cda         取消设置数据权限</span><br><span class="line"># getAcl /node1                        查看节点权限</span><br><span class="line"># set /node1 &#x27;node111111&#x27;               修改node1节点值</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869788101.png" alt="img"></p>
<p>（3）取消节点创建子节点的权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># setAcl /node1 world:anyone:da           取消节点创建子节点权限</span><br><span class="line"># getAcl /node1                         查看节点权限</span><br><span class="line"># create /node1/node2 &#x27;node2&#x27;             在node1节点下创建子节点node2并赋值 node2</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869796360.png" alt="img"></p>
<p>（4）取消删除子节点的权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create /node8 &#x27;888&#x27;    </span><br><span class="line"># create /node8/node9 &#x27;999&#x27;</span><br><span class="line"># setAcl /node8 world:anyone:a          取消删除子节点的权限</span><br><span class="line"># delete /node8/node9                  删除node8节点下的子节点node9</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869805623.png" alt="img"></p>
<p>（5）取消ACL相关权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create /node666 &#x27;666&#x27;</span><br><span class="line"># setAcl /node666 world:anyone:           取消node666 全部权限</span><br><span class="line"># getAcl /node666</span><br><span class="line"># setAcl /node666 world:anyone:rwcda</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869815284.png" alt="img"></p>
<p>取消了a权限以后，getAcl setAcl命令都没有权限。</p>
<h3 id="4-5、IP权限模式"><a href="#4-5、IP权限模式" class="headerlink" title="4.5、IP权限模式"></a>4.5、IP权限模式</h3><p>该模式使用的ACL方式是 ip:10.30.196.4:[r][w][c][d][a]</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create /node20 &#x27;node20&#x27;</span><br><span class="line"># setAcl /node20 ip:10.30.196.4:rwcda</span><br><span class="line"># getAcl /node20</span><br><span class="line"># get /node20</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869825592.png" alt="img"></p>
<p>设置只有ip为10.30.196.4的客户端连接才能进行rwdca操作，其他ip啥操作都做不了。因为图中设置ACL的客户端不是这个ip，所以设置了后，他就失去对该节点的权限了，所以getAcl命令会没有权限。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869835037.png" alt="img"></p>
<p>我们用ip为10.30.196.4的客户端链接，有rwdca权限，所以能够执行getAcl操作和读读取节点数据。</p>
<h3 id="4-6、auth授权模式"><a href="#4-6、auth授权模式" class="headerlink" title="4.6、auth授权模式"></a>4.6、auth授权模式</h3><p>这个要配合addauth命令。<br>第一步：先添加授权用户 addauth digest username:password<br>第二步：设置该节点只有登录了该授权用户的客户端连接才能进行操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># addauth digest admin:123456</span><br><span class="line"># create /nodeAuth “nodeAuth”</span><br><span class="line"># setAcl /nodeAuth auth:admin:rwdca</span><br><span class="line"># getAcl /nodeAuth</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869847425.png" alt="img"></p>
<p>把客户端quit退出重新连接后：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># get /nodeAuth</span><br></pre></td></tr></table></figure>

<p>失去了对该节点的权限。需要使用addauth命令添加授权才行。类似登录之后才能对该节点有权限。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># addauth digest admin：123456 </span><br><span class="line"># get /nodeAuth</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869856821.png" alt="img"></p>
<h3 id="4-7、digest授权模式"><a href="#4-7、digest授权模式" class="headerlink" title="4.7、digest授权模式"></a>4.7、digest授权模式</h3><p>digest授权模式基于账号密码的授权模式，与Auth模式类似，只是他设置权限之前不用使用addauth digest username:password进行权限用户添加。直接使用命令 setAcl path  digest:username:password:acl 进行授权就行(只是这里的密码要使用加密后的密码，不能使用铭文密码)。<br>linux命令行输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># echo -n admin:123456 | openssl dgst -binary -sha1 | openssl base64</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869868781.png" alt="img"></p>
<p>zkCli客户端输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># create /nodeDigest &#x27;nodeDigest&#x27;</span><br><span class="line"># setAcl /nodeDigest digest:admin:0uek/hZ/V9fgiM35b0Z2226acMQ=:rwdca</span><br><span class="line"># getAcl /nodeDigest</span><br><span class="line"># get /nodeDigest</span><br><span class="line"># addauth digest admin:123456</span><br><span class="line"># getAcl /nodeDigest</span><br><span class="line"># get /nodeDigest</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869877128.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869883439.png" alt="img"></p>
<h3 id="4-8多种授权模式"><a href="#4-8多种授权模式" class="headerlink" title="4.8多种授权模式"></a>4.8多种授权模式</h3><p>同一个节点可以同时使用多种模式授权</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create /nodeManyAcl &#x27;nodeManyAcl&#x27;</span><br><span class="line"># addauth digest admin:123456</span><br><span class="line"># setAcl /nodeManyAcl ip:10.30.196.3:rdca,auth:admin:rwdca,digest:admin:0uek/hZ/V9fgiM35b0Z2226acMQ=:rwdca</span><br><span class="line"># getAcl /nodeManyAcl</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869917359.png" alt="img"></p>
<h2 id="54-ZooKeeper实验：JavaAPI"><a href="#54-ZooKeeper实验：JavaAPI" class="headerlink" title="54.ZooKeeper实验：JavaAPI"></a>54.ZooKeeper实验：JavaAPI</h2><blockquote>
<h3 id="目的-12"><a href="#目的-12" class="headerlink" title="目的"></a>目的</h3><p>学会使用ZooKeeper Java API操作。</p>
<h3 id="要求-12"><a href="#要求-12" class="headerlink" title="要求"></a>要求</h3><p>学会使用ZooKeeper Java API 来对节点的增删改查。</p>
<h3 id="原理-12"><a href="#原理-12" class="headerlink" title="原理"></a>原理</h3><p>znode 是 ZooKeeper 集合的核心组件，zookeeper API提供了一小组方法使用 Zookeeper 集合来操纵 znode 的所有细节。客户端应该遵循以步骤，与Zookeeper服务器进行清晰和干净的交互。</p>
<p>连接到Zookeeper服务器。Zookeeper服务器为客户端分配会话ID。<br>定期向服务器发送心跳。否则，Zookeeper服务器将过期会话ID，客户端需要重新连接。<br>只要会话ID处于活动状态，就可以获取&#x2F;设置znode。<br>所有任务完成后，断开与 Zookeeper 服务器的连接。如果客户端长时间不活动，则 Zookeeper 服务器将自动断开客户端。</p>
</blockquote>
<p>项目架构</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869951615.png" alt="img"></p>
<h3 id="4-1-导入jar包"><a href="#4-1-导入jar包" class="headerlink" title="4.1 导入jar包"></a>4.1 导入jar包</h3><p>将zookeeper&#x2F;lib目录下的jar，通过WinSCP导入到IDEA开发工具中。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869960966.png" alt="img"></p>
<p>本次实验已经启动了ZooKeeper集群，（启动ZooKeeper集群，具体步骤可以参考ZooKeeper实验：安装与部署）</p>
<h3 id="4-2连接到ZooKeeper"><a href="#4-2连接到ZooKeeper" class="headerlink" title="4.2连接到ZooKeeper"></a>4.2连接到ZooKeeper</h3><p>ZooKeeper(String connectionString, int sessionTimeout, Watcher watcher)<br>connectionString - zookeeper主机<br>sessionTimeout - 会话超时（以毫秒为单位)<br>watcher - 实现“监视器”对象。zookeeper集合通过监视器对象返回连接状态。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZookeeperConnection</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line"><span class="comment">// 计数器对象</span></span><br><span class="line">CountDownLatch countDownLatch=<span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">             <span class="comment">// arg1:服务器的ip和端口</span></span><br><span class="line">             <span class="comment">// arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的</span></span><br><span class="line">             <span class="comment">// arg3:监视器对象</span></span><br><span class="line">                 ZooKeeper zooKeeper=<span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(<span class="string">&#x27;192.168.60.130:2181&#x27;</span>,<span class="number">5000</span>, <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">              <span class="keyword">if</span>(event.getState()==Event.KeeperState.SyncConnected)&#123;</span><br><span class="line">      System.out.println(<span class="string">&#x27;连接创建成功!&#x27;</span>);</span><br><span class="line">      countDownLatch.countDown();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">           &#125;);</span><br><span class="line">           <span class="comment">// 主线程阻塞等待连接对象的创建成功</span></span><br><span class="line">           countDownLatch.await();</span><br><span class="line">           <span class="comment">// 会话编号</span></span><br><span class="line">           System.out.println(zooKeeper.getSessionId());</span><br><span class="line">           zooKeeper.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception ex) &#123;</span><br><span class="line">            ex.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-3新增节点"><a href="#4-3新增节点" class="headerlink" title="4.3新增节点"></a>4.3新增节点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 同步方式</span><br><span class="line">create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode)</span><br><span class="line">// 异步方式</span><br><span class="line">create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode, AsyncCallback.StringCallback callBack, Object ctx)</span><br></pre></td></tr></table></figure>

<p>path - znode路径。例如，&#x2F;create&#x2F;node1&#x2F;<br>data - 要存储在指定znode路径中的数据<br>acl - 要创建的节点的访问控制列表。zookeeper API提供了一个静态接口 ZooDefs.Ids 来获取一些基本的acl列表。例如，ZooDefs.Ids.OPEN_ACL_UNSAFE 返回打开znode的acl列表。<br>createMode - 节点的类型,这是一个枚举。<br>callBack-异步回调接口<br>ctx-传递上下文参数</p>
<p>（1）在终端上启动zkCli客户端并创建一个create节点（为Java API 新增节点提供环境）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># create /create “create”</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869978702.png" alt="img"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.CreateMode;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooDefs;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.ACL;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Id;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZKCreate</span> &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zooKeeper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title function_">before</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">IP</span> <span class="operator">=</span> <span class="string">&#x27;10.30.196.3:2181&#x27;</span>;</span><br><span class="line">        <span class="comment">// 计数器对象</span></span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">countDownLatch</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// arg1:服务器的ip和端口</span></span><br><span class="line">        <span class="comment">// arg2:客户端与服务器之间的会话超时时间 以毫秒为单位的</span></span><br><span class="line">        <span class="comment">// arg3:监视器对象</span></span><br><span class="line">        zooKeeper = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(IP, <span class="number">5000</span>, event -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (event.getState() == Watcher.Event.KeeperState.SyncConnected) &#123;</span><br><span class="line">                System.out.println(<span class="string">&#x27;连接创建成功!&#x27;</span>);</span><br><span class="line">                countDownLatch.countDown();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 主线程阻塞等待连接对象的创建成功</span></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">after</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        zooKeeper.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create1</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// arg1:节点的路径</span></span><br><span class="line">        <span class="comment">// arg2:节点的数据</span></span><br><span class="line">        <span class="comment">// arg3:权限列表 world:anyone: cdrwa</span></span><br><span class="line">        <span class="comment">// arg4:节点类型 PERSISTENT 持久化节点</span></span><br><span class="line">        zooKeeper.create(<span class="string">&#x27;/create/node1&#x27;</span>, <span class="string">&#x27;node1&#x27;</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create2</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 设置只读权限 Ids.READ_ACL_UNSAFE world:anyone:r</span></span><br><span class="line">        zooKeeper.create(<span class="string">&#x27;/create/node2&#x27;</span>, <span class="string">&#x27;node2&#x27;</span>.getBytes(), ZooDefs.Ids.READ_ACL_UNSAFE, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create3</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// world授权模式</span></span><br><span class="line">        <span class="comment">// 权限列表</span></span><br><span class="line">        List&lt;ACL&gt; acls = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 授权模式和授权对象</span></span><br><span class="line">        <span class="type">Id</span> <span class="variable">id</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Id</span>(<span class="string">&#x27;world&#x27;</span>, <span class="string">&#x27;anyone&#x27;</span>);</span><br><span class="line">        <span class="comment">// 权限设置</span></span><br><span class="line">        acls.add(<span class="keyword">new</span> <span class="title class_">ACL</span>(ZooDefs.Perms.READ, id));</span><br><span class="line">        acls.add(<span class="keyword">new</span> <span class="title class_">ACL</span>(ZooDefs.Perms.WRITE, id));</span><br><span class="line">        zooKeeper.create(<span class="string">&#x27;/create/node3&#x27;</span>, <span class="string">&#x27;node3&#x27;</span>.getBytes(), acls, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create4</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// ip授权模式</span></span><br><span class="line">        <span class="comment">// 权限列表</span></span><br><span class="line">        List&lt;ACL&gt; acls = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 授权模式和授权对象</span></span><br><span class="line">        <span class="type">Id</span> <span class="variable">id</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Id</span>(<span class="string">&#x27;ip&#x27;</span>, <span class="string">&#x27;10.30.196.4&#x27;</span>);</span><br><span class="line">        <span class="comment">// 权限设置</span></span><br><span class="line">        acls.add(<span class="keyword">new</span> <span class="title class_">ACL</span>(ZooDefs.Perms.ALL, id));</span><br><span class="line">        zooKeeper.create(<span class="string">&#x27;/create/node4&#x27;</span>, <span class="string">&#x27;node4&#x27;</span>.getBytes(), acls, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create5</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// auth授权模式</span></span><br><span class="line">        <span class="comment">// 添加授权用户</span></span><br><span class="line">        zooKeeper.addAuthInfo(<span class="string">&#x27;digest&#x27;</span>, <span class="string">&#x27;admin:123456&#x27;</span>.getBytes());</span><br><span class="line">        zooKeeper.create(<span class="string">&#x27;/create/node5&#x27;</span>, <span class="string">&#x27;node5&#x27;</span>.getBytes(), ZooDefs.Ids.CREATOR_ALL_ACL, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create6</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// auth授权模式</span></span><br><span class="line">        <span class="comment">// 添加授权用户</span></span><br><span class="line">        zooKeeper.addAuthInfo(<span class="string">&#x27;digest&#x27;</span>, <span class="string">&#x27;admin:123456&#x27;</span>.getBytes());</span><br><span class="line">        <span class="comment">// 权限列表</span></span><br><span class="line">        List&lt;ACL&gt; acls = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 授权模式和授权对象</span></span><br><span class="line">        <span class="type">Id</span> <span class="variable">id</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Id</span>(<span class="string">&#x27;auth&#x27;</span>, <span class="string">&#x27;admin&#x27;</span>);</span><br><span class="line">        <span class="comment">// 权限设置</span></span><br><span class="line">        acls.add(<span class="keyword">new</span> <span class="title class_">ACL</span>(ZooDefs.Perms.READ, id));</span><br><span class="line">        zooKeeper.create(<span class="string">&#x27;/create/node6&#x27;</span>, <span class="string">&#x27;node6&#x27;</span>.getBytes(), acls, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create7</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// digest授权模式</span></span><br><span class="line">        <span class="comment">// 权限列表</span></span><br><span class="line">        List&lt;ACL&gt; acls = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        <span class="comment">// 授权模式和授权对象</span></span><br><span class="line">        <span class="comment">// addauth digest admin:123456 #添加</span></span><br><span class="line">        <span class="type">Id</span> <span class="variable">id</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Id</span>(<span class="string">&#x27;digest&#x27;</span>, <span class="string">&#x27;admin:0uek/hZ/V9fgiM35b0Z2226acMQ=&#x27;</span>);</span><br><span class="line">        <span class="comment">// 权限设置</span></span><br><span class="line">        acls.add(<span class="keyword">new</span> <span class="title class_">ACL</span>(ZooDefs.Perms.ALL, id));</span><br><span class="line">        zooKeeper.create(<span class="string">&#x27;/create/node7&#x27;</span>, <span class="string">&#x27;node7&#x27;</span>.getBytes(), acls, CreateMode.PERSISTENT);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create8</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 持久化顺序节点</span></span><br><span class="line">        <span class="comment">// Ids.OPEN_ACL_UNSAFE world:anyone:cdrwa</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> zooKeeper.create(<span class="string">&#x27;/create/node8&#x27;</span>, <span class="string">&#x27;node8&#x27;</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,</span><br><span class="line">                CreateMode.PERSISTENT_SEQUENTIAL);</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create9</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 临时节点</span></span><br><span class="line">        <span class="comment">// Ids.OPEN_ACL_UNSAFE world:anyone:cdrwa</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> zooKeeper.create(<span class="string">&#x27;/create/node9&#x27;</span>, <span class="string">&#x27;node9&#x27;</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create10</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 临时顺序节点</span></span><br><span class="line">        <span class="comment">// Ids.OPEN_ACL_UNSAFE world:anyone:cdrwa</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">result</span> <span class="operator">=</span> zooKeeper.create(<span class="string">&#x27;/create/node10&#x27;</span>, <span class="string">&#x27;node10&#x27;</span>.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE,</span><br><span class="line">                CreateMode.EPHEMERAL_SEQUENTIAL);</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">create11</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">count</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 异步方式创建节点</span></span><br><span class="line">        <span class="comment">//- path - znode路径。例如，/node1 /node1/node11</span></span><br><span class="line">        <span class="comment">// - data - 要存储在指定znode路径中的数据</span></span><br><span class="line">        <span class="comment">// - acl - 要创建的节点的访问控制列表。zookeeper API提供了一个静态接口 ZooDefs.Ids 来获取一些基本的acl列表。</span></span><br><span class="line">        <span class="comment">//          例如，ZooDefs.Ids.OPEN_ACL_UNSAFE 返回打开znode的acl列表。</span></span><br><span class="line">        <span class="comment">// - createMode - 节点的类型,这是一个枚举。</span></span><br><span class="line">        <span class="comment">// - callBack-异步回调接口</span></span><br><span class="line">        <span class="comment">// - ctx-传递上下文参数</span></span><br><span class="line">        zooKeeper.create(<span class="string">&#x27;/create/copy-node11&#x27;</span>, <span class="string">&#x27;node11&#x27;</span>.getBytes(),</span><br><span class="line">                ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT, (rc, path, ctx, name) -&gt; &#123;</span><br><span class="line">                    <span class="comment">// 0 代表创建成功</span></span><br><span class="line">                    System.out.println(<span class="string">&#x27;0 代表创建成功&#x27;</span> + rc);</span><br><span class="line">                    <span class="comment">// 节点的路径</span></span><br><span class="line">                    System.out.println(<span class="string">&#x27;节点的路径&#x27;</span> + path);</span><br><span class="line">                    <span class="comment">// 上下文参数</span></span><br><span class="line">                    System.out.println(<span class="string">&#x27;上下文参数&#x27;</span> + ctx);</span><br><span class="line">                    <span class="comment">// 节点的名称</span></span><br><span class="line">                    System.out.println(<span class="string">&#x27;节点的名称&#x27;</span> + name);</span><br><span class="line">                    count.countDown();</span><br><span class="line">                &#125;, <span class="string">&#x27;I am context&#x27;</span>);</span><br><span class="line">        System.out.println(<span class="string">&#x27;暂停&#x27;</span>);</span><br><span class="line">        count.await();</span><br><span class="line">        System.out.println(<span class="string">&#x27;结束&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行ZKCreate 程序，最终结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681869994786.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870001068.png" alt="img"></p>
<h3 id="4-4-更新节点"><a href="#4-4-更新节点" class="headerlink" title="4.4 更新节点"></a>4.4 更新节点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 同步方式</span><br><span class="line">setData(String path, byte[] data, int version)</span><br><span class="line"> </span><br><span class="line"> // 异步方式</span><br><span class="line">setData(String path, byte[] data, int version，AsyncCallback.StatCallback callBack， Object ctx)</span><br></pre></td></tr></table></figure>

<p>path- znode路径<br>data - 要存储在指定znode路径中的数据。<br>version- znode的当前版本。每当数据更改时，ZooKeeper会更新znode的版本<br>号。<br>callBack-异步回调接口<br>ctx-传递上下文参数</p>
<p>在zkCli客户端创建 &#x2F;set&#x2F;node1 节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># create /set “set”</span><br><span class="line"># create /set/node1 &#x27;node1&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870016404.png" alt="img"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Stat;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZKSet</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zookeeper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">before</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">IP</span> <span class="operator">=</span> <span class="string">&#x27;10.30.196.3:2181&#x27;</span>;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">countDownLatch</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// arg1:zookeeper服务器的ip地址和端口号</span></span><br><span class="line">        <span class="comment">// arg2:连接的超时时间 以毫秒为单位</span></span><br><span class="line">        <span class="comment">// arg3:监听器对象</span></span><br><span class="line">        zookeeper = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(IP, <span class="number">5000</span>, event -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (event.getState() == Watcher.Event.KeeperState.SyncConnected) &#123;</span><br><span class="line">                System.out.println(<span class="string">&#x27;连接创建成功!&#x27;</span>);</span><br><span class="line">                countDownLatch.countDown();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 使主线程阻塞等待we</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">after</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        zookeeper.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">set1</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// arg1:节点的路径</span></span><br><span class="line">        <span class="comment">// arg2:节点修改的数据</span></span><br><span class="line">        <span class="comment">// arg3:版本号 -1代表版本号不作为修改条件</span></span><br><span class="line">        <span class="type">Stat</span> <span class="variable">stat</span> <span class="operator">=</span> zookeeper.setData(<span class="string">&#x27;/set/node1&#x27;</span>, <span class="string">&#x27;node10&#x27;</span>.getBytes(), <span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 节点的版本号</span></span><br><span class="line">        System.out.println(<span class="string">&#x27;节点的版本号:&#x27;</span> + stat.getVersion());</span><br><span class="line">        <span class="comment">// 节点的创建时间</span></span><br><span class="line">        System.out.println(<span class="string">&#x27;节点的创建时间:&#x27;</span> + stat.getCtime());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">set2</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">count</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 异步方式修改节点</span></span><br><span class="line">        zookeeper.setData(<span class="string">&#x27;/set/node1&#x27;</span>, <span class="string">&#x27;node21&#x27;</span>.getBytes(), -<span class="number">1</span>, (rc, path, ctx, stat) -&gt; &#123;</span><br><span class="line">            <span class="comment">// 0 代表修改成功</span></span><br><span class="line">            System.out.println(rc);</span><br><span class="line">            <span class="comment">// 修改节点的路径</span></span><br><span class="line">            System.out.println(path);</span><br><span class="line">            <span class="comment">// 上线文的参数对象</span></span><br><span class="line">            System.out.println(ctx);</span><br><span class="line">            <span class="comment">// 的属性信息</span></span><br><span class="line">            System.out.println(stat.getVersion());</span><br><span class="line">            count.countDown();</span><br><span class="line">        &#125;, <span class="string">&#x27;I am Context&#x27;</span>);</span><br><span class="line">        count.await();</span><br><span class="line">        System.out.println(<span class="string">&#x27;结束&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行 ZKSet 程序，最终结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870027362.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870034117.png" alt="img"></p>
<h3 id="4-5删除节点"><a href="#4-5删除节点" class="headerlink" title="4.5删除节点"></a>4.5删除节点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 同步方式</span><br><span class="line">delete(String path, int version)</span><br><span class="line">// 异步方式</span><br><span class="line">delete(String path, int version, AsyncCallback.VoidCallback callBack, Object ctx)</span><br></pre></td></tr></table></figure>

<p>path - znode路径。<br>version - znode的当前版本<br>allBack-异步回调接口<br>ctx-传递上下文参数</p>
<p>在zkCli客户端创建 &#x2F;delete 节点并在其下再创建两个子节点分别为node1、node2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># create /delete &#x27;delete&#x27;</span><br><span class="line"># create /delete/node1 &#x27;node1&#x27;</span><br><span class="line"># create /delete/node2 &#x27;node2&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870048988.png" alt="img"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.*;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZKDelete</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zooKeeper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">before</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">IP</span> <span class="operator">=</span> <span class="string">&#x27;10.30.196.3:2181&#x27;</span>;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">countDownLatch</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// arg1:zookeeper服务器的ip地址和端口号</span></span><br><span class="line">        <span class="comment">// arg2:连接的超时时间 以毫秒为单位</span></span><br><span class="line">        <span class="comment">// arg3:监听器对象</span></span><br><span class="line">        zooKeeper = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(IP, <span class="number">5000</span>, event -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (event.getState() == Watcher.Event.KeeperState.SyncConnected) &#123;</span><br><span class="line">                System.out.println(<span class="string">&#x27;连接创建成功!&#x27;</span>);</span><br><span class="line">                countDownLatch.countDown();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 使主线程阻塞等待</span></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">after</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        zooKeeper.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">delete1</span><span class="params">()</span> <span class="keyword">throws</span> KeeperException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// arg1:删除节点的节点路径</span></span><br><span class="line">        <span class="comment">// arg2:数据版本信息 -1代表删除节点时不考虑版本信息</span></span><br><span class="line">        zooKeeper.delete(<span class="string">&#x27;/delete/node1&#x27;</span>, -<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">delete2</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">count</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 异步使用方式</span></span><br><span class="line">        zooKeeper.delete(<span class="string">&#x27;/delete/node2&#x27;</span>, -<span class="number">1</span>, (rc, path, ctx) -&gt; &#123;</span><br><span class="line">            <span class="comment">// 0代表删除成功</span></span><br><span class="line">            System.out.println(rc);</span><br><span class="line">            <span class="comment">// 节点的路径</span></span><br><span class="line">            System.out.println(path);</span><br><span class="line">            <span class="comment">// 上下文参数对象</span></span><br><span class="line">            System.out.println(ctx);</span><br><span class="line">            count.countDown();</span><br><span class="line">        &#125;, <span class="string">&#x27;I am Context&#x27;</span>);</span><br><span class="line">        count.await();</span><br><span class="line">        System.out.println(<span class="string">&#x27;结束&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行ZKDelete 程序，最终结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870059228.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870066250.png" alt="img"></p>
<h3 id="4-6查看节点"><a href="#4-6查看节点" class="headerlink" title="4.6查看节点"></a>4.6查看节点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 同步方式</span><br><span class="line">getData(String path, boolean b, Stat stat)</span><br><span class="line">// 异步方式</span><br><span class="line">getData(String path, boolean b，AsyncCallback.DataCallback callBack，Object ctx)</span><br></pre></td></tr></table></figure>

<p>path - znode路径。<br>b- 是否使用连接对象中注册的监视器。<br>stat - 返回znode的元数据。<br>callBack-异步回调接口<br>ctx-传递上下文参数</p>
<p>在zkCli客户端创建 &#x2F;get 节点并在其下再创建子节点为node1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># create /get &#x27;get&#x27;</span><br><span class="line"># create /get/node1 &#x27;node1&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870076315.png" alt="img"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Stat;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZKGet</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zooKeeper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">before</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">IP</span> <span class="operator">=</span> <span class="string">&#x27;10.30.196.3:2181&#x27;</span>;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">countDownLatch</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// arg1:zookeeper服务器的ip地址和端口号</span></span><br><span class="line">        <span class="comment">// arg2:连接的超时时间 以毫秒为单位</span></span><br><span class="line">        <span class="comment">// arg3:监听器对象</span></span><br><span class="line">        zooKeeper = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(IP, <span class="number">5000</span>, event -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (event.getState() == Watcher.Event.KeeperState.SyncConnected) &#123;</span><br><span class="line">                System.out.println(<span class="string">&#x27;连接创建成功!&#x27;</span>);</span><br><span class="line">                countDownLatch.countDown();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 使主线程阻塞等待</span></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">after</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        zooKeeper.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">get1</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// arg1:节点的路径</span></span><br><span class="line">        <span class="comment">// arg3:读取节点属性的对象</span></span><br><span class="line">        <span class="type">Stat</span> <span class="variable">stat</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Stat</span>();</span><br><span class="line">        <span class="type">byte</span>[] bys = zooKeeper.getData(<span class="string">&#x27;/get/node1&#x27;</span>, <span class="literal">false</span>, stat);</span><br><span class="line">        <span class="comment">// 打印数据</span></span><br><span class="line">        System.out.println(<span class="keyword">new</span> <span class="title class_">String</span>(bys));</span><br><span class="line">        <span class="comment">// 版本信息</span></span><br><span class="line">        System.out.println(stat.getVersion());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">get2</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">count</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">//异步方式</span></span><br><span class="line">        zooKeeper.getData(<span class="string">&#x27;/get/node1&#x27;</span>, <span class="literal">false</span>, (rc, path, ctx, data, stat) -&gt; &#123;</span><br><span class="line">            <span class="comment">// 0代表读取成功</span></span><br><span class="line">            System.out.println(rc);</span><br><span class="line">            <span class="comment">// 节点的路径</span></span><br><span class="line">            System.out.println(path);</span><br><span class="line">            <span class="comment">// 上下文参数对象</span></span><br><span class="line">            System.out.println(ctx);</span><br><span class="line">            <span class="comment">// 数据</span></span><br><span class="line">            System.out.println(<span class="keyword">new</span> <span class="title class_">String</span>(data));</span><br><span class="line">            <span class="comment">// 属性对象</span></span><br><span class="line">            System.out.println(stat.getVersion());</span><br><span class="line">            count.countDown();</span><br><span class="line">        &#125;, <span class="string">&#x27;I am Context&#x27;</span>);</span><br><span class="line">        System.out.println(<span class="string">&#x27;暂停&#x27;</span>);</span><br><span class="line">        count.await();</span><br><span class="line">        System.out.println(<span class="string">&#x27;结束&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行ZKGet 程序，最终结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870087407.png" alt="img"></p>
<h3 id="4-7查看子节点"><a href="#4-7查看子节点" class="headerlink" title="4.7查看子节点"></a>4.7查看子节点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 同步方式</span><br><span class="line"> getChildren(String path, boolean b)</span><br><span class="line"> // 异步方式</span><br><span class="line"> getChildren(String path, boolean b, AsyncCallback.ChildrenCallback callBack, Object ctx)</span><br></pre></td></tr></table></figure>

<p>path - Znode路径。<br>b- 是否使用连接对象中注册的监视器。<br>callBack - 异步回调接口。<br>ctx - 传递上下文参数</p>
<p>在zkCli客户端 &#x2F;get 节点再创建两个子节点为node2、node3</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># create /get/node2 &#x27;node2&#x27;</span><br><span class="line"># create /get/node3 &#x27;node3&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870098744.png" alt="img"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.AsyncCallback;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Stat;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZKGetChild</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zooKeeper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">before</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">IP</span> <span class="operator">=</span> <span class="string">&#x27;10.30.196.3:2181&#x27;</span>;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">countDownLatch</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// arg1:zookeeper服务器的ip地址和端口号</span></span><br><span class="line">        <span class="comment">// arg2:连接的超时时间 以毫秒为单位</span></span><br><span class="line">        <span class="comment">// arg3:监听器对象</span></span><br><span class="line">        zooKeeper = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(IP, <span class="number">5000</span>, event -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (event.getState() == Watcher.Event.KeeperState.SyncConnected) &#123;</span><br><span class="line">                System.out.println(<span class="string">&#x27;连接创建成功!&#x27;</span>);</span><br><span class="line">                countDownLatch.countDown();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 使主线程阻塞等待</span></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">after</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        zooKeeper.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">get1</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// arg1:节点的路径</span></span><br><span class="line">        List&lt;String&gt; list = zooKeeper.getChildren(<span class="string">&#x27;/get&#x27;</span>, <span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">for</span> (String str : list) &#123;</span><br><span class="line">            System.out.println(str);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">get2</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">countDownLatch</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 异步用法</span></span><br><span class="line">        zooKeeper.getChildren(<span class="string">&#x27;/get&#x27;</span>, <span class="literal">false</span>, (rc, path, ctx, children) -&gt; &#123;</span><br><span class="line">            <span class="comment">// 0代表读取成功</span></span><br><span class="line">            System.out.println(rc);</span><br><span class="line">            <span class="comment">// 节点的路径</span></span><br><span class="line">            System.out.println(path);</span><br><span class="line">            <span class="comment">// 上下文参数对象</span></span><br><span class="line">            System.out.println(ctx);</span><br><span class="line">            <span class="comment">// 子节点信息</span></span><br><span class="line">            <span class="keyword">for</span> (String str : children) &#123;</span><br><span class="line">                System.out.println(str);</span><br><span class="line">            &#125;</span><br><span class="line">            countDownLatch.countDown();</span><br><span class="line">        &#125;, <span class="string">&#x27;I am Context&#x27;</span>);</span><br><span class="line">        countDownLatch.await();</span><br><span class="line">        System.out.println(<span class="string">&#x27;结束&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行 ZKGetChildt 程序，最终结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870109248.png" alt="img"></p>
<h3 id="4-8检查节点是否存在"><a href="#4-8检查节点是否存在" class="headerlink" title="4.8检查节点是否存在"></a>4.8检查节点是否存在</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 同步方法</span><br><span class="line">exists(String path, boolean b)</span><br><span class="line">// 异步方法</span><br><span class="line">exists(String path, boolean b，AsyncCallback.StatCallback callBack, Objectctx)</span><br></pre></td></tr></table></figure>

<p>path- znode路径。<br>b- 是否使用连接对象中注册的监视器。<br>callBack - 异步回调接口。<br>ctx-传递上下文参数</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.data.Stat;</span><br><span class="line"><span class="keyword">import</span> org.junit.After;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.CountDownLatch;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ZKExists</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> ZooKeeper zookeeper;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">before</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">IP</span> <span class="operator">=</span> <span class="string">&#x27;10.30.196.3:2181&#x27;</span>;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">countDownLatch</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// arg1:zookeeper服务器的ip地址和端口号</span></span><br><span class="line">        <span class="comment">// arg2:连接的超时时间 以毫秒为单位</span></span><br><span class="line">        <span class="comment">// arg3:监听器对象</span></span><br><span class="line">        zookeeper = <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(IP, <span class="number">5000</span>, event -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (event.getState() == Watcher.Event.KeeperState.SyncConnected) &#123;</span><br><span class="line">                System.out.println(<span class="string">&#x27;连接创建成功!&#x27;</span>);</span><br><span class="line">                countDownLatch.countDown();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 使主线程阻塞等待</span></span><br><span class="line">        countDownLatch.await();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@After</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">after</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        zookeeper.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">exists1</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// arg1:节点的路径</span></span><br><span class="line">        <span class="type">Stat</span> <span class="variable">stat</span> <span class="operator">=</span> zookeeper.exists(<span class="string">&#x27;/get&#x27;</span>, <span class="literal">false</span>);</span><br><span class="line">        System.out.println(stat.getVersion());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">exists2</span><span class="params">()</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="type">CountDownLatch</span> <span class="variable">count</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CountDownLatch</span>(<span class="number">1</span>);</span><br><span class="line">        <span class="comment">// 异步使用方式</span></span><br><span class="line">        zookeeper.exists(<span class="string">&#x27;/exists1&#x27;</span>, <span class="literal">false</span>, (rc, path, ctx, stat) -&gt; &#123;</span><br><span class="line">            <span class="comment">// 0 判断成功</span></span><br><span class="line">            System.out.println(rc);</span><br><span class="line">            <span class="comment">// 路径</span></span><br><span class="line">            System.out.println(path);</span><br><span class="line">            <span class="comment">// 上下文参数</span></span><br><span class="line">            System.out.println(ctx);</span><br><span class="line">            <span class="comment">// null 节点不存在</span></span><br><span class="line">            System.out.println(stat);</span><br><span class="line">            count.countDown();</span><br><span class="line">        &#125;, <span class="string">&#x27;I am Context&#x27;</span>);</span><br><span class="line">        count.await();</span><br><span class="line">        System.out.println(<span class="string">&#x27;结束&#x27;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行 ZKExists 程序，最终结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870124680.png" alt="img"></p>
<h2 id="55-ZooKeeper实验：进程协作"><a href="#55-ZooKeeper实验：进程协作" class="headerlink" title="55.ZooKeeper实验：进程协作"></a>55.ZooKeeper实验：进程协作</h2><blockquote>
<h3 id="目的-13"><a href="#目的-13" class="headerlink" title="目的"></a>目的</h3><p>掌握Java代码如何连接ZooKeeper集群及通过代码读写ZooKeeper集群的目录下的数据，掌握ZooKeeper如何实现多个线程间的协作。</p>
<h3 id="要求-13"><a href="#要求-13" class="headerlink" title="要求"></a>要求</h3><p>用Java代码实现两个线程，一个向ZooKeeper中某一目录中写入数据，另一线程监听此目录，若目录下数据有更新则将目录中数据读取并显示出来。</p>
<h3 id="原理-13"><a href="#原理-13" class="headerlink" title="原理"></a>原理</h3><p>通过ZooKeeper实现不同物理机器上的进程间通信。<br>场景使用：客户端A需要向客户端B发送一条消息msg1。<br>实现方法：客户端A把msg1发送给ZooKeeper集群，然后由客户端B自行去ZooKeeper集群读取msg1。</p>
</blockquote>
<h3 id="4-1启动ZooKeeper集群"><a href="#4-1启动ZooKeeper集群" class="headerlink" title="4.1启动ZooKeeper集群"></a>4.1启动ZooKeeper集群</h3><p>启动ZooKeeper集群，具体步骤可以参考ZooKeeper实验：安装与部署</p>
<h3 id="4-2启动zkCli客户端"><a href="#4-2启动zkCli客户端" class="headerlink" title="4.2启动zkCli客户端"></a>4.2启动zkCli客户端</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/zookeeper/</span><br><span class="line"></span><br><span class="line"># bin/zkCli.sh -server master:2181,slave1:2181,slave2:2181</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870207314.png" alt="img"></p>
<p>创建&#x2F;testZK节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># create /testZK &#x27;test&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870213583.png" alt="img"></p>
<h3 id="4-3创建java项目"><a href="#4-3创建java项目" class="headerlink" title="4.3创建java项目"></a>4.3创建java项目</h3><p>项目架构如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870226327.png" alt="img"></p>
<h3 id="4-3-1-导入jar包"><a href="#4-3-1-导入jar包" class="headerlink" title="4.3.1 导入jar包"></a>4.3.1 导入jar包</h3><p>将zookeeper&#x2F;lib目录下的jar通过WinSCP,导入到IDEA开发工具中。</p>
<h3 id="4-3-2编写java代码"><a href="#4-3-2编写java代码" class="headerlink" title="4.3.2编写java代码"></a>4.3.2编写java代码</h3><p>WriteMsg 代码：</p>
<p>向&#x2F;testZk目录写数据线程代码实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WriteMsg</span> <span class="keyword">extends</span> <span class="title class_">Thread</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">ZooKeeper</span> <span class="variable">zk</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(<span class="string">&#x27;master:2181,slave1:2181,slave2:2181&#x27;</span>, <span class="number">500000</span>, <span class="literal">null</span>);</span><br><span class="line">            <span class="type">String</span> <span class="variable">content</span> <span class="operator">=</span> Long.toString(<span class="keyword">new</span> <span class="title class_">Date</span>().getTime());</span><br><span class="line"><span class="comment">//             修改节点/testZk下的数据，第三个参数为版本，如果是-1，那会无视被</span></span><br><span class="line"><span class="comment">//            修改的数据版本，直接改掉</span></span><br><span class="line">             zk.setData(<span class="string">&#x27;/testZK&#x27;</span>, content.getBytes(), -<span class="number">1</span>);</span><br><span class="line">            <span class="comment">// 关闭session</span></span><br><span class="line">            zk.close();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ReadMsg 代码：</p>
<p>监听&#x2F;testZk目录若数据改变则读取数据并显示线程代码实现：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.zookeeper.WatchedEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.Watcher;</span><br><span class="line"><span class="keyword">import</span> org.apache.zookeeper.ZooKeeper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ReadMsg</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">ZooKeeper</span> <span class="variable">zk</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ZooKeeper</span>(<span class="string">&#x27;master:2181,slave1:2181,slave2:2181&#x27;</span>, <span class="number">500000</span>, <span class="literal">null</span>);</span><br><span class="line">        <span class="comment">//定义watch</span></span><br><span class="line">        <span class="type">Watcher</span> <span class="variable">wacher</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Watcher</span>() &#123;</span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">process</span><span class="params">(WatchedEvent event)</span> &#123;</span><br><span class="line">                <span class="comment">//监听到数据变化取出数据</span></span><br><span class="line">                <span class="keyword">if</span>(Event.EventType.NodeDataChanged == event.getType())&#123;</span><br><span class="line">                    <span class="type">byte</span>[] bb;</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        bb = zk.getData(<span class="string">&#x27;/testZK&#x27;</span>, <span class="literal">null</span>, <span class="literal">null</span>);</span><br><span class="line">                        System.out.println(<span class="string">&#x27;/testZK的数据: &#x27;</span>+<span class="keyword">new</span> <span class="title class_">String</span>(bb));</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="comment">//设置watch</span></span><br><span class="line">        zk.exists(<span class="string">&#x27;/testZK&#x27;</span>, wacher);</span><br><span class="line">        <span class="comment">//更新/testZk目录信息，触发wacth</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">            <span class="keyword">new</span> <span class="title class_">WriteMsg</span>().start();</span><br><span class="line">            <span class="comment">//watch一次生效就会删除需重新设置</span></span><br><span class="line">            zk.exists(<span class="string">&#x27;/testZK&#x27;</span>, wacher);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-4做成jar包"><a href="#4-4做成jar包" class="headerlink" title="4.4做成jar包"></a>4.4做成jar包</h3><p>点击File -&gt; Project Structure… -&gt; Artifacts -&gt; + -&gt; jar  -&gt; From modules with dependencies… -&gt; OK -&gt; ok -&gt; 点击导航栏  Build -&gt; Build Artifacts… -&gt;Build  完成，我们再用WinSCP将我们的jar包上传至我们的集群中。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870247166.png" alt="img"></p>
<h3 id="4-5运行jar包"><a href="#4-5运行jar包" class="headerlink" title="4.5运行jar包"></a>4.5运行jar包</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># java -jar cn.cstor.zkdemo.jar</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681870254031.png" alt="img"></p>
<h2 id="56-Pig实验：Pig安装与部署"><a href="#56-Pig实验：Pig安装与部署" class="headerlink" title="56.Pig实验：Pig安装与部署"></a>56.Pig实验：Pig安装与部署</h2><blockquote>
<h3 id="目的-14"><a href="#目的-14" class="headerlink" title="目的"></a>目的</h3><p>本节实验将介绍 Pig 组件的安装，并通过案例来进行教学。</p>
<h3 id="要求-14"><a href="#要求-14" class="headerlink" title="要求"></a>要求</h3><p>搭建 Pig 环境<br>Pig 查询语言</p>
<h3 id="原理-14"><a href="#原理-14" class="headerlink" title="原理"></a>原理</h3><h3 id="什么是Pig"><a href="#什么是Pig" class="headerlink" title="什么是Pig?"></a>什么是Pig?</h3><p>Pig是一种数据流语言和运行环境，常用于检索和分析数据量较大的数据集。Pig包括两部分：一是用于描述数据流的语言，称为Pig Latin；二是用于运行Pig Latin程序的执行环境。</p>
<h3 id="Pig与Hive的区别？"><a href="#Pig与Hive的区别？" class="headerlink" title="Pig与Hive的区别？"></a>Pig与Hive的区别？</h3><p>Pig与Hive作为一种高级数据语言，均运行于HDFS之上，是hadoop上层的衍生架构，用于简化hadoop任务，并对MapReduce进行一个更高层次的封装。Pig与Hive的区别如下：<br>1、Pig是一种面向过程的数据流语言；Hive是一种数据仓库语言，并提供了完整的sql查询功能。<br>2、Pig更轻量级，执行效率更快，适用于实时分析；Hive适用于离线数据分析。<br>3、Hive查询语言为Hql，支持分区；Pig查询语言为Pig Latin，不支持分区。<br>4、Hive支持JDBC&#x2F;ODBC；Pig不支持JDBC&#x2F;ODBC。<br>5、Pig适用于半结构化数据(如：日志文件)；Hive适用于结构化数据。</p>
<p>用于使用Pig分析Hadoop中的数据的语言称为 Pig Latin  ，是一种高级数据处理语言，它提供了一组丰富的数据类型和操作符来对数据执行各种操作。要执行特定任务时，程序员使用Pig，需要用Pig  Latin语言编写Pig脚本，并使用任何执行机制（Grunt  Shell，UDFs，Embedded）执行它们。执行后，这些脚本将通过应用Pig框架的一系列转换来生成所需的输出。在内部，Apache  Pig将这些脚本转换为一系列MapReduce作业，因此，它使程序员的工作变得容易。Apache Pig的架构如下所示。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872516399.png" alt="img"></p>
<h3 id="Pig-有两种运行模式："><a href="#Pig-有两种运行模式：" class="headerlink" title="Pig 有两种运行模式："></a>Pig 有两种运行模式：</h3><p>Local 模式：Pig 运行于 Local 模式，只涉及到单独的一台计算机<br>MapReduce 模式：Pig 运行于 MapReduce 模式，需要能访问一个 Hadoop 集群，并且需要装上 HDFS</p>
<h3 id="Pig-的调用机制："><a href="#Pig-的调用机制：" class="headerlink" title="Pig 的调用机制："></a>Pig 的调用机制：</h3><p>Grunt shell 方式：通过交互的方式，输入命令执行任务。<br>Pig script 方式：通过 script 脚本的方式来运行任务。<br>嵌入式方式：嵌入 java 源代码中，通过 java 调用来运行任务</p>
</blockquote>
<h3 id="4-1启动hadoop"><a href="#4-1启动hadoop" class="headerlink" title="4.1启动hadoop"></a>4.1启动hadoop</h3><p>创建环境并一键搭建（实验环境内已启动hadoop无需再次启动）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># jps           确保 NameNode 和 DataNode 都有启动</span><br></pre></td></tr></table></figure>

<p>查看一下我们的集群中是否已经有了Pig（在我们的环境中已经安装了Pig，我们直接进行部署就好了）</p>
<h3 id="4-2安装Pig"><a href="#4-2安装Pig" class="headerlink" title="4.2安装Pig"></a>4.2安装Pig</h3><p>在 Apache 下载 Pig 软件包，点击下载会推荐最快的镜像站点（我们的平台已经安装了Pig组件，我们可以跳过这步，想自己安装的小伙伴可以输入以下命令尝试一下)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># wget http://archive.apache.org/dist/pig/pig-0.15.0/pig-0.15.0.tar.gz</span><br><span class="line"></span><br><span class="line"># tar -zxvf pig-0.15.0.tar.gz</span><br></pre></td></tr></table></figure>

<h3 id="4-3-部署Pig"><a href="#4-3-部署Pig" class="headerlink" title="4.3 部署Pig"></a>4.3 部署Pig</h3><p>设置环境变量<br>使用如下命令编辑 &#x2F;etc&#x2F;profile 文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/profile</span><br><span class="line"></span><br><span class="line"># export PIG_HOME=/usr/csor/pig</span><br><span class="line"># export PATH=$PIG_HOME/bin</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872534427-1750061213450-404.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872539922-1750061213450-406.png" alt="img"></p>
<p>修改完成后，我们输入以下命令激活我们修改的环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># source /etc/profile</span><br><span class="line"></span><br><span class="line"># pig -version    激活后就可以查看到我们的版本号</span><br></pre></td></tr></table></figure>

<p><img src="http://10.131.2.101/static/upload/exp/20220225155226_qijiarui_image.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872547421-1750061213450-408.png" alt="img"></p>
<h3 id="4-4pig-运行"><a href="#4-4pig-运行" class="headerlink" title="4.4pig 运行"></a>4.4pig 运行</h3><p>我们vim一个名为student.txt数据如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">学生信息（学号，姓名，性别，年龄，专业）</span><br><span class="line">2022022201:张三:男:21:美术</span><br><span class="line">2022022202:李四:男:20:舞蹈</span><br><span class="line">2022022203:王五:男:19:体育</span><br><span class="line">2022022204:小红:女:21:声乐</span><br><span class="line">2022022205:小明:男:20:美术</span><br><span class="line">2022022206:小美:女:19:舞蹈</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872555782.png" alt="img"></p>
<h3 id="4-4-1Local-模式"><a href="#4-4-1Local-模式" class="headerlink" title="4.4.1Local 模式"></a>4.4.1Local 模式</h3><p>本地运行模式下，Pig运行在单个JVM中，访问本地文件系统，该模式用于测试或处理小规模数据集。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># pig -x local</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872567444.png" alt="img"></p>
<h3 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h3><p>（1） Grunt Shell 方式<br>Grunt Shell方式Grunt Shell和Windows中的DOS窗口非常类似，在这里用户可以一条一条地输入命令对数据进行操作，我们来简单的读一个文件，启动命令如下所示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># a = load &#x27;/root/test/student.txt&#x27; using PigStorage(&#x27;:&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872579359.png" alt="img"></p>
<p>查看我们读取的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># dump a ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872589032.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872595140.png" alt="img"></p>
<p>（2）脚本文件</p>
<p>脚本文件方式使用脚本文件作为批处理作业来运行Pig命令，它实际上是第一种运行方式中命令的集合，使用如下命令可以运行Pig脚本。</p>
<p>test.pig 脚本内容如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872604057.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># pig -x local /root/test/test.pig</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872613609.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872620612.png" alt="img"></p>
<p>（3）嵌入式程序方式</p>
<p>我们可以把 Pig 命令嵌入到主机语言中，并且运行这个嵌入式程序。和运行普通的 Java 程序相同，这里需要书写特定的 Java  程序，并且将其编译生成对应的 class 文件或 package 包，然后再调用 main 函数运行程序。用户可以使用下面的命令对 Java  源文件进行编译：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># javac -cp pig-*.*.*-core.jar local.java</span><br></pre></td></tr></table></figure>

<p>这里“ pig-<em>.</em>.<em>-core.jar ”放在 Pig 安装目录下，“ local.java ”为用户编写的 java 源文件，并且“ pig-</em>.<em>.</em>-core.jar ”和“ local.java ”需要用户正确地指定相应的位置。例如，我们的“ pig-<em>.</em>.*-core.jar ”文件放在“ &#x2F;root&#x2F;hadoop-0.20.2&#x2F; ”目录下，“ local.java ”文件放在“ &#x2F;root&#x2F;pigTmp ”目录下，所以这一条命令我们应该写成：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># javac -cp /root/hadoop-0.20.2/ pig- 0 . 20 . 2 -core.jar /root/pigTmp/ local.java</span><br></pre></td></tr></table></figure>

<p>当编译完成后， Java 会生成“ local.class ”文件，然后用户可以通过如下命令调用执行此文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -cp pig-*.*.*-core.jar:. Local</span><br></pre></td></tr></table></figure>

<h3 id="4-4-2MapReduce-模式"><a href="#4-4-2MapReduce-模式" class="headerlink" title="4.4.2MapReduce 模式"></a>4.4.2MapReduce 模式</h3><p>在MapReduce运行模式下，Pig访问HDFS，并将查询翻译为MapReduce任务提交到Hadoop集群中进行处理。<br>需要我们将文件上传至HDFS，在HDFS上创建一个input目录并将数据上传</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir /input</span><br><span class="line"># hdfs dfs -put student.txt /input</span><br></pre></td></tr></table></figure>

<h3 id="运行机制-1"><a href="#运行机制-1" class="headerlink" title="运行机制"></a>运行机制</h3><p>（1） Grunt Shell 方式<br>  用户在 Linux 终端下输入如下命令进入 Grunt Shell 的 MapReduce 模式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># pig -x mapreduce</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872636537.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># a = load &#x27;hdfs://master:8020/input/student.txt&#x27; using PigStorage(&#x27;:&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872656114.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># dump a ;  过程有点久，请耐心等待</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872669392.png" alt="img"></p>
<p>（2） 脚本文件方式<br>  用户可以使用如下命令在 MapReduce 模式下运行 Pig 脚本文件。<br>需要我们.pig脚本文件位置更改一下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># pig -x mapreduce /root/test/test.pig 请耐心等待</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872679070.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681872699230.png" alt="img"></p>
<p>（3） 嵌入式程序<br>和Local模式相同，在 MapReduce 模式下运行嵌入式程序同样需要经过编译和执行两个步骤。用户可以使用如下两条命令，完成相应的操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># javac -cp pig-0.7.0-core.jar mapreduce.java</span><br><span class="line"></span><br><span class="line"># java -cp pig-0.7.0-core.jar:. mapreduce</span><br></pre></td></tr></table></figure>

<h2 id="57-Pig实验：PigLatin语法"><a href="#57-Pig实验：PigLatin语法" class="headerlink" title="57.Pig实验：PigLatin语法"></a>57.Pig实验：PigLatin语法</h2><blockquote>
<h3 id="目的-15"><a href="#目的-15" class="headerlink" title="目的"></a>目的</h3><p>本节课程主要以 Pig 的基本操作为主，面向有一定的脚本语言基础的学员，用于了解 Pig 的体系结构和使用方法。<br>请把文档中所有的示例代码都输入一遍，尽可能不要使用复制粘贴，只有这样才能够更加熟悉代码，遇到问题的话仔细对照文档。</p>
<h3 id="要求-15"><a href="#要求-15" class="headerlink" title="要求"></a>要求</h3><p>了解Pig 的特性<br>了解Pig 的架构<br>掌握 Pig Latin基础操作</p>
<h3 id="原理-15"><a href="#原理-15" class="headerlink" title="原理"></a>原理</h3><h3 id="Pig-的特性"><a href="#Pig-的特性" class="headerlink" title="Pig 的特性"></a>Pig 的特性</h3><p>目前，Pig 的基础设施层是由一个编译器组成的，该编译器会生成 MapReduce 程序的序列。 Pig 的语言层是由一种叫做 Pig Latin 的文本语言组成，它具有以下关键属性:<br>1、易于编程：Pig 可实现简单的并行数据分析任务，它可以把多个相互关联的数据构成的复杂任务显式编码为数据流的序列，从而使这些任务易于编写、理解和维护。<br>2、自动优化：上述任务的编码方式允许 Pig 自动对它们的执行进行优化，因此用户只需要关注语义，而不是计算效率。<br>3、可扩展性：用户可以创建自己的函数（UDF）来进行特殊目的的处理。</p>
<h3 id="Pig-架构"><a href="#Pig-架构" class="headerlink" title="Pig 架构"></a>Pig 架构</h3><p>使用 Pig 来进行分析数据的语言被称为 Pig Latin 。它是一种高级的数据处理语言，提供了一组丰富的数据类型和操作符，用于对数据执行各种操作。<br>通常，为了使用 Pig 来执行特定的任务，用户需要使用 Pig Latin 语言编写一个 Pig 的脚本，并通过一些执行机制（例如 Grunt Shell、 UDF 等）来执行这些脚本。执行之后，这些脚本将经过 Pig 框架的一系列转换，产生所需要的输出。</p>
</blockquote>
<p>Pig命令可以分为两大类：数据加载存储和数据转换。数据加载使用LOAD命令，加载数据可从本地或HDFS。存储使用STORE命令，数据可存储到本地或HDFS,STORE命令可触发编译器，将查询的逻辑计划编译为物理计划，DUMP命令也有此功能。</p>
<h3 id="4-1数据加载与储存"><a href="#4-1数据加载与储存" class="headerlink" title="4.1数据加载与储存"></a>4.1数据加载与储存</h3><h3 id="4-1-1导入数据（LOAD）"><a href="#4-1-1导入数据（LOAD）" class="headerlink" title="4.1.1导入数据（LOAD）"></a>4.1.1导入数据（LOAD）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># D=LOAD &#x27;mydata_d.txt&#x27; AS(</span><br><span class="line">id: chararray, name: chararray, age: int, salary: double, states: bag &#123;t: tuple(state1: chararray, state2: chararray)&#125;</span><br><span class="line">); </span><br><span class="line"></span><br><span class="line">DUMP语句的作用是输出关系到控制台</span><br><span class="line"># DUMP A;</span><br></pre></td></tr></table></figure>

<h3 id="4-1-2数据存储（STORE）"><a href="#4-1-2数据存储（STORE）" class="headerlink" title="4.1.2数据存储（STORE）"></a>4.1.2数据存储（STORE）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">使用USING PigStorage(&#x27;, &#x27;)指定存储的数据分隔符为逗号。</span><br><span class="line"># STORE C INTO &#x27;C&#x27;USING PigStorage(&#x27;,&#x27;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">使用USING JsonStorage()指定存储的数据为json类型,例如这数据id:1,name:Tom,age:21,salary:2000</span><br><span class="line"># STORE C INTO &#x27;C&#x27;USING JsonStorage();</span><br></pre></td></tr></table></figure>

<h3 id="4-2数据转换"><a href="#4-2数据转换" class="headerlink" title="4.2数据转换"></a>4.2数据转换</h3><h3 id="4-2-1分组命令（GROUP）"><a href="#4-2-1分组命令（GROUP）" class="headerlink" title="4.2.1分组命令（GROUP）"></a>4.2.1分组命令（GROUP）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># GROUP 关系名 BY(字段1，字段2，···)</span><br><span class="line"># GROUP 关系名 ALL;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2过滤命令（FILTER）"><a href="#4-2-2过滤命令（FILTER）" class="headerlink" title="4.2.2过滤命令（FILTER）"></a>4.2.2过滤命令（FILTER）</h3><p>filter 关系名 BY 表达式<br>FILTER语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># FA=FILTER salaries BY salary &gt;=10000.0; FB=FILTER salaries BY gender==&#x27;F&#x27; AND age 〉=50; FC=FILTER salaries BY NOT gender MATCHES &#x27;F&#x27;;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-3筛选命令（LIMIT）"><a href="#4-2-3筛选命令（LIMIT）" class="headerlink" title="4.2.3筛选命令（LIMIT）"></a>4.2.3筛选命令（LIMIT）</h3><p>LIMIT语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LIMIT 关系名 数值</span><br><span class="line">例如，基于代码清单5-1创建的关系salaries，使用LIMIT命令创建关系，命令如下：</span><br><span class="line"># salaries limit=LIMIT salaries 3；</span><br></pre></td></tr></table></figure>

<h3 id="4-2-4去重命令（DISTINCT）"><a href="#4-2-4去重命令（DISTINCT）" class="headerlink" title="4.2.4去重命令（DISTINCT）"></a>4.2.4去重命令（DISTINCT）</h3><p>DISTINCT语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DISTINCT 关系名；</span><br><span class="line">例如，基于代码清单5-1创建的关系salaries，使用LIMIT命令创建关系，命令如下：</span><br><span class="line"># unique_salaries=DISTINCT salaries；</span><br></pre></td></tr></table></figure>

<h3 id="4-2-5排序命令（ORDER-BY）"><a href="#4-2-5排序命令（ORDER-BY）" class="headerlink" title="4.2.5排序命令（ORDER BY）"></a>4.2.5排序命令（ORDER BY）</h3><p>ORDER BY语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ORDER 关系名 BY 表达式</span><br><span class="line">例如，基于代码清单5-1创建的关系salaries，使用ORDER BY命令创建关系，命令如下所示：</span><br><span class="line"># orderbyage=ORDER salaries BY age ASC；orderbyage_salary=ORDER salaries BY age ASC，salary DESC；</span><br></pre></td></tr></table></figure>

<h3 id="4-2-6遍历命令（FOREACH）"><a href="#4-2-6遍历命令（FOREACH）" class="headerlink" title="4.2.6遍历命令（FOREACH）"></a>4.2.6遍历命令（FOREACH）</h3><p>FOREACH语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FOREACH 关系名 GENERATE 表达式</span><br><span class="line"># A=FOREACH salaries GENERATE gender, age, salary; </span><br><span class="line"># A1=FOREACH salaries GENERATE.. salary;</span><br><span class="line"># B=FOREACH salaries GENERATE age.. zip; B1=FOREACH salaries GENERATE age..; </span><br><span class="line"># C=FOREACH salaries GENERATE salary, salary * 0.07 AS bonus; salariesbygender=GROUP salaries BY gender; group_count=FOREACH salariesbygender GENERATE group, COUNT(salarie s) AS salaries count;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-7嵌套命令（FOREACH）"><a href="#4-2-7嵌套命令（FOREACH）" class="headerlink" title="4.2.7嵌套命令（FOREACH）"></a>4.2.7嵌套命令（FOREACH）</h3><p>嵌套FOREACH语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">FOREACH关系名&#123;</span><br><span class="line">关系名1 = 命令表达式;</span><br><span class="line">关系名2 = 命令表达式;</span><br><span class="line">GENERATE表达式;</span><br><span class="line">&#125;；</span><br><span class="line">#嵌套FOREACH一般配合分组使用，其中中为子句，子句一般是对组进行的操作。</span><br><span class="line"># gender_grp=GROUP salaries BY gender; </span><br><span class="line"># unique_ages=FOREACH gender_grp&#123;</span><br><span class="line">ages=gender_grp. age; unique_age=DISTINCT ages; GENERATE group, COUNT(unique_age);</span><br></pre></td></tr></table></figure>

<h3 id="4-2-8分支命令（CASE）"><a href="#4-2-8分支命令（CASE）" class="headerlink" title="4.2.8分支命令（CASE）"></a>4.2.8分支命令（CASE）</h3><p>CASE语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FOREACH 关系名 GENERATE 字段名，</span><br><span class="line">(CASE WHEN 表达式1 THEN 表达式或字段名</span><br><span class="line">      WHEN 表达式1 THEN 表达式或字段名 </span><br><span class="line">      END）AS字段名；</span><br><span class="line"> </span><br><span class="line"># bonuses=FOREACH salaries GENERATE salary,(</span><br><span class="line">CASE </span><br><span class="line">WHEN salary 〉=70000.00 THEN salary *0.10</span><br><span class="line">WHEN salary&lt;70000.00 AND salary 〉=30000.0 THEN salary * 0.05</span><br><span class="line">WHEN salary〈=30000.0 THEN 0.0</span><br><span class="line">END) AS bonus;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-9连接命令（JOIN）"><a href="#4-2-9连接命令（JOIN）" class="headerlink" title="4.2.9连接命令（JOIN）"></a>4.2.9连接命令（JOIN）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">（1）内连接</span><br><span class="line">语法如下：</span><br><span class="line"># 关系3=J0IN 关系1 BY key1，关系2BY key2；</span><br><span class="line">（2）外连接</span><br><span class="line">外连接包括全连接、左连接、右连接3种连接方式。</span><br><span class="line">外连接语法如下：</span><br><span class="line"># 关系3=J0IN 关系1 BY key1 [FULL|LEFT|RIGHT] OUTER，关系2 BY key2；</span><br><span class="line"></span><br><span class="line"># outerjoin=JOIN loc BY firstname FULL OUTER,dep BY firstname;</span><br><span class="line"># leftjoin=JOIN loc BY firstname LEFT OUTER,dep BY firstname; </span><br><span class="line"># rightjoin=JOIN loc BY firstname RIGHT OUTER,dep BY firstname;</span><br></pre></td></tr></table></figure>

<h3 id="4-3-训练"><a href="#4-3-训练" class="headerlink" title="4.3 训练"></a>4.3 训练</h3><p>对以上的语句进行一下训练</p>
<h3 id="4-3-1准备数据"><a href="#4-3-1准备数据" class="headerlink" title="4.3.1准备数据"></a>4.3.1准备数据</h3><p>我们创建三个文件student.txt，course.txt，sc.txt，分别存储学生信息，课程信息，选课信息。实现找出成绩少于90的学生，并且输出学生的姓名和对应课程和成绩。样例数据如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">student.txt：</span><br><span class="line"></span><br><span class="line">学生信息（学号，姓名，性别，年龄，专业）</span><br><span class="line">2022022201:张三:男:21:美术</span><br><span class="line">2022022202:李四:男:20:舞蹈</span><br><span class="line">2022022203:王五:男:19:体育</span><br><span class="line">2022022204:小红:女:21:声乐</span><br><span class="line">2022022205:小明:男:20:美术</span><br><span class="line">2022022206:小美:女:19:舞蹈</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873083141.png" alt="img"></p>
<p>course.txt：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">课程信息（课号，课程名，学分）</span><br><span class="line">01,English,4</span><br><span class="line">02,Math,3</span><br><span class="line">03,Chinese,3</span><br><span class="line">04,History,3</span><br><span class="line">05,Biology,2</span><br><span class="line">06,Physics,2</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873095317.png" alt="img"></p>
<p>sc.txt：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">成绩信息（学号，课号，成绩）</span><br><span class="line">2022022201,01,92</span><br><span class="line">2022022201,03,83</span><br><span class="line">2022022202,01,84</span><br><span class="line">2022022202,04,91</span><br><span class="line">2022022203,02,90</span><br><span class="line">2022022203,05,81</span><br><span class="line">2022020004,01,72</span><br><span class="line">2022020004,04,83</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873106182.png" alt="img"></p>
<h3 id="4-3-2启动-grunt-shell"><a href="#4-3-2启动-grunt-shell" class="headerlink" title="4.3.2启动 grunt shell"></a>4.3.2启动 grunt shell</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># pig -x local</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873119339.png" alt="img"></p>
<h3 id="4-3-3读入数据"><a href="#4-3-3读入数据" class="headerlink" title="4.3.3读入数据"></a>4.3.3读入数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># a = load &#x27;/root/test/student.txt&#x27; using PigStorage(&#x27;:&#x27;) as (sno:chararray,sname:chararray,sex:chararray,age:int,dept:chararray) ;</span><br><span class="line"># dump a ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873130162.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873136295.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873146958.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># b = load &#x27;/root/test/course.txt&#x27; using PigStorage(&#x27;,&#x27;) as (cno:chararray,cname:chararray,grade:chararray) ;</span><br><span class="line"></span><br><span class="line"># dump b ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873156625.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873163132.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># c = load &#x27;/root/test/sc.txt&#x27; using PigStorage(&#x27;,&#x27;) as (sno:chararray,cno:chararray,score:float);</span><br><span class="line"># dump c ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873176823.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873182947.png" alt="img"></p>
<h3 id="4-3-4-数据处理"><a href="#4-3-4-数据处理" class="headerlink" title="4.3.4 数据处理"></a>4.3.4 数据处理</h3><p>要求低于90分的学生信息，但是在每一个表中，都没有全面的而对应信息，所以使用join连接这些表，首先连接a与c(首先连接哪两个看个人意愿)，两个里面都有sno，根据sno去连接并查看结果。输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># a_join_c = join a by sno, c by sno ;</span><br><span class="line"># dump a_join_c ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873195328.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873203634.png" alt="img"></p>
<p>查看连接后的各个字段信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># describe a_join_c ;</span><br></pre></td></tr></table></figure>

<p>在连接a_join_c与另外的一个b（里面的c::cno是因为在c表中有与b表相同的一个内容，需要使用相同字段连接，而且连接是在a_join_c，这个里面的表字段比较多，这样写还便于区分）（使用了如下命令之后，都可以使用dump 变量名 的格式去查看变量里面的内容）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># abc_join = join b by cno,a_join_c by c::cno ; </span><br><span class="line"># dump abc_join ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873216907.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873225154.png" alt="img"><br>使用filter去筛选成绩少于90的学生信息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># not_excl = filter abc_join by score &lt; 90 ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873233442.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873240577.png" alt="img"></p>
<p>在成绩少于90的学生信息中，去查找学生姓名，对应课程名与成绩。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># foreach_data = foreach not_excl generate sname,cname,score ; </span><br><span class="line"># dump foreach_data ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873262660.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873269256.png" alt="img"></p>
<h3 id="4-3-5数据储存"><a href="#4-3-5数据储存" class="headerlink" title="4.3.5数据储存"></a>4.3.5数据储存</h3><p>我们将我们处理的数据储存到本地</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># store foreach_data into &#x27;/root/test/foreach_data&#x27; using PigStorage(&#x27;,&#x27;) ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873281273.png" alt="img"></p>
<p>退出 grunt shell ，查看我们的 分析出数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># quit</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873288659.png" alt="img"></p>
<blockquote>
<p>退出后我们进去我们保存文件的目录进行查看 <code># cd foreach_data/ # cat part-r-00000</code> <img src="/static/upload/resource/exp/ins/873e2a33b3e44a7c8a846275064d4aa9/image/image.1681873303418.png" alt="image.png"> 最后我们看到数据的结果，发现李四的英语、张三的语文、王五的生物成绩少于90分，训练结束。</p>
</blockquote>
<h2 id="58-Pig实验：Pig分析数据"><a href="#58-Pig实验：Pig分析数据" class="headerlink" title="58.Pig实验：Pig分析数据"></a>58.Pig实验：Pig分析数据</h2><blockquote>
<h3 id="目的-16"><a href="#目的-16" class="headerlink" title="目的"></a>目的</h3><p>1、在Pig的Hadoop模式下统计任意一个HDFS文本中的单词出现次数，并将统计结果打印出来；<br>2、用Pig查询统计HDFS文件系统中结构化的数据。</p>
<h3 id="要求-16"><a href="#要求-16" class="headerlink" title="要求"></a>要求</h3><p>了解Pig和MapReduce之间的关系，掌握Pig Latin编程语言，加深对Pig相关概念的理解。</p>
<h3 id="原理-16"><a href="#原理-16" class="headerlink" title="原理"></a>原理</h3><p>Pig是一种探索大规模数据集的脚本语言。MapReducer的一个主要的缺点就是开发的周期太长了。我们要编写mapper和reducer，然后对代码进行编译打出jar包，提交到本地的JVM或者是hadoop的集群上，最后获取结果，这个周期是非常耗时的，即使使用Streaming  (它是hadoop的一个工具，用来创建和运行一类特殊的map&#x2F;reduce作业。所谓的特殊的map&#x2F;reduce作业可以是可执行文件或脚本本件（python、PHP、c等）。Streaming使用“标准输入”和“标准输出”与我们编写的Map和Reduce进行数据的交换。由此可知，任何能够使用“标准输入”和“标准输出”的编程语言都可以用来编写MapReduce程序，能在这个过程中除去代码的编译和打包的步骤，但是这一个过程还是很耗时， Pig的强大之处 就是他只要 几行Pig Latin代码就能处理TB级别的数据 。Pig提供了多个命令用于 检查和处理程序中的数据结构  ，因此它能很好的支持我们写 查询 。Pig的一个很有用的特性就是它支持在输入数据中有代表性的一个小的数据集上试运行。  所以。我们在处理大的数据集前可以用那一个小的数据集检查我们的程序是不是有错误的。</p>
<p>Pig为大型的数据集的处理提供了更高层次的抽象。MapReducer能够让我们自己定义连续执行的map和reduce函数，但是数据处理往往需要很多的MapReducer过程才能实现，所以将数据处理要求改写成MapReducer模式是很复杂的。和MapReducer相比，Pig提供了更加丰富的数据结构，一般都是多值和嵌套的数据结构。Pig还提供了一套更强大的数据交换操作，包括了MapReducer中被忽视的”join”操作。</p>
<p>Pig被设计为可以扩展的，处理路径上的每一个部分，载入，存储，过滤，分组，连接，都是可以定制的，这些操作都可以使用用户定义函数（user-defined function,  UDF）进行修改，这些函数作用于Pig的嵌套数据模型。因此，它们可以在底层与Pig的操作集成，UDF的另外的一个好处是它们比MapReducer程序开发的库更易于重用。</p>
<p>但是，Pig并不适合处理所有的“数据处理”任务。和MapReducer一样，它是为数据 批处理 而设计的，如果想执行的查询只涉及一个大型数据集的一小部分数据，Pig的实现不是很好， 因为它要扫描整个数据集或其中的很大一部分。</p>
<p>Pig Latin程序是由一系列的” 操作”(operation)或”变换”(transformation)组成。每个操作或变换对输入进行 数据处理 ，然后产生 输出的结果 。这些操作整体上描述了一个 数据流  ，Pig执行的环境把数据流翻译为可执行的内部表示，并运行它。在Pig的内部，这些变换和操作被转换成一系列的MapReducer，但是我们一般情况下并不知道这些转换是怎么进行的，我们的主要的精力就花在数据上，而不是执行的细节上面。</p>
<p>在有些情况下，Pig的表现不如MapReducer程序。总结为一句就是要么话花大量的时间来优化Java MapReducer程序，要么使用Pig Latin来编写查询确实能节约时间。</p>
</blockquote>
<h3 id="4-1非结构化单词计数"><a href="#4-1非结构化单词计数" class="headerlink" title="4.1非结构化单词计数"></a>4.1非结构化单词计数</h3><h3 id="4-1-1准备数据"><a href="#4-1-1准备数据" class="headerlink" title="4.1.1准备数据"></a>4.1.1准备数据</h3><p>在HDFS文件系统创建一个input目录，并从本地上传任意一个文件到目录中，为后面的Pig单词统计准备数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir /input</span><br><span class="line"># hdfs dfs -ls /</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873349626.png" alt="img"></p>
<p>我们就任意分析一个数据，比如Hadoop下的README.txt文件，将它上传至HDFS</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop</span><br><span class="line"># ls</span><br><span class="line"># cat README.txt</span><br><span class="line"># hdfs dfs -put README.txt /input</span><br><span class="line"># hdfs dfs -ls /input</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873366643.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873383777.png" alt="img"></p>
<h3 id="4-1-2启动grunt-shell"><a href="#4-1-2启动grunt-shell" class="headerlink" title="4.1.2启动grunt shell"></a>4.1.2启动grunt shell</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/pig</span><br><span class="line"></span><br><span class="line"># pig -x local</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873452643.png" alt="img"></p>
<h3 id="4-1-3读取并转换数据"><a href="#4-1-3读取并转换数据" class="headerlink" title="4.1.3读取并转换数据"></a>4.1.3读取并转换数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># a = load &#x27;hdfs://master:8020/input/README.txt&#x27; as (line: chararray) ;   按行读取HDFS文件</span><br><span class="line"># b = foreach a generate flatten(TOKENIZE(line,&#x27;\t ,.&#x27;))as word;        将每行单词用tab、逗号，句号以及空格分隔单词</span><br><span class="line"># dump b ；</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873470319.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873476692.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873484973.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># c = group b by word ;       按单词分组，将相同的单词归并到一起</span><br><span class="line"># dump c ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873494493.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873501725.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># d = foreach c generate group, COUNT(b)  as count ;     统计每个单词的个数</span><br><span class="line"># dump d ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873509546.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873516334.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># e = order d by count desc ;   将统计出的次数来排序（降序）</span><br><span class="line"># dump e ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873524601.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873530192.png" alt="img"></p>
<h3 id="4-2分析结构化数据"><a href="#4-2分析结构化数据" class="headerlink" title="4.2分析结构化数据"></a>4.2分析结构化数据</h3><h3 id="4-2-1准备数据"><a href="#4-2-1准备数据" class="headerlink" title="4.2.1准备数据"></a>4.2.1准备数据</h3><p>数据内容如下并上传至HDFS：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim student.txt</span><br><span class="line"></span><br><span class="line">2022022201:张三:男:21:美术</span><br><span class="line">2022022202:李四:男:20:舞蹈</span><br><span class="line">2022022203:王五:男:19:体育</span><br><span class="line">2022022204:小红:女:21:声乐</span><br><span class="line">2022022205:小明:男:20:美术</span><br><span class="line">2022022206:小美:女:19:舞蹈</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873542581.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -put student.txt /input </span><br><span class="line"># hdfs dfs -ls /input</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873550400.png" alt="img"></p>
<h3 id="4-2-2读取数据"><a href="#4-2-2读取数据" class="headerlink" title="4.2.2读取数据"></a>4.2.2读取数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># A = load &#x27;hdfs://master:8020/input/student.txt&#x27; using PigStorage(&#x27;:&#x27;) as (sno:chararray,sname:chararray,sex:chararray,age:int,dept:chararray) ;      读取HDFS文件，并将数据按冒号分割解析 学号、姓名、性别、年龄、专业</span><br><span class="line"># dump A ;          打印A中的数据</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873562475.png" alt="img"></p>
<h3 id="4-2-3读取并转换数据"><a href="#4-2-3读取并转换数据" class="headerlink" title="4.2.3读取并转换数据"></a>4.2.3读取并转换数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># B = foreach A generate sname ;    只查询sname字段，并打印</span><br><span class="line"># dump B ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873569613.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873575988.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># describe A ;    查看A的详细结构内容</span><br><span class="line"># C = foreach A generate sname, sex, age+1 ;  查询姓名、性别并将年龄加1</span><br><span class="line"># dump C ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873587190.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873594288.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># D = filter A by (age &gt; 20) ;  查询年龄大于20岁的人员</span><br><span class="line"># dump D ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873601102.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873611519.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># E = group A all ;       统计数据总条数</span><br><span class="line"># F = foreach E generate COUNT (A) ;</span><br><span class="line"># dump F ;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873621097.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/image.1681873627581.png" alt="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">tong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/">http://example.com/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/./img/touxiang.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/./img/WeChat_pay.jpg" target="_blank"><img class="post-qr-code-img" src="/./img/WeChat_pay.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/./img/Alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/./img/Alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/" title="Hadoop应用与开发2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Hadoop应用与开发2</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发221.Spark实验：SparkSQL入门实战 目的1.学习spark的入门编程 要求2.掌握spark基础操作和原理 原理Spark SQLSpark SQL 是 Spark 用来处理结构化数据的模块，通过 Spark SQL 我们可以像之前使用 SQL 语句分析关系型数据库表一样方便地在  Spark 上对海量结构化数据进行快速分析，我们只需要关注数据分析的逻辑而不需要担心底层分布式存储、计算、通信、以及作业解析和调度的细节。 Spark SQL 支持从 JSON 文件、CSV 文件、Hive 表、Parquest 文件中读取数据，通过 SQL  语句对数据进行交互式查询；也可以读取传统关系型数据库中的数据进行数据分析。同时还能与传统的 RDD 编程相结合，让我们能够同时使用 SQL 和  RDD 进行复杂的数据分析。 另外，它还能与其它核心组件比如： Spark Streaming、MLlib、GraphX 等联合使用进行数据分析，所以 Spark SQL...</div></div></div></a><a class="pagination-related" href="/2025/06/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%914/" title="Hadoop应用与开发4"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hadoop应用与开发4</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发459.Hadoop实验：HDFS基础与部署 目的1.理解大数据生态圈各组件特性2.理解HDFS存在的优势3.理解HDFS体系架构4.学会在环境中部署HDFS学会HDFS基本命令 要求1.要求实验结束时，能够构建出HDFS集群2.要求能够在构建出的HDFS集群上使用基本的HDFS命令3.要求能够大致了解Hadoop生态圈中各个组件 原理3.1 Hadoop 生态：Hadoop是一个提供高可靠，可扩展（横向）的分布式计算的开源软件平台。1.Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构。2.主要解决，海量数据的存储和海量数据的分析计算问题。3.我们现在讲HADOOP 通常是指一个更广泛的概念——HADOOP...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/" title="Hadoop应用与开发1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-20</div><div class="info-item-2">Hadoop应用与开发1</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/" title="Hadoop应用与开发2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-11</div><div class="info-item-2">Hadoop应用与开发2</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发221.Spark实验：SparkSQL入门实战 目的1.学习spark的入门编程 要求2.掌握spark基础操作和原理 原理Spark SQLSpark SQL 是 Spark 用来处理结构化数据的模块，通过 Spark SQL 我们可以像之前使用 SQL 语句分析关系型数据库表一样方便地在  Spark 上对海量结构化数据进行快速分析，我们只需要关注数据分析的逻辑而不需要担心底层分布式存储、计算、通信、以及作业解析和调度的细节。 Spark SQL 支持从 JSON 文件、CSV 文件、Hive 表、Parquest 文件中读取数据，通过 SQL  语句对数据进行交互式查询；也可以读取传统关系型数据库中的数据进行数据分析。同时还能与传统的 RDD 编程相结合，让我们能够同时使用 SQL 和  RDD 进行复杂的数据分析。 另外，它还能与其它核心组件比如： Spark Streaming、MLlib、GraphX 等联合使用进行数据分析，所以 Spark SQL...</div></div></div></a><a class="pagination-related" href="/2025/06/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%914/" title="Hadoop应用与开发4"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">Hadoop应用与开发4</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发459.Hadoop实验：HDFS基础与部署 目的1.理解大数据生态圈各组件特性2.理解HDFS存在的优势3.理解HDFS体系架构4.学会在环境中部署HDFS学会HDFS基本命令 要求1.要求实验结束时，能够构建出HDFS集群2.要求能够在构建出的HDFS集群上使用基本的HDFS命令3.要求能够大致了解Hadoop生态圈中各个组件 原理3.1 Hadoop 生态：Hadoop是一个提供高可靠，可扩展（横向）的分布式计算的开源软件平台。1.Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构。2.主要解决，海量数据的存储和海量数据的分析计算问题。3.我们现在讲HADOOP 通常是指一个更广泛的概念——HADOOP...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/./img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">tong</div><div class="author-info-description">这里是描述</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">现在没有公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913"><span class="toc-text">Hadoop应用与开发3</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#42-Flink%E5%AE%9E%E9%AA%8C%EF%BC%9AFlink%E7%AE%80%E4%BB%8B%E4%B8%8E%E9%85%8D%E7%BD%AE"><span class="toc-text">42.Flink实验：Flink简介与配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1Flink%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.1Flink介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2Flink%E7%89%B9%E6%80%A7"><span class="toc-text">3.2Flink特性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3Flink%E5%8A%9F%E8%83%BD%E6%A8%A1%E5%9D%97"><span class="toc-text">3.3Flink功能模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4Flink%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.4Flink编程模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5Flink%E6%9E%B6%E6%9E%84%E6%A8%A1%E5%9E%8B%E5%9B%BE"><span class="toc-text">3.5Flink架构模型图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1Flink%E7%9A%84local%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="toc-text">4.1Flink的local模式部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1%E7%9B%B4%E6%8E%A5%E5%90%AF%E5%8A%A8local%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.1.1直接启动local模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2%E6%9F%A5%E7%9C%8B%E8%BF%9B%E7%A8%8Bjps"><span class="toc-text">4.1.2查看进程jps</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-3%E9%AA%8C%E8%AF%81"><span class="toc-text">4.1.3验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-4%E5%85%B3%E9%97%AD%E5%8D%95%E8%8A%82%E7%82%B9flink"><span class="toc-text">4.1.4关闭单节点flink</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2Flink%E7%9A%84Standalone%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="toc-text">4.2Flink的Standalone模式部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1%E6%9B%B4%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">4.2.1更改配置文件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9flink-conf-yaml"><span class="toc-text">修改flink-conf.yaml</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9masters%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">修改masters配置文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9slaves%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">修改slaves配置文件</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2%E5%AE%89%E8%A3%85%E5%8C%85%E5%88%86%E5%8F%91"><span class="toc-text">4.2.2安装包分发</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3%E5%90%AF%E5%8A%A8flink%E9%9B%86%E7%BE%A4"><span class="toc-text">4.2.3启动flink集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4%E9%A1%B5%E9%9D%A2%E8%AE%BF%E9%97%AE"><span class="toc-text">4.2.4页面访问</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-5%E8%BF%90%E8%A1%8Cflink%E8%87%AA%E5%B8%A6%E7%9A%84%E6%B5%8B%E8%AF%95%E7%94%A8%E4%BE%8B"><span class="toc-text">4.2.5运行flink自带的测试用例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3Flink%E7%9A%84Standalone%E6%A8%A1%E5%BC%8F%E7%9A%84HA%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2"><span class="toc-text">4.3Flink的Standalone模式的HA环境部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E9%A6%96%E5%85%88%E5%90%AF%E5%8A%A8Hadoop%E9%9B%86%E7%BE%A4"><span class="toc-text">4.3.1首先启动Hadoop集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">4.3.2修改配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3hdfs%E4%B8%8A%E9%9D%A2%E5%88%9B%E5%BB%BAflink%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-text">4.3.3hdfs上面创建flink对应的文件夹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4%E6%8B%B7%E8%B4%9D%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">4.3.4拷贝配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-5%E5%90%AF%E5%8A%A8zookeeper%E5%92%8Cflink%E9%9B%86%E7%BE%A4"><span class="toc-text">4.3.5启动zookeeper和flink集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4flink-on-yarn%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.4flink on yarn模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1%E4%BF%AE%E6%94%B9yarn-site-xml%E6%96%87%E4%BB%B6"><span class="toc-text">4.4.1修改yarn-site.xml文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2%E5%90%AF%E5%8A%A8Hadoop%E9%9B%86%E7%BE%A4%EF%BC%88HDFS%E5%92%8CYarn%EF%BC%89"><span class="toc-text">4.4.2启动Hadoop集群（HDFS和Yarn）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-3%E5%90%91yarn%E9%9B%86%E7%BE%A4%E7%94%B3%E8%AF%B7%E8%B5%84%E6%BA%90"><span class="toc-text">4.4.3向yarn集群申请资源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-4%E6%8F%90%E4%BA%A4%E4%BB%BB%E5%8A%A1"><span class="toc-text">4.4.4提交任务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#43-%E7%BB%BC%E5%90%88%E5%AE%9E%E6%88%98%EF%BC%9A%E7%8E%AF%E5%A2%83%E5%A4%A7%E6%95%B0%E6%8D%AE"><span class="toc-text">43.综合实战：环境大数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-1"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-1"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-1"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6"><span class="toc-text">4.1 分析数据文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%B0%86%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E8%87%B3HDFS"><span class="toc-text">4.2 将数据文件上传至HDFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E7%BC%96%E5%86%99%E6%9C%88%E5%B9%B3%E5%9D%87%E6%B0%94%E6%B8%A9%E7%BB%9F%E8%AE%A1%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.3 编写月平均气温统计程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%9F%A5%E7%9C%8B%E6%9C%88%E5%B9%B3%E5%9D%87%E6%B0%94%E6%B8%A9%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C"><span class="toc-text">4.4 查看月平均气温统计结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E7%BC%96%E5%86%99%E6%AF%8F%E6%97%A5%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E7%BB%9F%E8%AE%A1%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.5 编写每日空气质量统计程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E6%9F%A5%E7%9C%8B%E6%AF%8F%E6%97%A5%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C"><span class="toc-text">4.6 查看每日空气质量统计结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-%E5%B0%86%E6%AF%8F%E6%97%A5%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E7%BB%9F%E8%AE%A1%E6%96%87%E4%BB%B6%E8%BF%9B%E8%A1%8C%E6%95%B4%E5%90%88"><span class="toc-text">4.7 将每日空气质量统计文件进行整合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-%E7%BC%96%E5%86%99%E5%90%84%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E5%A4%A9%E6%95%B0%E7%BB%9F%E8%AE%A1%E7%A8%8B%E5%BA%8F"><span class="toc-text">4.8 编写各空气质量天数统计程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-9-%E6%9F%A5%E7%9C%8B%E5%90%84%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E5%A4%A9%E6%95%B0%E7%BB%9F%E8%AE%A1%E7%BB%93%E6%9E%9C"><span class="toc-text">4.9 查看各空气质量天数统计结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#44-Flink%E5%AE%9E%E9%AA%8C%EF%BC%9A%E4%BD%BF%E7%94%A8%E5%86%85%E7%BD%AEScala-Shell"><span class="toc-text">44.Flink实验：使用内置Scala-Shell</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-2"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-2"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-2"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8flink%E7%9A%84scala-shell"><span class="toc-text">4.1启动flink的scala-shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E9%AA%8C%E8%AF%81%E4%B8%80%E4%B8%8BScala%E7%9A%84Hello-World"><span class="toc-text">4.2验证一下Scala的Hello World</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E4%BD%BF%E7%94%A8Flink%E9%9B%86%E7%BE%A4"><span class="toc-text">4.3使用Flink集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E8%BF%9C%E7%A8%8BFlink%E9%9B%86%E7%BE%A4"><span class="toc-text">4.3.1远程Flink集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F-yarn-scala-shell-cluster"><span class="toc-text">4.3.2集群模式 yarn scala shell cluster</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3yarn-session%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.3.3yarn session模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E5%AE%8C%E6%95%B4%E7%9A%84%E5%8F%82%E6%95%B0%E9%80%89%E9%A1%B9"><span class="toc-text">4.4完整的参数选项</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1flink-run%E5%8F%82%E6%95%B0"><span class="toc-text">4.4.1flink run参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2flink-run-m-yarn-cluster%E5%8F%82%E6%95%B0"><span class="toc-text">4.4.2flink run -m yarn-cluster参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-3flink-list"><span class="toc-text">4.4.3flink-list</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-4flink-cancel"><span class="toc-text">4.4.4flink cancel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-5flink-stop-%E4%BB%85%E4%BB%85%E9%92%88%E5%AF%B9Streaming-job"><span class="toc-text">4.4.5flink stop :仅仅针对Streaming job</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-6Flink-Scala-Shell"><span class="toc-text">4.4.6Flink Scala Shell</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#45-Flink%E5%AE%9E%E9%AA%8C%EF%BC%9A%E7%AE%97%E5%AD%90%E5%AE%9E%E4%BE%8B%E8%A7%A3%E6%9E%90"><span class="toc-text">45.Flink实验：算子实例解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-3"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-3"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-3"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8flink-shell"><span class="toc-text">4.1启动flink shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86%E5%92%8C%E8%BD%AC%E5%8C%96"><span class="toc-text">4.2数据流进行处理和转化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1map"><span class="toc-text">4.2.1map</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2filter"><span class="toc-text">4.2.2filter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3flatMap"><span class="toc-text">4.2.3flatMap</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3Key%E7%9A%84%E5%88%86%E7%BB%84%E8%BD%AC%E6%8D%A2"><span class="toc-text">4.3Key的分组转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1keyBy"><span class="toc-text">4.3.1keyBy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2aggregation"><span class="toc-text">4.3.2aggregation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3reduce"><span class="toc-text">4.2.3reduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BD%AC%E6%8D%A2"><span class="toc-text">4.4多数据流转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1union"><span class="toc-text">4.4.1union</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2connect"><span class="toc-text">4.4.2connect</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E5%B9%B6%E8%A1%8C%E5%BA%A6%E5%92%8C%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%86%E5%B8%83"><span class="toc-text">4.5并行度和数据重分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-1%E5%B9%B6%E8%A1%8C%E5%BA%A6"><span class="toc-text">4.5.1并行度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-2%E6%95%B0%E6%8D%AE%E9%87%8D%E5%88%86%E5%B8%83"><span class="toc-text">4.5.2数据重分布</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89shuffle"><span class="toc-text">（1）shuffle</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89rebalance%E4%B8%8Erescale"><span class="toc-text">（2）rebalance与rescale</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%883%EF%BC%89broadcast"><span class="toc-text">（3）broadcast</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%884%EF%BC%89global"><span class="toc-text">（4）global</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%885%EF%BC%89partitionCustom"><span class="toc-text">（5）partitionCustom</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#46-Flink%E6%8C%91%E6%88%98%EF%BC%9A%E7%94%A8Flink%E5%AE%9E%E7%8E%B0Wordcount"><span class="toc-text">46.Flink挑战：用Flink实现Wordcount</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-4"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-4"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-4"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E4%BD%BF%E7%94%A8Streaming%E7%8E%AF%E5%A2%83"><span class="toc-text">4.1使用Streaming环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E4%BD%BF%E7%94%A8Batch%E7%8E%AF%E5%A2%83"><span class="toc-text">4.2使用Batch环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E4%BD%BF%E7%94%A8%E5%85%B6%E4%BB%96%E4%BE%9D%E8%B5%96"><span class="toc-text">4.3使用其他依赖</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#47-Sqoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%9F%BA%E7%A1%80%E4%B8%8E%E6%90%AD%E5%BB%BA"><span class="toc-text">47.Sqoop实验：基础与搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-5"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-5"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-5"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1Sqoop%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-text">3.1Sqoop基本原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%BD%95%E4%B8%BASqoop%EF%BC%9F"><span class="toc-text">3.1 何为Sqoop？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E7%94%A8Sqoop%EF%BC%9F"><span class="toc-text">3.1.2 为什么需要用Sqoop？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-%E5%85%B3%E7%B3%BB%E5%9B%BE"><span class="toc-text">3.1.3 关系图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-text">3.1.4 架构图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%B8%8B%E8%BD%BD%E5%B9%B6%E8%A7%A3%E5%8E%8B"><span class="toc-text">4.1 下载并解压</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">4.2 修改配置文件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#48-Sqoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5"><span class="toc-text">48.Sqoop实验：数据导入</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-6"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-6"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-6"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E5%AF%BC%E5%85%A5%E5%8E%9F%E7%90%86"><span class="toc-text">3.1导入原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E9%AA%8C%E8%AF%81sqoop%E6%98%AF%E5%90%A6%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F"><span class="toc-text">4.1验证sqoop是否安装成功</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%88%9D%E5%A7%8B%E5%8C%96mysql"><span class="toc-text">4.2 初始化mysql</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E5%85%A8%E9%83%A8%E5%AF%BC%E5%85%A5"><span class="toc-text">4.3.1全部导入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E6%9F%A5%E8%AF%A2%E5%AF%BC%E5%85%A5"><span class="toc-text">4.3.2查询导入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3%E7%BB%93%E6%9E%9C%E6%9F%A5%E8%AF%A2"><span class="toc-text">4.3.3结果查询</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E8%87%B3Hive-Hbase"><span class="toc-text">4.4 数据导入至Hive&#x2F;Hbase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1-%E5%AF%BC%E5%85%A5%E5%88%B0Hive%E5%91%BD%E4%BB%A4%E5%8F%82%E8%80%83"><span class="toc-text">4.4.1 导入到Hive命令参考</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2%E5%AF%BC%E5%85%A5%E5%88%B0Hbase%E5%91%BD%E4%BB%A4%E5%8F%82%E8%80%83"><span class="toc-text">4.4.2导入到Hbase命令参考</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#49-Sqoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA"><span class="toc-text">49.Sqoop实验：数据导出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-7"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-7"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-7"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E5%AF%BC%E5%87%BA%E5%8E%9F%E7%90%86"><span class="toc-text">3.1导出原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-text">4.1 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E7%BC%96%E5%86%99%E8%84%9A%E6%9C%AC"><span class="toc-text">4.2 编写脚本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC"><span class="toc-text">4.3 执行脚本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E5%88%9B%E5%BB%BA%E6%96%B0%E8%A1%A8staff1"><span class="toc-text">4.3.1 创建新表staff1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC"><span class="toc-text">4.3.2 执行脚本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-%E6%9F%A5%E7%9C%8B%E8%A1%A8staff1"><span class="toc-text">4.3.3 查看表staff1</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#50-Sqoop%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="toc-text">50.Sqoop实验：常用命令</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-8"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-8"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-8"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%88%97%E4%B8%BE"><span class="toc-text">3.1 常用命令列举</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%85%AC%E7%94%A8%E5%8F%82%E6%95%B0%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5"><span class="toc-text">4.1 公用参数：数据库连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%85%AC%E7%94%A8%E5%8F%82%E6%95%B0%EF%BC%9Aimport"><span class="toc-text">4.2 公用参数：import</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%85%AC%E7%94%A8%E5%8F%82%E6%95%B0%EF%BC%9Aexport"><span class="toc-text">4.3 公用参数：export</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%85%AC%E7%94%A8%E5%8F%82%E6%95%B0%EF%BC%9Ahive"><span class="toc-text">4.4 公用参数：hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Aimport"><span class="toc-text">4.5 命令&amp;参数：import</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Aexport"><span class="toc-text">4.6 命令&amp;参数：export</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Acodegen"><span class="toc-text">4.7 命令&amp;参数：codegen</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Acreate-hive-table"><span class="toc-text">4.8 命令&amp;参数：create-hive-table</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-9-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Aeval"><span class="toc-text">4.9 命令&amp;参数：eval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-10-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Aimport-all-tables"><span class="toc-text">4.10 命令&amp;参数：import-all-tables</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-11-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Ajob"><span class="toc-text">4.11 命令&amp;参数：job</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-12-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Alist-databases"><span class="toc-text">4.12 命令&amp;参数：list-databases</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-13-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Alist-tables"><span class="toc-text">4.13 命令&amp;参数：list-tables</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-14-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Amerge"><span class="toc-text">4.14 命令&amp;参数：merge</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-15-%E5%91%BD%E4%BB%A4-%E5%8F%82%E6%95%B0%EF%BC%9Ametastore"><span class="toc-text">4.15 命令&amp;参数：metastore</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#51-ZooKeeper%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2"><span class="toc-text">51.ZooKeeper实验：安装与部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-9"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-9"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-9"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E8%AE%A4%E8%AF%86Zookeeper"><span class="toc-text">3.1认识Zookeeper</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-zookeeper%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="toc-text">3.2 zookeeper工作机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3%E9%9B%86%E7%BE%A4%E7%BB%93%E6%9E%84"><span class="toc-text">3.3集群结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-Zookeeper%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-text">3.4 Zookeeper的数据结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%AE%89%E8%A3%85Zookeeper"><span class="toc-text">4.1安装Zookeeper</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="toc-text">4.2本地模式部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-text">4.2.1 配置环境变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3%E6%B5%8B%E8%AF%95%E6%98%AF%E5%90%A6%E5%AE%89%E8%A3%85%E6%88%90%E5%8A%9F"><span class="toc-text">4.2.3测试是否安装成功</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4-%E6%B5%8B%E8%AF%95Zookeeper%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">4.2.4 测试Zookeeper客户端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="toc-text">4.3 集群模式部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E9%83%A8%E7%BD%B2%E5%89%8D%E7%9A%84%E5%87%86%E5%A4%87"><span class="toc-text">4.3.1部署前的准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E5%88%9B%E5%BB%BAmyid%E6%96%87%E4%BB%B6"><span class="toc-text">4.3.2创建myid文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6zoo-cfg"><span class="toc-text">4.3.3修改配置文件zoo.cfg</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4%E5%90%AF%E5%8A%A8Zookeeper%E9%9B%86%E7%BE%A4"><span class="toc-text">4.3.4启动Zookeeper集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-5%E6%9F%A5%E7%9C%8B%E7%8A%B6%E6%80%81%E4%BF%A1%E6%81%AF"><span class="toc-text">4.3.5查看状态信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-6%E5%90%AF%E5%8A%A8zkCli%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">4.3.6启动zkCli客户端</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#52-Zookeeper%E5%AE%9E%E9%AA%8C%EF%BC%9AzkCli%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">52.Zookeeper实验：zkCli客户端</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-10"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-10"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-10"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E3%80%81session%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%EF%BC%9A"><span class="toc-text">3.1、session基本原理：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2%E3%80%81watch%E6%9C%BA%E5%88%B6"><span class="toc-text">3.2、watch机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">4.1启动客户端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E6%9F%A5%E8%AF%A2%E5%91%BD%E4%BB%A4"><span class="toc-text">4.2查询命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E7%BB%B4%E6%8A%A4%E5%91%BD%E4%BB%A4"><span class="toc-text">4.3维护命令</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E8%8A%82%E7%82%B9%E7%9B%91%E5%90%AC"><span class="toc-text">4.4节点监听</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#53-ZooKeeper%E5%AE%9E%E9%AA%8C%EF%BC%9AACL%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6"><span class="toc-text">53.ZooKeeper实验：ACL权限控制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-11"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-11"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-11"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E3%80%81ACL%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%EF%BC%9A"><span class="toc-text">3.1、ACL权限控制：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2%E3%80%81Zookeeper-ACL%E7%9A%84%E7%89%B9%E6%80%A7%EF%BC%9A"><span class="toc-text">3.2、Zookeeper ACL的特性：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3%E3%80%81%E6%9D%83%E9%99%90%E6%A8%A1%E5%BC%8F"><span class="toc-text">3.3、权限模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4%E3%80%81%E6%8E%88%E4%BA%88%E6%9D%83%E9%99%90"><span class="toc-text">3.4、授予权限</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E3%80%81%E5%90%AF%E5%8A%A8ZooKeeper%E9%9B%86%E7%BE%A4"><span class="toc-text">4.1、启动ZooKeeper集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E3%80%81%E5%90%AF%E5%8A%A8zkCli%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">4.2、启动zkCli客户端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E3%80%81%E6%9F%A5%E7%9C%8B%E8%8A%82%E7%82%B9ACL"><span class="toc-text">4.3、查看节点ACL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E3%80%81world%E6%9D%83%E9%99%90"><span class="toc-text">4.4、world权限</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E3%80%81IP%E6%9D%83%E9%99%90%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.5、IP权限模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6%E3%80%81auth%E6%8E%88%E6%9D%83%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.6、auth授权模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7%E3%80%81digest%E6%8E%88%E6%9D%83%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.7、digest授权模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8%E5%A4%9A%E7%A7%8D%E6%8E%88%E6%9D%83%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.8多种授权模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#54-ZooKeeper%E5%AE%9E%E9%AA%8C%EF%BC%9AJavaAPI"><span class="toc-text">54.ZooKeeper实验：JavaAPI</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-12"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-12"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-12"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%AF%BC%E5%85%A5jar%E5%8C%85"><span class="toc-text">4.1 导入jar包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E8%BF%9E%E6%8E%A5%E5%88%B0ZooKeeper"><span class="toc-text">4.2连接到ZooKeeper</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E6%96%B0%E5%A2%9E%E8%8A%82%E7%82%B9"><span class="toc-text">4.3新增节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%9B%B4%E6%96%B0%E8%8A%82%E7%82%B9"><span class="toc-text">4.4 更新节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E5%88%A0%E9%99%A4%E8%8A%82%E7%82%B9"><span class="toc-text">4.5删除节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6%E6%9F%A5%E7%9C%8B%E8%8A%82%E7%82%B9"><span class="toc-text">4.6查看节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7%E6%9F%A5%E7%9C%8B%E5%AD%90%E8%8A%82%E7%82%B9"><span class="toc-text">4.7查看子节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8%E6%A3%80%E6%9F%A5%E8%8A%82%E7%82%B9%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8"><span class="toc-text">4.8检查节点是否存在</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#55-ZooKeeper%E5%AE%9E%E9%AA%8C%EF%BC%9A%E8%BF%9B%E7%A8%8B%E5%8D%8F%E4%BD%9C"><span class="toc-text">55.ZooKeeper实验：进程协作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-13"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-13"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-13"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8ZooKeeper%E9%9B%86%E7%BE%A4"><span class="toc-text">4.1启动ZooKeeper集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%90%AF%E5%8A%A8zkCli%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">4.2启动zkCli客户端</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%88%9B%E5%BB%BAjava%E9%A1%B9%E7%9B%AE"><span class="toc-text">4.3创建java项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-%E5%AF%BC%E5%85%A5jar%E5%8C%85"><span class="toc-text">4.3.1 导入jar包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E7%BC%96%E5%86%99java%E4%BB%A3%E7%A0%81"><span class="toc-text">4.3.2编写java代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E5%81%9A%E6%88%90jar%E5%8C%85"><span class="toc-text">4.4做成jar包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E8%BF%90%E8%A1%8Cjar%E5%8C%85"><span class="toc-text">4.5运行jar包</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#56-Pig%E5%AE%9E%E9%AA%8C%EF%BC%9APig%E5%AE%89%E8%A3%85%E4%B8%8E%E9%83%A8%E7%BD%B2"><span class="toc-text">56.Pig实验：Pig安装与部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-14"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-14"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-14"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFPig"><span class="toc-text">什么是Pig?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pig%E4%B8%8EHive%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-text">Pig与Hive的区别？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pig-%E6%9C%89%E4%B8%A4%E7%A7%8D%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%EF%BC%9A"><span class="toc-text">Pig 有两种运行模式：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pig-%E7%9A%84%E8%B0%83%E7%94%A8%E6%9C%BA%E5%88%B6%EF%BC%9A"><span class="toc-text">Pig 的调用机制：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8hadoop"><span class="toc-text">4.1启动hadoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%AE%89%E8%A3%85Pig"><span class="toc-text">4.2安装Pig</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E9%83%A8%E7%BD%B2Pig"><span class="toc-text">4.3 部署Pig</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4pig-%E8%BF%90%E8%A1%8C"><span class="toc-text">4.4pig 运行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1Local-%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.4.1Local 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6"><span class="toc-text">运行机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2MapReduce-%E6%A8%A1%E5%BC%8F"><span class="toc-text">4.4.2MapReduce 模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6-1"><span class="toc-text">运行机制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#57-Pig%E5%AE%9E%E9%AA%8C%EF%BC%9APigLatin%E8%AF%AD%E6%B3%95"><span class="toc-text">57.Pig实验：PigLatin语法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-15"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-15"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-15"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pig-%E7%9A%84%E7%89%B9%E6%80%A7"><span class="toc-text">Pig 的特性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Pig-%E6%9E%B6%E6%9E%84"><span class="toc-text">Pig 架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%82%A8%E5%AD%98"><span class="toc-text">4.1数据加载与储存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%EF%BC%88LOAD%EF%BC%89"><span class="toc-text">4.1.1导入数据（LOAD）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%EF%BC%88STORE%EF%BC%89"><span class="toc-text">4.1.2数据存储（STORE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-text">4.2数据转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1%E5%88%86%E7%BB%84%E5%91%BD%E4%BB%A4%EF%BC%88GROUP%EF%BC%89"><span class="toc-text">4.2.1分组命令（GROUP）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2%E8%BF%87%E6%BB%A4%E5%91%BD%E4%BB%A4%EF%BC%88FILTER%EF%BC%89"><span class="toc-text">4.2.2过滤命令（FILTER）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3%E7%AD%9B%E9%80%89%E5%91%BD%E4%BB%A4%EF%BC%88LIMIT%EF%BC%89"><span class="toc-text">4.2.3筛选命令（LIMIT）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4%E5%8E%BB%E9%87%8D%E5%91%BD%E4%BB%A4%EF%BC%88DISTINCT%EF%BC%89"><span class="toc-text">4.2.4去重命令（DISTINCT）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-5%E6%8E%92%E5%BA%8F%E5%91%BD%E4%BB%A4%EF%BC%88ORDER-BY%EF%BC%89"><span class="toc-text">4.2.5排序命令（ORDER BY）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-6%E9%81%8D%E5%8E%86%E5%91%BD%E4%BB%A4%EF%BC%88FOREACH%EF%BC%89"><span class="toc-text">4.2.6遍历命令（FOREACH）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-7%E5%B5%8C%E5%A5%97%E5%91%BD%E4%BB%A4%EF%BC%88FOREACH%EF%BC%89"><span class="toc-text">4.2.7嵌套命令（FOREACH）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-8%E5%88%86%E6%94%AF%E5%91%BD%E4%BB%A4%EF%BC%88CASE%EF%BC%89"><span class="toc-text">4.2.8分支命令（CASE）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-9%E8%BF%9E%E6%8E%A5%E5%91%BD%E4%BB%A4%EF%BC%88JOIN%EF%BC%89"><span class="toc-text">4.2.9连接命令（JOIN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%AE%AD%E7%BB%83"><span class="toc-text">4.3 训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.1准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2%E5%90%AF%E5%8A%A8-grunt-shell"><span class="toc-text">4.3.2启动 grunt shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3%E8%AF%BB%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.3读入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-text">4.3.4 数据处理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-5%E6%95%B0%E6%8D%AE%E5%82%A8%E5%AD%98"><span class="toc-text">4.3.5数据储存</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#58-Pig%E5%AE%9E%E9%AA%8C%EF%BC%9APig%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AE"><span class="toc-text">58.Pig实验：Pig分析数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-16"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-16"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-16"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E5%8D%95%E8%AF%8D%E8%AE%A1%E6%95%B0"><span class="toc-text">4.1非结构化单词计数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-text">4.1.1准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2%E5%90%AF%E5%8A%A8grunt-shell"><span class="toc-text">4.1.2启动grunt shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-3%E8%AF%BB%E5%8F%96%E5%B9%B6%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">4.1.3读取并转换数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%88%86%E6%9E%90%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2分析结构化数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2.1准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2.2读取数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3%E8%AF%BB%E5%8F%96%E5%B9%B6%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2.3读取并转换数据</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%914/" title="Hadoop应用与开发4">Hadoop应用与开发4</a><time datetime="2025-06-16T04:00:00.000Z" title="发表于 2025-06-16 12:00:00">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/" title="Hadoop应用与开发3">Hadoop应用与开发3</a><time datetime="2025-05-16T04:00:00.000Z" title="发表于 2025-05-16 12:00:00">2025-05-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/" title="Hadoop应用与开发2">Hadoop应用与开发2</a><time datetime="2025-05-11T04:00:00.000Z" title="发表于 2025-05-11 12:00:00">2025-05-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/" title="Hadoop应用与开发1">Hadoop应用与开发1</a><time datetime="2025-04-20T04:00:00.000Z" title="发表于 2025-04-20 12:00:00">2025-04-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/12/Java/mooc%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E2%80%94%E2%80%94Java/" title="mooc面向对象程序设计——Java">mooc面向对象程序设计——Java</a><time datetime="2025-03-12T10:22:00.000Z" title="发表于 2025-03-12 18:22:00">2025-03-12</time></div></div></div></div></div></div></main><footer id="footer" style="background: linear-gradient(20deg, #ffd6e0, #f5f5f5, #c1f0c1);"><div id="footer-wrap"><div class="copyright">&copy;2025 By tong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>