<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Hadoop应用与开发2 | Blog</title><meta name="author" content="tong"><meta name="copyright" content="tong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Hadoop应用与开发221.Spark实验：SparkSQL入门实战 目的1.学习spark的入门编程 要求2.掌握spark基础操作和原理 原理Spark SQLSpark SQL 是 Spark 用来处理结构化数据的模块，通过 Spark SQL 我们可以像之前使用 SQL 语句分析关系型数据库表一样方便地在  Spark 上对海量结构化数据进行快速分析，我们只需要关注数据分析的逻辑而不需要">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop应用与开发2">
<meta property="og:url" content="http://example.com/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/index.html">
<meta property="og:site_name" content="Blog">
<meta property="og:description" content="Hadoop应用与开发221.Spark实验：SparkSQL入门实战 目的1.学习spark的入门编程 要求2.掌握spark基础操作和原理 原理Spark SQLSpark SQL 是 Spark 用来处理结构化数据的模块，通过 Spark SQL 我们可以像之前使用 SQL 语句分析关系型数据库表一样方便地在  Spark 上对海量结构化数据进行快速分析，我们只需要关注数据分析的逻辑而不需要">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/touxiang.png">
<meta property="article:published_time" content="2025-05-11T04:00:00.000Z">
<meta property="article:modified_time" content="2025-10-31T08:02:06.482Z">
<meta property="article:author" content="tong">
<meta property="article:tag" content="hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/touxiang.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hadoop应用与开发2",
  "url": "http://example.com/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/",
  "image": "http://example.com/img/touxiang.png",
  "datePublished": "2025-05-11T04:00:00.000Z",
  "dateModified": "2025-10-31T08:02:06.482Z",
  "author": [
    {
      "@type": "Person",
      "name": "tong",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/./img/favicon1.ico"><link rel="canonical" href="http://example.com/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop应用与开发2',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background-image: url(https://i.loli.net/2019/09/09/5oDRkWVKctx2b6A.png);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/./img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Hadoop应用与开发2</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 链接</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Hadoop应用与开发2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-05-11T04:00:00.000Z" title="发表于 2025-05-11 12:00:00">2025-05-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-10-31T08:02:06.482Z" title="更新于 2025-10-31 16:02:06">2025-10-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="container post-content" id="article-container"><h1 id="Hadoop应用与开发2"><a href="#Hadoop应用与开发2" class="headerlink" title="Hadoop应用与开发2"></a>Hadoop应用与开发2</h1><h2 id="21-Spark实验：SparkSQL入门实战"><a href="#21-Spark实验：SparkSQL入门实战" class="headerlink" title="21.Spark实验：SparkSQL入门实战"></a>21.Spark实验：SparkSQL入门实战</h2><blockquote>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>1.学习spark的入门编程</p>
<h3 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h3><p>2.掌握spark基础操作和原理</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>Spark SQL<br>Spark SQL 是 Spark 用来处理结构化数据的模块，通过 Spark SQL 我们可以像之前使用 SQL 语句分析关系型数据库表一样方便地在  Spark 上对海量结构化数据进行快速分析，我们只需要关注数据分析的逻辑而不需要担心底层分布式存储、计算、通信、以及作业解析和调度的细节。</p>
<p>Spark SQL 支持从 JSON 文件、CSV 文件、Hive 表、Parquest 文件中读取数据，通过 SQL  语句对数据进行交互式查询；也可以读取传统关系型数据库中的数据进行数据分析。同时还能与传统的 RDD 编程相结合，让我们能够同时使用 SQL 和  RDD 进行复杂的数据分析。</p>
<p>另外，它还能与其它核心组件比如： Spark Streaming、MLlib、GraphX 等联合使用进行数据分析，所以 Spark SQL 在大数据应用中扮演了处理中间结构化数据的角色，它的作用是非常大的。</p>
<p>DataFrame<br>DataFrame 是 Spark SQL 的核心数据抽象。对于输入的待处理数据，我们既可以将其转化为  DataFrame，然后调用 DataFrame API 进行处理；也可以将 DataFrame 注册成临时表，在临时表上直接使用 SQL  进行数据查询。</p>
<p>在这里我们需要注意一下的是 DataFrame 与 RDD 之间的区别：RDD 是整个 Spark  平台的一种基本通用的数据抽象，它更具有通用性，适用于各类数据源，无论是结构化数据、半结构化数据或非结构化数据都会被统一的转化为同一元素组成的  RDD，它并不了解每一条数据的内容是什么；而 DataFrame  是只针对结构化数据源的高层数据抽象，它能够提取出数据特定的结构信息，进而能够更加高效的处理结构化信息。</p>
<p>它们之间的区别如下图所示：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355611516.png" alt="img"></p>
</blockquote>
<p> 我们需要为本次实验准备数据，使用 JSON 文件作为数据源，创建 &#x2F;usr&#x2F;cstor&#x2F;courses.json 文件，并将文件上传至hdfs根目录下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim courses.json</span><br></pre></td></tr></table></figure>

<p>并输入下面的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;Linux&quot;, &quot;type&quot;:&quot;basic&quot;, &quot;length&quot;:10&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;TCPIP&quot;, &quot;type&quot;:&quot;project&quot;, &quot;length&quot;:15&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Python&quot;, &quot;type&quot;:&quot;project&quot;, &quot;length&quot;:8&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;GO&quot;, &quot;type&quot;:&quot;basic&quot;, &quot;length&quot;:2&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Ruby&quot;, &quot;type&quot;:&quot;basic&quot;, &quot;length&quot;:5&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355625792.png" alt="img"></p>
<p>上传数据文件至hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor</span><br><span class="line"># hdfs dfs -put courses.json /</span><br></pre></td></tr></table></figure>

<p>创建数据源<br>Spark SQL 编程主入口点是：SparkSession，我们可以通过 SparkSession.builder() 创建一个基本的 SparkSession 对象，并配置一些初始化参数。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line">val spark = SparkSession.builder().appName(&quot;courses&quot;).config(&quot;spark.some.config.option&quot;, &quot;some-value&quot;).getOrCreate()</span><br><span class="line">// 引入包便于把 RDD 隐式转换为 DataFrame</span><br><span class="line">import spark.implicits._</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355645390.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355654493.png" alt="img"></p>
<p>创建SparkSession对象<br>应用程序根据上一步创建的 SparkSession 对象提供的 API，可以从现有的 RDD 或其它结构化数据源中创建 DataFrame 对象。<br>在本例中我们从 JSON 文件创建 DataFrames：</p>
<p>&#x2F;&#x2F; 创建DataFrames，指明来源自JSON文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df = spark.read.json(&quot;/courses.json&quot;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355666510.png" alt="img"></p>
<p>df 数据源可以来自各种不同的数据源，但原理都类似，调用不同的创建函数去连接数据源。<br>下面的步骤中我们将对创建的 DataFrame 进行操作。<br>创建SparkSession对象<br>首先打印当前 DataFrame 里的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355674008.png" alt="img"></p>
<p>show()函数将打印出 JSON 文件中存储的数据表。<br>describe()获取指定字段的统计信息:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.describe(&quot;length&quot;).show() //其中，Count:记录条数，Mean:平均值，Stddev:样本标准差，in:最小值，</span><br><span class="line">Max:最大值</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355686515.png" alt="img"></p>
<p>用于获取全部记录的有： collect() 获取所有数据到数组，collectAsList()获取所有数据到 List。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.collect()</span><br><span class="line">df.collectAsList()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355703220.png" alt="img"></p>
<p>用于获取部分记录的有：first(): 获取第一行记录;head(n)：获取前 n 行记录;take(n): 获取前 n 行数据;takeAsList(n): 获取前 n 行数据，并以 List 的形式展示。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.take(2)</span><br></pre></td></tr></table></figure>

<p>我们使用printSchema()函数打印数据表的 Schema（结构信息）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355714467.png" alt="img"></p>
<p>对 df 进行处理<br>查询课程名为 Linux 或 有两个实验数量的课程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.where(&quot;name = &#x27;Linux&#x27; or length = 2&quot;).show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355722957.png" alt="img"></p>
<p>select 操作<br>展示所有的课程名：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;name&quot;).show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355731222.png" alt="img"></p>
<p>展示所有的课程名及课程长度：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;name&quot;, &quot;length&quot;).show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355740360.png" alt="img"></p>
<p>展示所有基础课，打印出课程名称：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.filter(df(&quot;type&quot;).equalTo(&quot;basic&quot;)).select(&quot;name&quot;, &quot;type&quot;).show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355748215.png" alt="img"></p>
<p>limit(n) 方法获取指定 DataFrame 的前 n 行记录，得到的是一个新转化生成的 DataFrame 对象。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.limit(3).show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355755289.png" alt="img"></p>
<p>排序操作有两种方法：orderBy() 和 sort(),都是按指定字段排序，默认为升序。desc 表示降序，asc 表示升序。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.orderBy(df(&quot;length&quot;).desc).show(false)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355762059.png" alt="img"></p>
<p>计算所有基础课和项目课的数量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.groupBy(&quot;type&quot;).count().show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355781543.png" alt="img"></p>
<p>SparkSession 提供了 SparkSession.sql() 方法，SQL 语句可以直接作为字符串传入 sql() 方法中。如果同学不清楚具体的 SQL 语句应该如何编写的，可以查看相关文档资料：菜鸟-SQL 教程<br>首先需要将 DataFrame 注册为 Table(临时表)才可以在该表上执行 SQL 语句：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(&quot;courses&quot;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355806696.png" alt="img"></p>
<p>查询课程长度在 5-10 之间的课程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val sqlDF = spark.sql(&quot;SELECT name FROM courses WHERE length &gt;= 5 and length &lt;= 10&quot;)</span><br><span class="line"></span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355814841.png" alt="img"></p>
<p>如果在同一个应用的不同 session 会话中需要重用一个临时表，就可以把它注册成为全局临时表，全局临时表会一直存在并在所有会话中共享直到应用程序终止。<br>&#x2F;&#x2F; 注册成为全局临时表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createGlobalTempView(&quot;GloCourses&quot;)</span><br></pre></td></tr></table></figure>

<p>&#x2F;&#x2F; newSession()返回一个新的spark对象，引用全局临时表需要 global_temp 标识</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.newSession().sql(&quot;SELECT name FROM global_temp.GloCourses WHERE length &gt;= 5 and length</span><br><span class="line"> &lt;= 10&quot;).show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355827158.png" alt="img"></p>
<p>parquet 是 Spark SQL 读取的默认数据文件格式，我们把先前从 JSON 中读取的 DataFrame 保存为这种格式，只保存课程名称和长度两项数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;name&quot;, &quot;length&quot;).write.format(&quot;parquet&quot;).save(&quot;/home/hadoop/courses.parquet&quot;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355852411.png" alt="img"></p>
<p>&#x2F;home&#x2F;hadoop&#x2F;courses.parquet 文件夹被创建并存入课程名称和长度数据，可以查看文件夹下的内容：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355858951.png" alt="img"></p>
<p>上述几个步骤都是最基本的 Spark SQL 操作，连接的是最简单的数据源 JSON 文件，DataFrame 支持由各种不同的源创建，逻辑实现类似，可以尝试连接 Hive 进行实验测试。<br>Spark SQL 的默认数据源格式为 parquet 格式。当文件是 parquet 格式时，spark SQL 可以直接在该文件上执行查询操作。<br>以下代码仅作为示例，不需要在 spark-shell 中执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.load(&#x27;examples/src/main/resources/users.parquet&#x27;)</span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;namesAndFavColors.parquet&quot;)</span><br></pre></td></tr></table></figure>

<p>手动指定选项<br>当数据源不是 parquet 文件却是内置格式的时候，使用指定简称(json, jdbc, orc, libsvm, csv, text)即可。同时还可以对 DataFrame 进行类型转换。<br>以下代码仅作为示例，不需要在 spark-shell 中执行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;)</span><br></pre></td></tr></table></figure>

<p>存储模式<br>保存操作可以选择使用 SaveMode , 它指定如何处理现有数据。当执行 Overwrite 时, 在写入新数据之前，原有的数据将被删除。<br>Scala&#x2F;Java| Any Language| Meaning<br>SaveMode.ErrorIfExists (default)| ‘error’ (default)| 将 DataFrame 保存到数据源时, 如果数据已经存在, 则会抛出异常<br>SaveMode.Append| ‘append’| 将 DataFrame 保存到数据源时, 如果 数据&#x2F;表 已存在, 则 DataFrame 的内容将被附加到现有数据中<br>SaveMode.Overwrite| ‘overwrite’| 覆盖模式意味着将 DataFrame 保存到 数据源 时, 如果数据表已经存在, 则预期 DataFrame 的内容将覆盖现有数据<br>SaveMode.Ignore |’ignore’| 忽略模式意味着当将 DataFrame 保存到 数据源 时, 如果数据已经存在, 则保存操作预期不会保存  DataFrame 的内容, 并且不更改现有数据。这与 SQL 中的 CREATE TABLE IF NOT EXISTS 类似<br>通用 load、save 函数<br>除了从外部导入结构化数据源以外，我们还可以把已有的 RDD 转化为 DataFrame 对象。但是这样的 RDD 对象依然必须在内部具有公有且鲜明的字段结构才能满足转化的条件。<br>将 RDDs 转化为 DataFrame 有两种方式，也就是隐式推断或显式指定 DataFrame 对象的结构信息。<br>在这里呢，我们主要讲解隐式转换，这也是最常用的一种类型；显示转换可以留待同学们下来自行探索学习。<br>首先，在 &#x2F;usr&#x2F;cstor 目录下新建 people.txt 文件，并上传至hdfs根目录并使用 vim 打开：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/cstor/people.txt</span><br></pre></td></tr></table></figure>

<p>向文件中输入如下内容，格式为’姓名, 年龄’：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Michael, 29</span><br><span class="line">Andy, 30</span><br><span class="line">Justin, 19</span><br></pre></td></tr></table></figure>

<p>在 spark-shell 中输入如下内容：<br>&#x2F;&#x2F; 声明 Person 样例类，用于装载 name、age 这两个不可变数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">case class Person(name: String,age: Long)</span><br></pre></td></tr></table></figure>

<p>&#x2F;&#x2F; 隐式转换</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br></pre></td></tr></table></figure>

<p>&#x2F;&#x2F; 读取 people.txt 文件内容并分割每一行将其转化为包含 Person 对象的 RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val personRDD = spark.sparkContext</span><br><span class="line">  .textFile(&quot;/home/hadoop/people.txt&quot;)</span><br><span class="line">  .map(_.split(&quot;,&quot;))</span><br><span class="line">  .map(attributes =&gt; Person(attributes(0), attributes(1).trim.toInt))</span><br></pre></td></tr></table></figure>

<p>&#x2F;&#x2F; 调用 RDD.toDF() 方法将 personrdd 隐式转化为 DataFrame</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val peopleDF = personRDD.toDF()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676355876815.png" alt="img"></p>
<h2 id="22-Spark实验：RDD高级应用与持久化"><a href="#22-Spark实验：RDD高级应用与持久化" class="headerlink" title="22.Spark实验：RDD高级应用与持久化"></a>22.Spark实验：RDD高级应用与持久化</h2><blockquote>
<h3 id="目的-1"><a href="#目的-1" class="headerlink" title="目的"></a>目的</h3><p>1.学习Spark RDD 的高级应用以及计算完成后的持久化。</p>
<h3 id="要求-1"><a href="#要求-1" class="headerlink" title="要求"></a>要求</h3><p>1.掌握并熟练使用spark的高级应用与持久化，加深了解spark运行机制。</p>
<h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>Spark-RDD持久化<br>多次对某个RDD进行transformation或者action，如果没有做RDD持久化，那么每次都要重新计算一个RDD，会消耗大量时间，降低Spark性能。<br>Spark非常重要的一个功能特性就是可以将RDD持久化在内存中。当对RDD执行持久化操作时，每个节点都会将自己操作的RDD的partition持久化到内存中，并且在之后对该RDD的反复使用中，直接使用内存缓存的partition。这样的话，对于针对一个RDD反复执行多个操作的场景，就只要对RDD计算一次即可，后面直接使用该RDD，而不需要反复计算多次该RDD。</p>
</blockquote>
<h3 id="4-1向Spark传递函数"><a href="#4-1向Spark传递函数" class="headerlink" title="4.1向Spark传递函数"></a>4.1向Spark传递函数</h3><p>Spark 的大部分转化操作和一部分行动操作都依赖于用户传递的函数进行计算，这里主要有两种方式：<br>第一种是匿名函数，可以减少代码量。匿名函数的定义 &#x3D;&gt; 左边是参数，参数可以省略参数类型，右边是函数体部分。<br>首先我们先启动spark shell，输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark-shell</span><br></pre></td></tr></table></figure>

<p>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val nums = sc.parallelize(List(1,2,39,53,80))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359060671.png" alt="img"></p>
<p>&#x2F;&#x2F; 把每一个数字都加1</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums.map(x =&gt; x+1).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359068589.png" alt="img"></p>
<p>&#x2F;&#x2F; 筛选大于50的数字</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums.filter(x =&gt; x&gt;50).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359076797.png" alt="img"></p>
<p>&#x2F;&#x2F; 筛选出奇数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums.filter(x =&gt; x%2 != 0).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359084242.png" alt="img"></p>
<p>第二种是传入静态方法和传入方法的引用。<br>传入全局单例对象的静态方法。比如，你可以先定义 Object Functions，然后传递 Functions.func1。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val nums = sc.parallelize(List(1,2,3,4))</span><br><span class="line">object Functions&#123;</span><br><span class="line">    def func1(x:Int):Int = &#123;x+1&#125;</span><br><span class="line">&#125;</span><br><span class="line">nums.map(Functions.func1).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359098514.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359104039.png" alt="img"></p>
<p>需要注意的是，虽然可以在同一个类实例中传递方法的引用，但是需要发送包含该类的对象连同该方法到集群，这个会比仅传递静态方法到集群的开销更大。<br>比如下面的伪代码（这一段代码仅作为示例，不能在命令行中输入）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class MyClass&#123;</span><br><span class="line">    def func1(s:String):String = &#123;...&#125;</span><br><span class="line">    def doStuff(rdd: RDD[String]):RDD[String] = &#123;</span><br><span class="line">        // 复制field到局部变量中，避免将整个对象发送到集群</span><br><span class="line">        val field_ = this.field</span><br><span class="line">        rdd.map(x =&gt; field_ + x)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-2理解闭包"><a href="#4-2理解闭包" class="headerlink" title="4.2理解闭包"></a>4.2理解闭包</h3><p>在 spark 中，变量和函数的作用范围以及生命周期在集群模式下运行会比较难理解。特别要注意的是使用 RDD 修改其作用范围的变量往往容易出现问题。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val data = Array(1,2,3,4,5)</span><br><span class="line">var counter = 0</span><br><span class="line">var rdd = sc.parallelize(data)</span><br><span class="line">rdd.foreach(x =&gt; counter+=x)</span><br><span class="line">println(&quot;Counter value:&quot; + counter)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359127395.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359134320.png" alt="img"></p>
<p>这个时候就出现问题了，计算出来的结果居然是 0！这是为什么呢？！因为在执行过程中，Spark 会将 RDD 拆分为多个 task  并分发给不同的执行器操作，而分发到每个执行器的 counter 是经过复制的，执行器彼此之间是不能相互访问的，这就会导致执行器在执行  foreach 方法的时候只能修改自身的值，驱动程序中的 counter 值没有被修改，最终输出的 counter 值依然是 0。执行器要在  RDD 上进行计算时必须对执行器节点可见的变量和方法就称为闭包，在这里就是 foreach() 方法。<br>这个问题如何解决呢？我们可以使用共享全局变量的方式来解决。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val data = Array(1,2,3,4,5)</span><br><span class="line">// 使用全局共享变量的累加器(accumulator)</span><br><span class="line">val counter = sc.accumulator(0)</span><br><span class="line">var rdd = sc.parallelize(data)</span><br><span class="line">rdd.foreach(x =&gt; counter+=x)</span><br><span class="line">println(&quot;Counter value:&quot; + counter)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359196889.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359204503.png" alt="img"></p>
<p>使用累加器之后就能正确的计算出结果啦！<br>打印 RDD 的元素<br>一个比较常见的习惯是使用  rdd.foreach(println)或rdd.map(println)来打印 RDD  的元素。这在单台机器上运行是能够成功打印出结果的。但是考虑到我们上面关于闭包的问题，如果把这句代码放在集群模式下运行，执行器输出写入的是执行器的 stdout，而不是驱动程序的 stdout。那么要想在驱动程序中打印元素，应该怎么办呢？如果数据量不多的话可以使用 collect()  方法，也就是 rdd.collect().foreach(println);如果数据量很大的话可以使用 take() 方法，也就是  rdd.take(num).foreach(println)。</p>
<h3 id="4-3共享变量"><a href="#4-3共享变量" class="headerlink" title="4.3共享变量"></a>4.3共享变量</h3><p>当传递给 Spark 的函数在集群中各个节点运行时，函数所用的变量在每个节点上都是独立的副本，变量被复制到所有节点上，并且相互独立，更新后也不会传递给驱动程序。Spark 为了能够让多个节点之间共享变量，提供了两种方法：广播变量和累加器。</p>
<p>广播变量<br>广播变量可以在每个机器上缓存一个只读的变量，可以通过sc.broadcast(v)方法创建。广播变量能够更高效的进行数据集的分配。<br>在 spark-shell 中输入如下代码：</p>
<p>&#x2F;&#x2F; 创建广播变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val broadcastVar = sc.broadcast(Array(1, 2, 3))</span><br></pre></td></tr></table></figure>

<p>&#x2F;&#x2F; 查看广播变量的值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">broadcastVar.value</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359216728.png" alt="img"></p>
<p>累加器<br>累加器只能够通过加操作的变量，因此在并行操作中具备更高的效率，可以实现 counters 和 sums。<br>在 spark-shell 中输入如下代码：</p>
<p>&#x2F;&#x2F; 创建累加器</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val accum = sc.accumulator(0, &quot;My Accumulator&quot;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359265919.png" alt="img"></p>
<p>&#x2F;&#x2F; 对RDD数据集的数值进行累加</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum += x)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359273370.png" alt="img"></p>
<p>&#x2F;&#x2F; 查看累加器的结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accum.value</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359282579.png" alt="img"></p>
<h3 id="4-4键值对RDD"><a href="#4-4键值对RDD" class="headerlink" title="4.4键值对RDD"></a>4.4键值对RDD</h3><p>大部分 Spark 操作支持含任意类型对象的 RDD ，少数特殊操作仅仅在键值(key-value)对的 RDD 可用。最常见的是分布式  ‘shuffle’ 操作，例如根据一个 key 对一组数据进行分组和聚合。PairRDDFunctions  类中提供了键值对操作，它自动包装元组的 RDD。<br>当需要把一个普通的 RDD 转化为 PairRDD 时，可以使用 map() 方法来实现。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.parallelize(List(&quot;I love bigdata&quot;,&quot;Yes I do&quot;))</span><br><span class="line">lines.map(x =&gt; (x.split(&quot; &quot;)(0),x)).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359294998.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359301277.png" alt="img"></p>
<p>在这里就是使用了每句话的第一个单词作为键创建出了一个 PairRDD 。</p>
<h3 id="4-5转化操作"><a href="#4-5转化操作" class="headerlink" title="4.5转化操作"></a>4.5转化操作</h3><p>Pair RDD 也可以使用前面介绍的 RDD 的方法，同时它还有一些自己独特的 RDD 方法，下面依次进行介绍。<br>reduceByKey(func)<br>合并具有相同键的值。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(List((1,1),(2,1),(3,1),(3,4)))</span><br><span class="line">rdd.reduceByKey((x,y)=&gt;x+y).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359321252.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359313792.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupByKey()</span><br></pre></td></tr></table></figure>

<p>对具有相同键的值进行分组。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.groupByKey().collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359342106.png" alt="img"></p>
<p>mapValues(func)<br>对键值对 RDD 的每个值应用一个函数而不改变对应的键。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.mapValues(x=&gt;x*3).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359350753.png" alt="img"></p>
<p>flatMapValues(func)<br>对键值对 RDD 中的每个值应用一个返回迭代器的函数，然后对返回的每个元素都生成一个对应原键的键值对记录，通常用于符号化。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.flatMapValues(x=&gt;(x to 4)).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359358796.png" alt="img"></p>
<p>keys()<br>返回一个仅包含所有键的 RDD。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.keys.collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359368007.png" alt="img"></p>
<p>values()<br>返回一个仅包含所有值的 RDD。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.values.collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676363777261.png" alt="img"></p>
<p>sortByKey()<br>返回一个根据键排序的 RDD。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.sortByKey().collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676363785808.png" alt="img"></p>
<p>combineByKey(createCombiner,mergeValue,mergeCombiners)<br>combineByKey() 是键值对 RDD 中较为核心的高级函数，很多其它聚合函数都是在这个之上实现的，通过使用 combineByKey()  函数，我们能够更加清楚的明白 spark 底层如何进行分布式计算。其中这 3 个参数分别对应着聚合操作的 3 个步骤。<br>combineByKey() 会遍历分区的所有元素，在这个过程中只会出现两种情况：一种是该元素对应的键没有遇到过；另外一种是该元素对应的键和之前的某一个元素的键是相同的。</p>
<p>如果是新的元素，combineByKey() 会使用 createCombiner() 函数来创建该键对应累加器的初始值。注意，是在每一个分区中第一次出现新键的时候创建，而不是在整个 RDD 中。</p>
<p>在当前分区中，如果遇到该键是已经存在的键，那么就调用 mergeValue() 方法将该键对应累加器的当前值与这个新的值合并。<br>由于有多个分区，每个分区都是独立处理的，所以最后需要调用 mergeCombiners() 方法将各个分区的结果合并。</p>
<p>下面我们来看一个实际的例子，在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">val input = sc.parallelize(Seq((&quot;t1&quot;, 1), (&quot;t1&quot;, 2), (&quot;t1&quot;, 3), (&quot;t2&quot;, 2), (&quot;t2&quot;, 5)))</span><br><span class="line"></span><br><span class="line">val result = input.combineByKey(</span><br><span class="line">// 分区内遇到新键时，创建 (累加值，出现次数) 的键值对</span><br><span class="line">(v) =&gt; (v, 1),</span><br><span class="line">// 分区内遇到已经创建过相应累加器的旧键，就更新对应累加器</span><br><span class="line">(acc: (Int, Int), v) =&gt; (acc._1 + v, acc._2 + 1),</span><br><span class="line">// 多个分区遇到同一个键的累加器，就更新主累加器</span><br><span class="line">(acc1: (Int, Int), acc2: (Int, Int)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">// 求平均值</span><br><span class="line">).map&#123; case (key, value) =&gt; (key, value._1 / value._2.toFloat) &#125;</span><br><span class="line">// 输出结果</span><br><span class="line">result.collectAsMap().foreach(println(_))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676363796736.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676363803003.png" alt="img"><br>subtractByKey()<br>删掉 RDD1 中键与 RDD2 的键相同的元素。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(List((1,2),(2,3),(2,4)))</span><br><span class="line">val rdd2 = sc.parallelize(List((2,4),(3,5)))</span><br><span class="line">rdd1.subtractByKey(rdd2).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676364064321.png" alt="img"></p>
<p>cogroup()<br>将两个 RDD 中拥有相同键的数据分组到一起。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd1.cogroup(rdd2).collect().foreach(println(_))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676364078202.png" alt="img"></p>
<h3 id="4-6行动操作"><a href="#4-6行动操作" class="headerlink" title="4.6行动操作"></a>4.6行动操作</h3><p>和转化操作类似，所有简单 RDD 支持的行动操作在键值对 RDD 上也是管用的，同时它还有一些更加编辑的行动操作。</p>
<p>countByKey()<br>对每个键对应的元素分别计数。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(List((1,2),(2,3),(2,4),(3,5)))</span><br><span class="line">rdd.countByKey().foreach(println(_))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676366667780.png" alt="img"></p>
<p>lookup()<br>返回给定键对应的所有值。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.lookup(2)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676366675552.png" alt="img"><br>collectAsMap()</p>
<p>将结果以映射表的形式返回，方便查询。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.collectAsMap()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676366682711.png" alt="img"></p>
<h3 id="4-7不同类型RDD之间的转换"><a href="#4-7不同类型RDD之间的转换" class="headerlink" title="4.7不同类型RDD之间的转换"></a>4.7不同类型RDD之间的转换</h3><p>有些函数只能用于特定类型的 RDD 上，比如 mean()、sum() 只能用于数值 RDD，而  reduceByKey()、groupByKey() 只能用于键值对 RDD。在 Scala 中这些函数都没有定义在标准的 RDD  类中，如果想要使用这些附加函数，我们必须确保获得了正确的专用 RDD 类。<br>在 Scala 中可以通过隐式转换将 RDD 转为有特定函数的 RDD ，也就是记得加上 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkContext._</span><br></pre></td></tr></table></figure>

<h3 id="4-8RDD的缓存、持久化"><a href="#4-8RDD的缓存、持久化" class="headerlink" title="4.8RDD的缓存、持久化"></a>4.8RDD的缓存、持久化</h3><p>正如前面所看到的，spark RDD 具有惰性求值的特性，而在实际的数据处理过程中，我们通常会接触到 TB 级别的数据量，并且会重复调用同一组数据，为了节省计算资源，我们可以选择把会频繁使用到的中间数据缓存或持久化到内存或是磁盘中，方便下一次调用。<br>首先，我们需要引入相关的模块：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.storage._</span><br></pre></td></tr></table></figure>

<h3 id="4-9RDD-缓存"><a href="#4-9RDD-缓存" class="headerlink" title="4.9RDD 缓存"></a>4.9RDD 缓存</h3><p>我们可以对 RDD 使用 cache() 方法进行缓存，这里的缓存相当于对 RDD 执行默认存储等级（MEMORY_ONLY）的持久化操作，也就是在集群相关节点的内存中进行缓存。<br>如果不需要缓存的数据了，可以执行 rdd.unpersist()清除缓存。<br>在 spark-shell 中输入如下代码:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.textFile(&quot;/etc/protocols&quot;)</span><br><span class="line">rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_)</span><br><span class="line">rdd.cache() //这里还没有执行缓存</span><br><span class="line">rdd.take(10) // 遇到执行操作开始真正计算RDD并缓存</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676366746888.png" alt="img"></p>
<h3 id="4-10RDD"><a href="#4-10RDD" class="headerlink" title="4.10RDD"></a>4.10RDD</h3><p>我们可以使用 persist() 函数来进行持久化，一般默认的存储空间还是在内存中，如果内存不够就会写入磁盘中。persist 持久化分为不同的等级，还可以在存储等级的末尾加上_2用于把持久化的数据存为 2 份，避免数据丢失。<br>下面的列表列出了不同的持久化等级：</p>
<table>
<thead>
<tr>
<th>级别</th>
<th>使用的空间</th>
<th>是否在内存中</th>
<th>是否在磁盘上</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>高</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>低</td>
<td>是</td>
<td>否</td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>高</td>
<td>部分</td>
<td>部分</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>低</td>
<td>部分</td>
<td>部分</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>低</td>
<td>否</td>
<td>是</td>
</tr>
<tr>
<td>在 spark-shell 中执行如下代码：</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.storage._</span><br><span class="line">val rdd = sc.makeRDD(1 to 100000)</span><br><span class="line">rdd.persist(StorageLevel.MEMORY_AND_DISK_SER_2)</span><br><span class="line">rdd.take(15)</span><br><span class="line">rdd.unpersist()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676366757281.png" alt="img"></p>
<h2 id="23-Spark实验：SparkStreaming"><a href="#23-Spark实验：SparkStreaming" class="headerlink" title="23.Spark实验：SparkStreaming"></a>23.Spark实验：SparkStreaming</h2><blockquote>
<h3 id="目的-2"><a href="#目的-2" class="headerlink" title="目的"></a>目的</h3><p>1． 了解Spark Streaming版本的WordCount和MapReduce版本的WordCount的区别；<br>2． 理解Spark Streaming的工作流程；<br>3． 理解Spark Streaming的工作原理。</p>
<h3 id="要求-2"><a href="#要求-2" class="headerlink" title="要求"></a>要求</h3><p>1.要求实验结束时，每位学生能正确运行成功本实验中所写的jar包程序，能正确的计算出单词数目。</p>
<h3 id="原理-2"><a href="#原理-2" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1Spark-Streaming架构"><a href="#3-1Spark-Streaming架构" class="headerlink" title="3.1Spark Streaming架构"></a>3.1Spark Streaming架构</h2><p>计算流程：Spark Streaming是将流式计算分解成一系列短小的批处理作业。这里的批处理引擎是Spark，也就是把Spark  Streaming的输入数据按照batch size（如1秒）分成一段一段的数据（Discretized  Stream），每一段数据都转换成Spark中的RDD（Resilient Distributed Dataset），然后将Spark  Streaming中对DStream的Transformation操作变为针对Spark中对RDD的Transformation操作，将RDD经过操作变成中间结果保存在内存中。整个流式计算根据业务的需求可以对中间的结果进行叠加，或者存储到外部设备。如图3-1所示：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280900773.png" alt="img"><br>图3-1</p>
<p>容错性：对于流式计算来说，容错性至关重要。首先我们要明确一下Spark中RDD的容错机制。每一个RDD都是一个不可变的分布式可重算的数据集，其记录着确定性的操作继承关系（lineage），所以只要输入数据是可容错的，那么任意一个RDD的分区（Partition）出错或不可用，都是可以利用原始输入数据通过转换操作而重新算出的。<br>对于Spark  Streaming来说，其RDD的传承关系如下图所示，图中的每一个椭圆形表示一个RDD，椭圆形中的每个圆形代表一个RDD中的一个Partition，图中的每一列的多个RDD表示一个DStream（图中有三个DStream），而每一行最后一个RDD则表示每一个Batch Size所产生的中间结果RDD。我们可以看到图中的每一个RDD都是通过lineage相连接的，由于Spark  Streaming输入数据可以来自于磁盘，例如HDFS（多份拷贝）或是来自于网络的数据流（Spark  Streaming会将网络输入数据的每一个数据流拷贝两份到其他的机器）都能保证容错性。所以RDD中任意的Partition出错，都可以并行地在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（如Storm）的效率更高。 如图3-2所示：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280907707.png" alt="img"><br>图3-2</p>
<p>实时性：对于实时性的讨论，会牵涉到流式处理框架的应用场景。Spark Streaming将流式计算分解成多个Spark  Job，对于每一段数据的处理都会经过Spark DAG图分解，以及Spark的任务集的调度过程。对于目前版本的Spark  Streaming而言，其最小的Batch Size的选取在0.5~2秒钟之间（Storm目前最小的延迟是100ms左右），所以Spark  Streaming能够满足除对实时性要求非常高（如高频实时交易）之外的所有流式准实时计算场景。<br>扩展性与吞吐量：Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB&#x2F;s的数据量（60M  records&#x2F;s），其吞吐量也比流行的Storm高2～5倍，图4是Berkeley利用WordCount和Grep两个用例所做的测试，在Grep这个测试中，Spark Streaming中的每个节点的吞吐量是670k records&#x2F;s，而Storm是115k records&#x2F;s。如图3-3所示：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280914711.png" alt="img"><br>图3-3</p>
<h2 id="3-2Spark-Streaming编程模型"><a href="#3-2Spark-Streaming编程模型" class="headerlink" title="3.2Spark Streaming编程模型"></a>3.2Spark Streaming编程模型</h2><p>Spark  Streaming的编程和Spark的编程如出一辙，对于编程的理解也非常类似。对于Spark来说，编程就是对于RDD的操作；而对于Spark  Streaming来说，就是对DStream的操作。下面将通过一个大家熟悉的WordCount的例子来说明Spark  Streaming中的输入操作、转换操作和输出操作。<br>Spark  Streaming初始化：在开始进行DStream操作之前，需要对Spark  Streaming进行初始化生成StreamingContext。参数中比较重要的是第一个和第三个，第一个参数是指定Spark  Streaming运行的集群地址，而第三个参数是指定Spark  Streaming运行时的batch窗口大小。在这个例子中就是将1秒钟的输入数据进行一次Spark Job处理。<br>val ssc &#x3D; new StreamingContext(“Spark:&#x2F;&#x2F;…”, “WordCount”, Seconds(1), [Homes], [Jars])<br>Spark Streaming的输入操作：目前Spark Streaming已支持了丰富的输入接口，大致分为两类：一类是磁盘输入，如以batch  size作为时间间隔监控HDFS文件系统的某个目录，将目录中内容的变化作为Spark  Streaming的输入；另一类就是网络流的方式，目前支持Kafka、Flume、Twitter和TCP  socket。在WordCount例子中，假定通过网络socket作为输入流，监听某个特定的端口，最后得出输入DStream（lines）。<br>val lines &#x3D; ssc.socketTextStream(“localhost”,8888)<br>Spark Streaming的转换操作：与Spark RDD的操作极为类似，Spark  Streaming也就是通过转换操作将一个或多个DStream转换成新的DStream。常用的操作包括map、filter、flatmap和join，以及需要进行shuffle操作的groupByKey&#x2F;reduceByKey等。在WordCount例子中，我们首先需要将DStream(lines)切分成单词，然后将相同单词的数量进行叠加, 最终得到的wordCounts就是每一个batch size的（单词，数量）中间结果。<br>val words &#x3D; lines.flatMap(<em>.split(“ “))<br>val wordCounts &#x3D; words.map(x &#x3D;&gt; (x, 1)).reduceByKey(</em> + <em>)<br>另外，Spark Streaming有特定的窗口操作，窗口操作涉及两个参数：一个是滑动窗口的宽度（Window  Duration）；另一个是窗口滑动的频率（Slide Duration），这两个参数必须是batch  size的倍数。例如以过去5秒钟为一个输入窗口，每1秒统计一下WordCount，那么我们会将过去5秒钟的每一秒钟的WordCount都进行统计，然后进行叠加，得出这个窗口中的单词统计。<br>val wordCounts &#x3D; words.map(x &#x3D;&gt; (x, 1)).reduceByKeyAndWindow(</em> + <em>, Seconds(5s)，seconds(1))<br>但上面这种方式还不够高效。如果我们以增量的方式来计算就更加高效，例如，计算t+4秒这个时刻过去5秒窗口的WordCount，那么我们可以将t+3时刻过去5秒的统计量加上[t+3，t+4]的统计量，在减去[t-2，t-1]的统计量，这种方法可以复用中间三秒的统计量，提高统计的效率。如图3-4所示：<br>val wordCounts &#x3D; words.map(x &#x3D;&gt; (x, 1)).reduceByKeyAndWindow(</em> + _, _ - _, Seconds(5s)，seconds(1))</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280923146.png" alt="img"><br>图3-4</p>
<p>Spark Streaming的输出操作：对于输出操作，Spark提供了将数据打印到屏幕及输入到文件中。在WordCount中我们将DStream wordCounts输入到HDFS文件中。<br>wordCounts &#x3D; saveAsHadoopFiles(“WordCount”)<br>Spark Streaming启动：经过上述的操作，Spark Streaming还没有进行工作，我们还需要调用Start操作，Spark Streaming才开始监听相应的端口，然后收取数据，并进行统计。<br>ssc.start()</p>
<h2 id="3-3-Spark-Streaming典型案例"><a href="#3-3-Spark-Streaming典型案例" class="headerlink" title="3.3 Spark Streaming典型案例"></a>3.3 Spark Streaming典型案例</h2><p>在互联网应用中，网站流量统计作为一种常用的应用模式，需要在不同粒度上对不同数据进行统计，既有实时性的需求，又需要涉及到聚合、去重、连接等较为复杂的统计需求。传统上，若是使用Hadoop  MapReduce框架，虽然可以容易地实现较为复杂的统计需求，但实时性却无法得到保证；反之若是采用Storm这样的流式框架，实时性虽可以得到保证，但需求的实现复杂度也大大提高了。Spark Streaming在两者之间找到了一个平衡点，能够以准实时的方式容易地实现较为复杂的统计需求。 下面介绍一下使用Kafka和Spark  Streaming搭建实时流量统计框架。<br>数据暂存：Kafka作为分布式消息队列，既有非常优秀的吞吐量，又有较高的可靠性和扩展性，在这里采用Kafka作为日志传递中间件来接收日志，抓取客户端发送的流量日志，同时接受Spark Streaming的请求，将流量日志按序发送给Spark Streaming集群。<br>数据处理：将Spark  Streaming集群与Kafka集群对接，Spark Streaming从Kafka集群中获取流量日志并进行处理。Spark  Streaming会实时地从Kafka集群中获取数据并将其存储在内部的可用内存空间中。当每一个batch窗口到来时，便对这些数据进行处理。<br>结果存储：为了便于前端展示和页面请求，处理得到的结果将写入到数据库中。<br>相比于传统的处理框架，Kafka+Spark Streaming的架构有以下几个优点。Spark框架的高效和低延迟保证了Spark  Streaming操作的准实时性。利用Spark框架提供的丰富API和高灵活性，可以精简地写出较为复杂的算法。编程模型的高度一致使得上手Spark Streaming相当容易，同时也可以保证业务逻辑在实时处理和批处理上的复用。<br>Spark  Streaming提供了一套高效、可容错的准实时大规模流式处理框架，它能和批处理及即时查询放在同一个软件栈中。如果你学会了Spark编程，那么也就学会了Spark Streaming编程，如果理解了Spark的调度和存储，Spark Streaming也类似。按照目前的发展趋势，Spark  Streaming一定将会得到更大范围的使用。</p>
</blockquote>
<h3 id="4-1编写Spark-steaming代码"><a href="#4-1编写Spark-steaming代码" class="headerlink" title="4.1编写Spark-steaming代码"></a>4.1编写Spark-steaming代码</h3><p>点击File -&gt; New -&gt; Scala project-&gt; -&gt; 输入name 新建一个project<br>把spark目录下jars的包全部下载到本地，点击project -&gt; java build path -&gt; libraries -&gt; add external jars,把下载的包全部导入到项目中<br>在目录下，点击src新建一个scala object命名为scalaSparkStreaming。<br>在SparkStreaming中键入代码。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkContext</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.StreamingContext</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Seconds</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.ReceiverInputDStream</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.DStream</span><br><span class="line"></span><br><span class="line">object scalaSparkStreaing &#123;</span><br><span class="line"></span><br><span class="line">  def <span class="title function_">main</span><span class="params">(args: Array[String])</span>: Unit = &#123;</span><br><span class="line">    <span class="type">val</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setAppName(<span class="string">&#x27;SparkStreamingWordCount&#x27;</span>).setMaster(<span class="string">&#x27;spark://master:7077&#x27;</span>);</span><br><span class="line">    <span class="type">val</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkContext</span>(conf)</span><br><span class="line">    <span class="type">val</span> <span class="variable">ssc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StreamingContext</span>(sc, Seconds(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    sc.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    val lines: ReceiverInputDStream[String] = ssc.socketTextStream(<span class="string">&#x27;master&#x27;</span>, <span class="number">9999</span>)</span><br><span class="line">    val result: DStream[(String, Int)] = lines.flatMap(_.split(<span class="string">&#x27; &#x27;</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>点击项目选择export-&gt;java-&gt;JAR file点击finish，在finish之前记得选好输出位置<br>选择刚才设置的jar包，上传到master上去。<br>新建一个SSH连接，登录master服务器，使用命令nc -lk 9999设置路由器。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># nc -lk 9999</span><br></pre></td></tr></table></figure>

<p>注：如果系统只没有nc这个命令，可以使用yum install nc安装nc命令。<br>进入spark的安装目录，执行下面的命令。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/spark</span><br><span class="line"># bin/spark-submit --class sparkStreaming root/sparkstreaming.jar</span><br></pre></td></tr></table></figure>

<p>在网络流中输入单词。按回车结束一次输出。<br>在命令提交的xshell连接中观察程序输出。</p>
<blockquote>
<p>实验结果如下： 在提交任务之后应该能看到以下结果（因屏幕刷新很快，所以只能看到部分结果）。 在nc -lk 9999 命令下输入 所示结果中应该立刻显示出如下内容 <img src="/static/upload/resource/exp/ins/e9169981905743c7a76235f545db15d0/image/image.1676280954550.png" alt="image.png"></p>
</blockquote>
<h2 id="24-Spark实验：SparkWordCount"><a href="#24-Spark实验：SparkWordCount" class="headerlink" title="24.Spark实验：SparkWordCount"></a>24.Spark实验：SparkWordCount</h2><blockquote>
<h3 id="目的-3"><a href="#目的-3" class="headerlink" title="目的"></a>目的</h3><p>1.熟悉Scala语言，基于Spark思想，编写SparkWordCount程序。</p>
<h3 id="要求-3"><a href="#要求-3" class="headerlink" title="要求"></a>要求</h3><p>1.熟悉Scala语言，理解Spark编程思想，并会编写Spark 版本的WordCount，然后能够在spark-shell中执行代码和分析执行过程。</p>
<h3 id="原理-3"><a href="#原理-3" class="headerlink" title="原理"></a>原理</h3><p>Scala 是一门以 Java 虚拟机（JVM）为目标运行环境并将面向对象 (OO) 和函数式编程语言 (FP) 的最佳特性结合在一起的编程语言。<br>它既有动态语言那样的灵活简洁，同时又保留了静态类型检查带来的安全保障和执行效率，加上其强大的抽象能力，既能处理脚本化的临时任务，又能处理高并发场景下的分布式互联网大数据应用，可谓能缩能伸。<br>Scala 运行在JVM 之上，因此它可以访问任何 Java 类库并且与 Java 框架进行互操作。其与Java的集成度很高，可以直接使用 Java  社区大量成熟的技术框架和方案。由于它直接编译成Java字节码，因此我们可以充分利用JVM这个高性能的运行平台。</p>
<h2 id="3-1-Scala是兼容的"><a href="#3-1-Scala是兼容的" class="headerlink" title="3.1 Scala是兼容的"></a>3.1 Scala是兼容的</h2><p>Scala被设计成无缝地与Java实施互操作。不需要你从Java平台后退两步然后跳到Java语言前面去。它允许你在现存代码中加点儿东西——在你已有的东西上建设，Scala程序会被编译为JVM的字节码。它们的执行期性能通常与Java程序一致。Scala代码可以调用Java方法，访问Java字段，继承自Java类和实现Java接口。这些都不需要特别的语法，显式接口描述，或粘接代码。实际上，几乎所有Scala代码都极度依赖于Java库，而程序员无须意识到这点 。</p>
<p>交互式操作的另一个方面是Scala极度重用了Java类型  。Scala的Int类型代表了Java的原始整数类型int，Float代表了float，Boolean代表boolean，等等。Scala的数组被映射到Java数组。Scala同样重用了许多标准Java库类型。例如，Scala里的字串文本”abc”是java.lang.String，而抛出的异常必须是java.lang.Throwable的子类。</p>
<p>Scala不仅重用了Java的类型，还把它们“打扮”得更漂亮  。例如，Scala的字串支持类似于toInt和toFloat的方法，可以把字串转换成整数或者浮点数。因此你可以写str.toInt替代Integer.parseInt(str)。如何在不打破互操作性的基础上做到这点呢？Java的String类当然不会有toInt方法。实际上，Scala有一个解决这种高级库设计和互操作性不相和谐的通用方案。Scala可以让你定义隐式转换：implicit  conversion，这常常用在类型失配，或者选用不存在的方法时。在上面的例子里，当在字串中寻找toInt方法时，Scala编译器会发现String类里没有这种方法，但它会发现一个把Java的String转换为Scala的RichString类的一个实例的隐式转换，里面定义了这么个方法。于是在执行toInt操作之前，转换被隐式应用 。<br>Scala代码同样可以由Java代码调用 。有时这种情况要更加微妙，因为Scala是一种比Java更丰富的语言，有些Scala更先进的特性在它们能映射到Java前需要先被编码一下。</p>
<h2 id="3-2Scala是简洁的"><a href="#3-2Scala是简洁的" class="headerlink" title="3.2Scala是简洁的"></a>3.2Scala是简洁的</h2><p>Scala程序一般都很短  。Scala程序员曾报告说与Java比起来代码行数可以减少到1&#x2F;10。这有可能是个极限的例子。较保守的估计大概标准的Scala程序应该有Java写的同样的程序一半行数左右。更少的行数不仅意味着更少的打字工作，同样意味着更少的话在阅读和理解程序上的努力及更少的出错可能。许多因素在减少代码行上起了作用 。Scala的语法避免了一些束缚Java程序的固定写法  。例如，Scala里的分号是可选的，且通常不写。Scala语法里还有很多其他的地方省略了东西。比方说，比较一下你在Java和Scala里是如何写类及构造函数的。</p>
<p>在Java里，带有构造函数的类经常看上去是这个样子：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class MyClass &#123;</span><br><span class="line">    private int index;</span><br><span class="line">    private String name;</span><br><span class="line">    public MyClass(int index, String name) &#123;</span><br><span class="line">        this.index = index;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Scala里，你会写成这样：<br>　　class MyClass(index: Int, name: String)<br>根据这段代码，Scala编译器将制造有两个私有成员变量的类，一个名为index的Int类型和一个叫做name的String类型，还有一个用这些变量作为参数获得初始值的构造函数。这个构造函数还将用作为参数传入的值初始化这两个成员变量。一句话，你实际拿到了与罗嗦得多的Java版本同样的功能。Scala类写起来更快，读起来更容易，最重要的是，比Java类更不容易犯错。<br>有助于Scala的简洁易懂的另一个因素是它的类型推断  。重复的类型信息可以被忽略，因此程序变得更有条理和易读，但或许减少代码最关键的是因为已经存在于你的库里而不需要写的代码。Scala给了你许多工具来定义强有力的库让你抓住并提炼出通用的行为。例如，库类的不同方面可以被分成若干特质，而这些又可以被灵活地混合在一起。或者，库方法可以用操作符参数化，从而让你有效地定义那些你自己控制的构造。这些构造组合在一起，就能够让库的定义既是高层级的又能灵活运用。</p>
<h2 id="3-3Scala是高级的"><a href="#3-3Scala是高级的" class="headerlink" title="3.3Scala是高级的"></a>3.3Scala是高级的</h2><p>程序员总是在和复杂性纠缠 。为了高产出的编程，你必须明白你工作的代码。过度复杂的代码成了很多软件工程崩溃的原因。不幸的是，重要的软件往往有复杂的需求。这种复杂性不可避免；必须（由不受控）转为受控。<br>Scala可以通过让你提升你设计和使用的接口的抽象级别来帮助你管理复杂性 。例如，假设你有一个String变量name，你想弄清楚是否String包含一个大写字符。<br>在Java里，你或许这么写：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">boolean nameHasUpperCase = false;</span><br><span class="line">for (int i = 0; i &gt; name.length(); ++i) &#123;</span><br><span class="line">    if (Character.isUpperCase(name.charAt(i))) &#123;</span><br><span class="line">        nameHasUpperCase = true;</span><br><span class="line">        break;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Scala里，你可以写成：<br>val nameHasUpperCase &#x3D; name.exists(<em>.isUpperCase)<br>Java代码把字符串看作循环中逐字符步进的低层级实体。Scala代码把同样的字串当作能用论断：predicate查询的字符高层级序列  。明显Scala代码更短并且——对训练有素的眼睛来说——比Java代码更容易懂。因此Scala代码在通盘复杂度预算上能极度地变轻，它也更少给你机会犯错 。<br>论断，</em>.isUpperCase，是一个Scala里面函数式文本的例子  。它描述了带一个字符参量（用下划线字符代表）的函数，并测试其是否为大写字母 。原则上，这种控制的抽象在Java中也是可能的  ，为此需要定义一个包含抽象功能的方法的接口。例如，如果你想支持对字串的查询，就应引入一个只有一个方法hasProperty的接口CharacterProperty：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">interface CharacterProperty &#123;</span><br><span class="line">    boolean hasProperty(char ch);</span><br><span class="line">&#125;</span><br><span class="line">然后你可以在Java里用这个接口格式一个方法exists：它带一个字串和一个CharacterProperty并返回真如果字串中有某个字符符合属性。然后你可以这样调用exists：</span><br><span class="line">exists(name, new CharacterProperty &#123;</span><br><span class="line">    boolean hasProperty(char ch) &#123;</span><br><span class="line">        return Character.isUpperCase(ch);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<p>然而，所有这些真的感觉很重。重到实际上多数Java程序员都不会惹这个麻烦。他们会宁愿写个循环并漠视他们代码里复杂性的累加。另一方面，Scala里的函数式文本真的很轻量，于是就频繁被使用。随着对Scala的逐步了解，你会发现越来越多定义和使用你自己的控制抽象的机会。你将发现这能帮助避免代码重复并因此保持你的程序简短和清晰。</p>
<h2 id="3-4Scala是静态类型的"><a href="#3-4Scala是静态类型的" class="headerlink" title="3.4Scala是静态类型的"></a>3.4Scala是静态类型的</h2><p>静态类型系统认定变量和表达式与它们持有和计算的值的种类有关  。Scala坚持作为一种具有非常先进的静态类型系统的语言。从Java那样的内嵌类型系统起步，能够让你使用泛型：generics参数化类型，用交集：intersection联合类型和用抽象类型：abstract  type隐藏类型的细节。这些为建造和组织你自己的类型打下了坚实的基础，从而能够设计出即安全又能灵活使用的接口。静态类型系统的经典优越性将更被赏识，其中最重要的包括程序抽象的可检验属性，安全的重构，以及更好的文档。</p>
<p>可检验属性  。静态类型系统可以保证消除某些运行时的错误。例如，可以保证这样的属性：布尔型不会与整数型相加；私有变量不会从类的外部被访问；函数带了正确个数的参数；只有字串可以被加到字串集之中  。不过当前的静态类型系统还不能查到其他类型的错误。比方说，通常查不到无法终结的函数，数组越界，或除零错误。同样也查不到你的程序不符合式样书（假设有这么一份式样书）。静态类型系统因此被认为不很有用而被忽视。舆论认为既然这种类型系统只能发现简单错误，而单元测试能提供更广泛的覆盖，又为何自寻烦恼呢？我们认为这种论调不对头。尽管静态类型系统确实不能替代单元测试，但是却能减少用来照顾那些确需测试的属性的单元测试的数量。同样，单元测试也不能替代静态类型。总而言之，如Edsger Dijkstra所说，测试只能证明存在错误，而非不存在。因此，静态类型能给的保证或许很简单，但它们是无论多少测试都不能给的真正的保证 。</p>
<p>安全的重构  。静态类型系统提供了让你具有高度信心改动代码基础的安全网。试想一个对方法加入额外的参数的重构实例。在静态类型语言中，你可以完成修改，重编译你的系统并容易修改所有引起类型错误的代码行。一旦你完成了这些，你确信已经发现了所有需要修改的地方。对其他的简单重构，如改变方法名或把方法从一个类移到另一个，这种确信都有效。所有例子中静态类型检查会提供足够的确认，表明新系统和旧系统可以一样的工作 。</p>
<p>文档  。静态类型是被编译器检查过正确性的程序文档。不像普通的注释，类型标注永远都不会过期（至少如果包含它的源文件近期刚刚通过编译就不会）。更进一步说，编译器和集成开发环境可以利用类型标注提供更好的上下文帮助。举例来说，集成开发环境可以通过判定选中表达式的静态类型，找到类型的所有成员，并全部显示出来。<br>虽然静态类型对程序文档来说通常很有用，当它们弄乱程序时，也会显得很讨厌。标准意义上来说，有用的文档是那些程序的读者不可能很容易地从程序中自己想出来的。在如下的方法定义中：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def f(x: String) = ...</span><br></pre></td></tr></table></figure>

<p>知道f的变量应该是String是有用的。另一方面，以下例子中两个标注至少有一个是讨厌的：</p>
<p>val x: HashMap[Int, String] &#x3D; new HashMap<a target="_blank" rel="noopener" href="http://10.131.2.101/#/course/laboratory?courseId=9beb8dc3ec64423898817968062fbf26&expId=d0b94b1899d84ffcb390235c8d939244&recordId=faffc39768104c3eb2810b0c6f60f756">Int, String</a></p>
<p>很明显，x是以Int为键，String为值的HashMap这句话说一遍就够了；没必要同样的句子重复两遍。<br>Scala有非常精于此道的类型推断系统，能让你省略几乎所有的通常被认为是讨厌的类型信息。在上例中，以下两个不太讨厌的替代品也能一样工作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val x = new HashMap[Int, String]()</span><br><span class="line">val x: Map[Int, String] = new HashMap()</span><br></pre></td></tr></table></figure>

<p>Scala里的类型推断可以走的很远  。实际上，就算用户代码丝毫没有显式类型也不稀奇。因此，Scala编程经常看上去有点像是动态类型脚本语言写出来的程序。尤其显著表现在作为粘接已写完的库控件的客户应用代码上。而对库控件来说不是这么回事，因为它们常常用到相当精妙的类型去使其适于灵活使用的模式。这很自然。综上，构成可重用控件接口的成员的类型符号应该是显式给出的，因为它们构成了控件和它的使用者间契约的重要部分。</p>
</blockquote>
<h3 id="4-1编写WordCount"><a href="#4-1编写WordCount" class="headerlink" title="4.1编写WordCount"></a>4.1编写WordCount</h3><p>在hdfs的&#x2F;user&#x2F;spark&#x2F;in&#x2F;目录下建一个in.txt文件，内容如下，单词之间用空格隔开</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello world</span><br><span class="line">ni hao</span><br><span class="line">hello my friend</span><br><span class="line">ni are my sunshine</span><br></pre></td></tr></table></figure>

<p>在spark-shell中编写WordCount代码和运行。<br>启动spark-shell。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/spark</span><br><span class="line"># bin/spark-shell --master spark://master:7077</span><br></pre></td></tr></table></figure>

<p>写入wordcount的scala代码并运行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val file=sc.textFile(&#x27;hdfs://master:8020/user/spark/in/in.txt&#x27;)</span><br><span class="line">scala&gt; val count=file.flatMap(line =&gt; line.split(&#x27; &#x27;)).map(word =&gt; (word,1)).reduceByKey(_+_)</span><br><span class="line">scala&gt; count.collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676359032206.png" alt="img"></p>
<h2 id="25-Spark实验：Spark机器学习入门实战"><a href="#25-Spark实验：Spark机器学习入门实战" class="headerlink" title="25.Spark实验：Spark机器学习入门实战"></a>25.Spark实验：Spark机器学习入门实战</h2><blockquote>
<h3 id="目的-4"><a href="#目的-4" class="headerlink" title="目的"></a>目的</h3><p>1.学习操作spark.ml库</p>
<h3 id="要求-4"><a href="#要求-4" class="headerlink" title="要求"></a>要求</h3><p>2.掌握本实验spark.mllib以及spark.ml的项目实例</p>
<h3 id="原理-4"><a href="#原理-4" class="headerlink" title="原理"></a>原理</h3><p>MLlib(Machine Learning Library) 是 Spark 的机器学习（ML）库。其目标是使实用的机器学习具有可扩展性并且变得容易,它能够较容易地解决一些实际的大规模机器学习问题。在较高的水平上，它提供了以下工具：<br>ML Algorithms （ML 算法）: 常用的学习算法，如分类，回归，聚类和协同过滤 Featurization （特征）:  特征提取，变换，降维和选择 Pipelines （管道）: 用于构建，评估和调整 ML Pipelines 的工具 Persistence  （持久性）: 保存和加载算法，模型和 Pipelines Utilities （实用）: 线性代数，统计学，数据处理等</p>
</blockquote>
<h3 id="4-1数据统计"><a href="#4-1数据统计" class="headerlink" title="4.1数据统计"></a>4.1数据统计</h3><p>运行 spark-shell:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure>

<p>首先我们使用 MLlib 中的 Statistics 模块对 RDD 进行基本的数据统计。主要用到的函数是colStats()。<br>引入统计相关的包，其中 Vectors 用来构建统计数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.stat.&#123;MultivariateStatisticalSummary, Statistics&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280502472.png" alt="img"></p>
<p>构建测试数据，我们使用三个学生的成绩 Vector 来构建所需的 RDD Vector，这个矩阵里的每个 Vector 都代表一个学生在四门课程里的分数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.linalg.&#123;Vector, Vectors&#125;</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line"></span><br><span class="line">val array1: Array[Double] = Array[Double](60, 70, 80, 0)</span><br><span class="line">val array2: Array[Double] = Array[Double](80, 50, 0, 90)</span><br><span class="line">val array3: Array[Double] = Array[Double](60, 70, 80, 0)</span><br><span class="line">val denseArray1 = Vectors.dense(array1)</span><br><span class="line">val denseArray2 = Vectors.dense(array2)</span><br><span class="line">val denseArray3 = Vectors.dense(array3)</span><br><span class="line"></span><br><span class="line">val seqDenseArray: Seq[Vector] = Seq(denseArray1, denseArray2, denseArray3)</span><br><span class="line"></span><br><span class="line">val basicTestRDD: RDD[Vector] = sc.parallelize[Vector](seqDenseArray)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280512466.png" alt="img"></p>
<p>获取统计信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val summary: MultivariateStatisticalSummary = Statistics.colStats(basicTestRDD)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280520403.png" alt="img"></p>
<p>可以查看下 summary 里的成员，这个对象中包含了大量的统计内容，我们把其中几项打印出来：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 每一列的平均分数，即每门课三位学生的平均分数</span><br><span class="line">println(summary.mean)</span><br><span class="line"></span><br><span class="line">// 每一列的方差</span><br><span class="line">println(summary.variance)</span><br><span class="line"></span><br><span class="line">// 每一列中不为0的成绩数量</span><br><span class="line">println(summary.numNonzeros)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280530479.png" alt="img"></p>
<h3 id="4-2使用Spark-ml"><a href="#4-2使用Spark-ml" class="headerlink" title="4.2使用Spark.ml"></a>4.2使用Spark.ml</h3><p>计算两组数据之间的相关性。MLlib 对 Pearson 相关系数和 Spearman 等级相关系数算法提供支持。<br>实验中，我们仍然先构建测试数据，8位用户在每天不同的时间段可以不中断持续学习的时间。分别提供的两组数据是上午和晚上的有效学习时间，我们计算这两组学习时间数据是否具有相关性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val morningStudyTime: RDD[Double] = sc.parallelize(List(55, 54, 60, 60, 45, 20, 85, 40), 2)</span><br><span class="line">val nightStudyTime: RDD[Double] = sc.parallelize(List(80, 90, 80, 90, 70, 20, 100, 60), 2)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280540210.png" alt="img"></p>
<p>选择 Pearson 相关系数算法，并调用corr()函数进行计算：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val corrType = &#x27;pearson&#x27;</span><br><span class="line">val corr: Double = Statistics.corr(morningStudyTime, nightStudyTime, corrType)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280548354.png" alt="img"></p>
<p>输出计算结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">println(s&#x27;Correlation: \t $corr&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280555387.png" alt="img"></p>
<p>可以自己调用 Spearman 等级相关系数算法对比下测试结果。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.clustering.&#123;KMeans, KMeansModel&#125;</span><br><span class="line">import org.apache.spark.mllib.linalg.Vectors</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280564700.png" alt="img"></p>
<p>首先构建数据集合，我们直接使用课程提供的数据，数据文件在&#x2F;usr&#x2F;cstor&#x2F;spark&#x2F;data&#x2F;mllib&#x2F;kmeans_data.txt。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 读取数据文件，创建RDD</span><br><span class="line">val dataFile = &#x27;/usr/cstor/spark/data/mllib/kmeans_data.txt&#x27;</span><br><span class="line">val lines = sc.textFile(dataFile)</span><br><span class="line"></span><br><span class="line">// 创建Vector，将每行的数据用空格分隔后转成浮点值返回numpy的array</span><br><span class="line">val data = lines.map(s =&gt; Vectors.dense(s.split(&#x27; &#x27;).map(_.toDouble))).cache()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280579694.png" alt="img"></p>
<p>调用 KMeans 模块的train()函数来使用算法，函数的参数一个是数据输入，一个是 K 值：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val numClusters = 2</span><br><span class="line">val numIterations = 20</span><br><span class="line">val model = KMeans.train(data, numClusters, numIterations)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280590749.png" alt="img"></p>
<p>统计聚类错误的样本比例代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val WSSSE = model.computeCost(data)</span><br><span class="line"></span><br><span class="line">println(s&#x27;Within Set Sum of Squared Errors = $WSSSE&#x27;)</span><br><span class="line"></span><br><span class="line">//输出模型的聚集中心</span><br><span class="line">model.clusterCenters</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280611921.png" alt="img"></p>
<p>MLlib 提供了 Linear Support Vector Machine 算法的实现。这种算法是大规模分类的一种标准算法，线性 SVM 算法输出的是 SVM 模型。<br>实验的数据我们直接使用官方提供的数据&#x2F;usr&#x2F;cstor&#x2F;spark&#x2F;data&#x2F;mllib&#x2F;sample_libsvm_data.txt。<br>下面开始对这个数据样本进行导入，模型训练并进行预测。<br>首先引入需要的SVMWithSGD模块：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.classification.&#123;SVMModel, SVMWithSGD&#125;</span><br><span class="line">import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</span><br><span class="line">import org.apache.spark.mllib.util.MLUtils</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280670294.png" alt="img"><br>加载数据文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val dataFile = &#x27;/usr/cstor/spark/data/mllib/sample_libsvm_data.txt&#x27;</span><br><span class="line">val data = MLUtils.loadLibSVMFile(sc, dataFile)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280678835.png" alt="img"><br>分割数据，一部分（80%）用来训练，一部分（20%）用来测试：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 随机分割数据，第一个参数指定了分割的比例，第二个参数是一个随机数种子</span><br><span class="line">val splits = data.randomSplit(Array(0.8, 0.2), seed = 9L)</span><br><span class="line">val training = splits(0).cache()</span><br><span class="line">val test = splits(1)</span><br><span class="line"></span><br><span class="line">// 打印分割后的数据量</span><br><span class="line">println(&#x27;TrainingCount:&#x27;+training.count())</span><br><span class="line">println(&#x27;TestingCount:&#x27;+test.count())</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280688232.png" alt="img"></p>
<p>你会发现并不是完全按照 randomSplit 的参数进行分配的。<br>开始训练，调用SVMWithSGD中的train()函数，这个函数的第一个参数为 RDD 训练数据输入，第二个参数为迭代次数，这里我们设置迭代 100 次，此外这个函数还有其他的参数来设置 SGD 步骤，正则化参数，每轮迭代输入的样本比例，正则化类型等：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val model = SVMWithSGD.train(training, 100)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280696597.png" alt="img"></p>
<p>得到的 SVM 模型后，我们需要进行后续的操作并把结果输出。<br>清除默认的阈值，清除后会输出预测的数字：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.clearThreshold()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280710928.png" alt="img"></p>
<p>使用测试数据进行预测计算：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val scoreAndLabels = test.map &#123; point =&gt;</span><br><span class="line">  val score = model.predict(point.features)</span><br><span class="line">  (score, point.label)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280718718.png" alt="img"></p>
<p>输出结果，包含预测的数字结果和 0&#x2F;1 结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for ((score, label) &lt;-  scoreAndLabels.collect())</span><br><span class="line">println(score+&#x27;,&#x27;+label)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280728038.png" alt="img"><br>其他 MLlib 算法实例可以在下面的目录找到：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/usr/cstor/spark/examples/src/main/scala/org/apache/spark/examples/mllib</span><br><span class="line">/usr/cstor/spark/examples/src/main/scala/org/apache/spark/examples/ml/</span><br></pre></td></tr></table></figure>

<p>测试数据可以在下面的目录找到：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/cstor/spark/data/mllib</span><br></pre></td></tr></table></figure>

<p>可以通过实例代码阅读，运行对比数据与结果来继续学习 MLlib 其他的算法。<br>Word2Vec 是一个估计器(拟合一个  DataFrame 来产生转换器的算法)，它采用一系列代表文档的词语来训练 Word2Vec  模型。这个模型把每一个词语映射到一个固定大小的向量。Word2Vec  模型使用文档中每个词语的平均数来把文档转换为向量，把这个向量作为预测特征用来计算文档相似度。<br>首先引入 feature.Word2Vec、linalg.Vector、sql.Row 模块：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.ml.feature.Word2Vec</span><br><span class="line">import org.apache.spark.ml.linalg.Vector</span><br><span class="line">import org.apache.spark.sql.Row</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280740760.png" alt="img"></p>
<p>输入数据，每行都是来自文档中的一句完整的话并将其转为 DataFrame 格式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val documentDF = spark.createDataFrame(Seq(</span><br><span class="line">  &#x27;Hi I heard about Spark&#x27;.split(&#x27; &#x27;),</span><br><span class="line">  &#x27;I wish Java could use case classes&#x27;.split(&#x27; &#x27;),</span><br><span class="line">  &#x27;Logistic regression models are neat&#x27;.split(&#x27; &#x27;)</span><br><span class="line">).map(Tuple1.apply)).toDF(&#x27;text&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280748256.png" alt="img"></p>
<p>使用 Word2Vec 类新建一个对象，学习从单词到矢量的映射：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val word2Vec = new Word2Vec()</span><br><span class="line">  .setInputCol(&#x27;text&#x27;)</span><br><span class="line">  .setOutputCol(&#x27;result&#x27;)</span><br><span class="line">  .setVectorSize(3)</span><br><span class="line">  .setMinCount(0)</span><br><span class="line"></span><br><span class="line">val model = word2Vec.fit(documentDF)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280757339.png" alt="img"><br>使用 foreach 依次打印出每句话中每一个单词对应的矢量映射：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val result = model.transform(documentDF)</span><br><span class="line"></span><br><span class="line">result.collect().foreach &#123; case Row(text: Seq[_], features: Vector) =&gt;println(s&#x27;Text: [$&#123;text.mkString(&#x27;, &#x27;)&#125;] =&gt; \nVector: $features\n&#x27;) &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280767545.png" alt="img"></p>
<h3 id="4-3朴素贝叶斯算法"><a href="#4-3朴素贝叶斯算法" class="headerlink" title="4.3朴素贝叶斯算法"></a>4.3朴素贝叶斯算法</h3><p>朴素贝叶斯算法的思想是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，在没有其它可用信息下，我们会选择条件概率最大的类别作为这个待分类项所对应的类别。<br>实验的数据我们直接使用官方提供的数据&#x2F;usr&#x2F;cstor&#x2F;spark&#x2F;data&#x2F;mllib&#x2F;sample_libsvm_data.txt。<br>首先引入 classification.NaiveBayes、evaluation.MulticlassClassificationEvaluator包：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.ml.classification.NaiveBayes</span><br><span class="line">import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280777688.png" alt="img"></p>
<p>加载官方提供的数据文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val data = spark.read.format(&#x27;libsvm&#x27;).load(&#x27;/usr/cstor/spark/data/mllib/sample_libsvm_data.txt&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280784523.png" alt="img"></p>
<p>把数据分为两份，其中 70% 作为训练数据集，30% 作为测试数据集：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3), seed = 1234L)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280793625.png" alt="img"></p>
<p>使用 NaiveBayes 类新建一个对象，训练朴素贝叶斯模型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val model = new NaiveBayes().fit(trainingData)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280803098.png" alt="img"></p>
<p>选择 20 行显示来查看数据内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val predictions = model.transform(testData)</span><br><span class="line"></span><br><span class="line">predictions.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280819767.png" alt="img"></p>
<p>使用 MulticlassClassificationEvaluator 类新建一个对象，选择(预测，真实标签)并计算测试错误：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val evaluator = new MulticlassClassificationEvaluator()</span><br><span class="line">  .setLabelCol(&#x27;label&#x27;)</span><br><span class="line">  .setPredictionCol(&#x27;prediction&#x27;)</span><br><span class="line">  .setMetricName(&#x27;accuracy&#x27;)</span><br><span class="line"></span><br><span class="line">val accuracy = evaluator.evaluate(predictions)</span><br><span class="line"></span><br><span class="line">println(s&#x27;Test set accuracy = $accuracy&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280827316.png" alt="img"></p>
<h3 id="4-4二分-K-均值算法"><a href="#4-4二分-K-均值算法" class="headerlink" title="4.4二分 K 均值算法"></a>4.4二分 K 均值算法</h3><p>二分 K  均值算法是一种层次聚类算法，使用自顶向下的逼近：所有的观察值开始是一个簇，递归地向下一个层级分裂。分裂依据为选择能最大程度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。以此进行下去，直到簇的数目等于用户给定的数目 k 为止。二分 K 均值常常比传统 K 均值算法有更快的计算速度，但产生的簇群与传统 K 均值算法往往也是不同的。<br>实验的数据我们直接使用官方提供的数据&#x2F;usr&#x2F;cstor&#x2F;spark&#x2F;data&#x2F;mllib&#x2F;sample_kmeans_data.txt。<br>首先引入clustering.BisectingKMeans模块：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.ml.clustering.BisectingKMeans</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280837942.png" alt="img"></p>
<p>加载官方提供的数据文件:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val dataset = spark.read.format(&#x27;libsvm&#x27;).load(&#x27;/sample_kmeans_data.txt&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280846557.png" alt="img"></p>
<p>使用 BisectingKMeans 新建一个对象，训练二分 K 均值模型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val bkm = new BisectingKMeans().setK(2).setSeed(1)</span><br><span class="line">val model = bkm.fit(dataset)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280856622.png" alt="img"></p>
<p>评估集群：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val cost = model.computeCost(dataset)</span><br><span class="line">println(s&#x27;Within Set Sum of Squared Errors = $cost&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280865077.png" alt="img"></p>
<p>展示训练结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">println(&#x27;Cluster Centers: &#x27;)</span><br><span class="line">val centers = model.clusterCenters</span><br><span class="line">centers.foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280872367.png" alt="img"></p>
<h2 id="26-Spark实验：Spark综例"><a href="#26-Spark实验：Spark综例" class="headerlink" title="26.Spark实验：Spark综例"></a>26.Spark实验：Spark综例</h2><blockquote>
<h3 id="目的-5"><a href="#目的-5" class="headerlink" title="目的"></a>目的</h3><p>1． 理解Spark编程思想；<br>2． 学会在Spark Shell中编写Scala程序；<br>3． 学会在Spark Shell中运行Scala程序。</p>
<h3 id="要求-5"><a href="#要求-5" class="headerlink" title="要求"></a>要求</h3><p>实验结束后，能够编写Scala代码解决一下问题，并能够自行分析执行过程。<br>有三个RDD，要求统计rawRDDA中“aa”、“bb”两个单词出现的次数；要求对去重后的rawRDDA再去掉rawRDDB中的内容；最后将上述两个结果合并成同一个文件然后存入HDFS中。</p>
<h3 id="原理-5"><a href="#原理-5" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1Scala"><a href="#3-1Scala" class="headerlink" title="3.1Scala"></a>3.1Scala</h2><p>Scala是一门多范式的编程语言，一种类似java的编程语言，设计初衷是实现可伸缩的语言、并集成面向对象编程和函数式编程的各种特性。<br>Scala有几项关键特性表明了它的面向对象的本质。例如，Scala中的每个值都是一个对象，包括基本数据类型（即布尔值、数字等）在内，连函数也是对象。另外，类可以被子类化，而且Scala还提供了基于mixin的组合（mixin-based composition）。<br>与只支持单继承的语言相比，Scala具有更广泛意义上的类重用。Scala允许定义新类的时候重用“一个类中新增的成员定义（即相较于其父类的差异之处）”。Scala称之为mixin类组合。</p>
<p>Scala还包含了若干函数式语言的关键概念，包括高阶函数（Higher-Order Function）、局部套用（Currying）、嵌套函数（Nested Function）、序列解读（Sequence Comprehensions）等等。</p>
<p>Scala是静态类型的，这就允许它提供泛型类、内部类、甚至多态方法（Polymorphic Method）。另外值得一提的是，Scala被特意设计成能够与Java和.NET互操作。<br>Scala可以与Java互操作。它用scalac这个编译器把源文件编译成Java的class文件。你可以从Scala中调用所有的Java类库，也同样可以从Java应用程序中调用Scala的代码。<br>这让Scala得以使用为Java1.4、5.0或者6.0编写的巨量的Java类库和框架，Scala会经常性地针对这几个版本的Java进行测试。Scala可能也可以在更早版本的Java上运行，但没有经过正式的测试。Scala以BSD许可发布，并且数年前就已经被认为相当稳定了。</p>
<p>Scala旨在提供一种编程语言，能够统一和一般化分别来自面向对象和函数式两种不同风格的关键概念。藉着这个目标与设计，Scala得以提供一些出众的特性，包括：<br>（1）面向对象风格<br>（2）函数式风格<br>（3）更高层的并发模型<br>Scala把Erlang风格的基于actor的并发带进了JVM。开发者可以利用Scala的actor模型在JVM上设计具伸缩性的并发应用程序，它会自动获得多核心处理器带来的优势，而不必依照复杂的Java线程模型来编写程序。<br>（4）轻量级的函数语法<br>高阶；<br>嵌套；<br>局部套用（Currying）；<br>匿名。<br>（5）与XML集成<br>可在Scala程序中直接书写XML；<br>可将XML转换成Scala类。<br>（6）与Java无缝地互操作<br>总而言之，Scala是一种函数式面向对象语言，它融汇了许多前所未有的特性，而同时又运行于JVM之上。</p>
<h2 id="3-2Spark-Shell"><a href="#3-2Spark-Shell" class="headerlink" title="3.2Spark Shell"></a>3.2Spark Shell</h2><p>该命令用于以交互式方式编写并执行Spark App，且书写语法为Scala。<br>下面的示例命令用于进入交互式执行器，进入执行器后，即可使用Scala语句以交互式方式编写并执行Spark-App。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master spark]# bin/spark-shell --master spark://master:7077</span><br></pre></td></tr></table></figure>

<p>在该示例中，写明“–master spark:&#x2F;&#x2F;master:7077”的目的是使Spark Shell进入集群模式，若不写明，则Spark Shell会默认进入单机模式。<br>由于Spark使用Scala开发，而Scala实际上在JVM中执行，因此，我们搭建好Spark环境后，无需另外安装Scala组件。</p>
</blockquote>
<h3 id="4-1-启动Spark-Shell"><a href="#4-1-启动Spark-Shell" class="headerlink" title="4.1 启动Spark Shell"></a>4.1 启动Spark Shell</h3><p>登录master服务器，在集群模式下启动Spark Shell</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/spark/</span><br><span class="line"># bin/spark-shell --master spark://master:7077</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676358827022.png" alt="img"></p>
<h3 id="4-2编写并执行Scala代码"><a href="#4-2编写并执行Scala代码" class="headerlink" title="4.2编写并执行Scala代码"></a>4.2编写并执行Scala代码</h3><p>在Spark Shell执行器中编写如下scala代码并运行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rawRDDA = sc.parallelize(List(&#x27;!!bb##cc&#x27;, &#x27;%%ccbb%%&#x27;, &#x27;cc&amp;&amp;++aa&#x27;), 3)</span><br><span class="line">scala&gt; val rawRDDB = sc.parallelize(List((&#x27;xx&#x27;, 99), (&#x27;yy&#x27;, 88), (&#x27;xx&#x27;, 99), (&#x27;zz&#x27;, 99)), 2)</span><br><span class="line">scala&gt; val rawRDDC = sc.parallelize(List((&#x27;yy&#x27;,88)), 1)</span><br><span class="line">scala&gt; import org.apache.spark.HashPartitioner</span><br><span class="line">scala&gt; var tempResultRDDA = rawRDDA.flatMap(line=&gt;line.split(&#x27;&#x27;)</span><br><span class="line">                    ).filter(allWord=&gt;&#123;allWord.contains(&#x27;aa&#x27;) || allWord.contains(&#x27;bb&#x27;)&#125;</span><br><span class="line">                    ).map(word=&gt;(word, 1)</span><br><span class="line">                    ).partitionBy(new HashPartitioner(2)</span><br><span class="line">                    ).groupByKey(</span><br><span class="line">                    ).map((P:(String, Iterable[Int]))=&gt;(P._1, P._2.sum))</span><br><span class="line">scala&gt; var tempResultRDDBC = rawRDDB.distinct.subtract(rawRDDC)</span><br><span class="line">scala&gt; var resultRDDABC = tempResultRDDA.union(tempResultRDDBC)</span><br><span class="line">scala&gt; resultRDDABC.saveAsTextFile(&#x27;hdfs://master:8020/user/spark/resultRDDABC&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676358843205.png" alt="img"></p>
<h3 id="4-3查看执行结果"><a href="#4-3查看执行结果" class="headerlink" title="4.3查看执行结果"></a>4.3查看执行结果</h3><p>执行器中执行下列命令，退出Spark Shell</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; exit</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676358878620.png" alt="img"></p>
<p>查看hdfs上结果文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop</span><br><span class="line"># bin/hadoop fs -cat /user/spark/resultRDDABC/p*</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676358870078.png" alt="img"></p>
<h2 id="27-Spark实验：Spark实现流量日志分析"><a href="#27-Spark实验：Spark实现流量日志分析" class="headerlink" title="27.Spark实验：Spark实现流量日志分析"></a>27.Spark实验：Spark实现流量日志分析</h2><blockquote>
<h3 id="目的-6"><a href="#目的-6" class="headerlink" title="目的"></a>目的</h3><p> 1 分析日志能帮助企业营销做出决策，本节课将介绍如何用 Spark 分析日志。</p>
<h3 id="要求-6"><a href="#要求-6" class="headerlink" title="要求"></a>要求</h3><p> 1 本次试验后，要求学生能学会如何用Spark实现流量日志分析。</p>
<h3 id="原理-6"><a href="#原理-6" class="headerlink" title="原理"></a>原理</h3><h4 id="3-1日志格式简介"><a href="#3-1日志格式简介" class="headerlink" title="3.1日志格式简介"></a>3.1日志格式简介</h4><p>日志在计算机系统中是一个非常广泛的概念，任何程序都有可能输出日志：操作系统内核、各种应用服务器等等。日志包含很多有用的信息，例如访问者的 IP、访问的时间、访问的目标网页、来源的地址以及访问者所使用的客户端的 UserAgent 信息等。<br>随着互联网的发展，产生了大量的 Web  日志，日志包含了用户最重要的信息，通过日志分析，用户可以获取到网站或应用的访问量，哪个网页访问人数最多，哪个网页有价值等。一般中型的网站(10 万 的 PV)，每天会产生 1GB 以上的 Web 日志文件，大型的网站，可能每天产生 500G 及以上的数据量。常见的日志格式主要有两类：一种是 Apache 的 NCSA 格式，另一种是 IIS 的 W3C 格式。对于日志的这种规模的数据，通过 Spark  进行分析与处理，能够达到很好的效果。</p>
<h4 id="3-2日志各字段说明"><a href="#3-2日志各字段说明" class="headerlink" title="3.2日志各字段说明"></a>3.2日志各字段说明</h4><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741338308.png" alt="img"><br>部分分析日志指标如下： PV（Page View）：网站页面访问数。 UV（Page View）：页面 IP 的访问量统计，即独立 IP。 PVPU（Page  View Per User）：平均每位用户访问页面数。 Time：用户每小时 PV 的统计。 Source：用户来源域名的统计。  Browser：用户的访问设备统计。 …..<br>如上图所示，数据字段太多，我们抽取黄色的字段，汇总如下。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741351095.png" alt="img"><br>过滤后的部分数据如下(选取其中一条仅作示例)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1363157986072     18320173382    84-25-DB-4F-10-1A:CMCC-EASY    120.196.100.99    input.shouji.sogou.com    搜索引擎    21    18    9531    2412    200</span><br></pre></td></tr></table></figure>

<p>继续抽取上图红色框里的数据，作为本节课实验数据，各字段之间用 \t 分隔符分开，最终数据如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">1493901764492    13326854564    16193    4743</span><br><span class="line">1493910704492    13426854534    73434    2809</span><br><span class="line">1493888804492    13626854565    81858    15477</span><br><span class="line">1493909204492    13726854561    30144    3965</span><br><span class="line">1493900564492    13926854564    79050    56108</span><br><span class="line">1493887604492    13826854534    96411    4914</span><br><span class="line">1493884124492    13726854569    37707    55218</span><br><span class="line">1493909504493    13226854545    98537    8529</span><br><span class="line">1493909744493    13526854569    44872    99132</span><br><span class="line">1493887364493    13626854545    88181    57680</span><br><span class="line">1493898884493    13526854555    37478    7935</span><br><span class="line">1493899784493    13532434342    27011    88622</span><br><span class="line">1493884064493    13776854556    69130    14111</span><br><span class="line">1493909444493    13426854544    9424    7944</span><br><span class="line">1493881124493    13726455555    60670    9896</span><br><span class="line">1493888144495    13026854539    90589    20091</span><br><span class="line">1493878244495    13245546774    10157    70072</span><br><span class="line">1493890424495    13685456946    54542    92281</span><br><span class="line">1493886644496    13656853434    78754    6029</span><br><span class="line">1493898584496    13026832435    28938    57137</span><br><span class="line">1493895164496    13026854539    13804    45769</span><br><span class="line">1493883644496    13026324344    64352    8799</span><br><span class="line">1493896064496    13368545391    96531    57707</span><br><span class="line">1493883464496    13026855696    50270    51114</span><br><span class="line">1493876624496    13435545456    42725    7474</span><br><span class="line">1493895764496    13434354354    1292    44650</span><br><span class="line">1493906624496    13324234355    36891    66919</span><br><span class="line">1493881244496    13554654765    33422    3859</span><br><span class="line">1493894684496    13734546757    18356    95838</span><br><span class="line">1493894564496    13335435466    40851    54767</span><br><span class="line">1493900864496    13432436546    52265    9739</span><br><span class="line">1493907224497    13943354354    83598    77744</span><br><span class="line">1493895044497    13677878785    1493    71046</span><br><span class="line">1493904044497    13826852343    8337    10934</span><br><span class="line">1493881724497    13026854539    60948    1681</span><br><span class="line">1493887964497    13626854564    52019    92273</span><br><span class="line">1493901164497    13026854534    84997    36689</span><br><span class="line">1493889164497    13232435435    99434    16073</span><br><span class="line">1493888504497    13534543546    15049    2980</span><br><span class="line">1493910284497    13434343444    37073    191</span><br><span class="line">1493891024497    13789654643    6593    93537</span><br><span class="line">1493880044497    13212432434    39299    49042</span><br><span class="line">1493902484497    13226854569    41811    42073</span><br><span class="line">1493890124498    13435435646    9565    69941</span><br><span class="line">1493905724498    14324354354    69444    72664</span><br><span class="line">1493899724498    13743543546    82207    11035</span><br><span class="line">1493904884498    13726854523    34207    32098</span><br><span class="line">1493885624498    13324325434    12683    2862</span><br><span class="line">1493909864498    13026854539    11500    9944</span><br><span class="line">1493893484498    13776854556    77444    5137</span><br></pre></td></tr></table></figure>

<p>其中第一列为 reportTime（报告时间戳）。 其中第二列为 phoneNum（手机号，上图中为 msisdn，这里修改为  phoneNum,方便识别）。 其中第三列为 upPayLoad（上行流量）。 其中第四列为 downPayLoad（下行流量）。<br>需求描述：对文本中数据记录进行排序，排序规则如下： 以 phoneNum 为基准，分别按照 upPayLoad，downPayLoad，reportTime 进行降序排序，即先按照  upPayLoad 排序，如果 upPayLoad 相同，再比较 downPayLoad，如果 downPayLoad 相同，再比较  reportTime，最后选择前 6 条记录输出。<br>核心编程思想描述:  所谓二次排序就是排序的时候，考虑两个维度。有一个纬度相同的时候，考虑另外一个维度。首先，将需要将待排序的字段封装成一个类，实现  Serializable 接口，并实现接口中的方法。同时为待排序的属性字段提供 getter 、setter 、hashcode 以及  equals 方法。在 application 应用程序中 将 key 封装为之前我们定义好的对象，之后调用 sortedByKey()  方法进行排序。<br>具体实现步骤: 第一步：按照 Serrializable 接口实现自定义排序的 Key  第二步：将要进行二次排序的文件加载进来生成 key-value 类型的 RDD 第三步：使用 sortByKey 基于自定义的 Key  进行二次排序 第四步：去除掉排序的 key，只保留排序结果</p>
</blockquote>
<h3 id="4-1-准备数据"><a href="#4-1-准备数据" class="headerlink" title="4.1 准备数据"></a>4.1 准备数据</h3><p>实验所需的数据在root&#x2F;data&#x2F;spark&#x2F;LogAnalysis&#x2F;access_20170504.log</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># more access_20170504.log</span><br><span class="line">查看一下数据</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741364615.png" alt="img"><br>将数据文件通过WinSCP下载至本地。</p>
<h3 id="4-2-创建-maven-项目"><a href="#4-2-创建-maven-项目" class="headerlink" title="4.2 创建 maven 项目"></a>4.2 创建 maven 项目</h3><p>打开IDEA，创建maven项目 New Project -&gt; Maven -&gt; “quickstart” -&gt;Next<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677742163079.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677742170079.png" alt="img"><br>按照图示输入 “Group Id”， “Artifact Id”， “ Package” -&gt; Finish 。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677742178019.png" alt="img"><br>修改pom.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span><br><span class="line">  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;</span><br><span class="line"></span><br><span class="line">  &lt;groupId&gt;cn.cstor&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;LogAnalysis&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">  &lt;name&gt;LogAnalysis&lt;/name&gt;</span><br><span class="line">  &lt;!-- FIXME change it to the project&#x27;s website --&gt;</span><br><span class="line">  &lt;url&gt;http://www.example.com&lt;/url&gt;</span><br><span class="line"></span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">  &lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;junit&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;junit&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;3.8.1&lt;/version&gt;</span><br><span class="line">      &lt;scope&gt;test&lt;/scope&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.5.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.5.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.5.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.5.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;2.6.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.5.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;spark-streaming-flume_2.10&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;1.5.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;httpclient&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;4.4.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">      &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;httpcore&lt;/artifactId&gt;</span><br><span class="line">      &lt;version&gt;4.4.1&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line">  &lt;/dependencies&gt;</span><br><span class="line"></span><br><span class="line">  &lt;build&gt;</span><br><span class="line">    &lt;sourceDirectory&gt;src/main/java&lt;/sourceDirectory&gt;</span><br><span class="line">    &lt;testSourceDirectory&gt;src/main/&lt;/testSourceDirectory&gt;</span><br><span class="line"></span><br><span class="line">    &lt;plugins&gt;</span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;descriptorRefs&gt;</span><br><span class="line">            &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;</span><br><span class="line">          &lt;/descriptorRefs&gt;</span><br><span class="line">          &lt;archive&gt;</span><br><span class="line">            &lt;manifest&gt;</span><br><span class="line">              &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">            &lt;/manifest&gt;</span><br><span class="line">          &lt;/archive&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">        &lt;executions&gt;</span><br><span class="line">          &lt;execution&gt;</span><br><span class="line">            &lt;id&gt;make-assembly&lt;/id&gt;</span><br><span class="line">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">              &lt;goal&gt;single&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">          &lt;/execution&gt;</span><br><span class="line">        &lt;/executions&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;1.2.1&lt;/version&gt;</span><br><span class="line">        &lt;executions&gt;</span><br><span class="line">          &lt;execution&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">              &lt;goal&gt;exec&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">          &lt;/execution&gt;</span><br><span class="line">        &lt;/executions&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;executable&gt;java&lt;/executable&gt;</span><br><span class="line">          &lt;includeProjectDependencies&gt;true&lt;/includeProjectDependencies&gt;</span><br><span class="line">          &lt;includePluginDependencies&gt;false&lt;/includePluginDependencies&gt;</span><br><span class="line">          &lt;classpathScope&gt;compile&lt;/classpathScope&gt;</span><br><span class="line">          &lt;mainClass&gt;&lt;/mainClass&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">      &lt;plugin&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;</span><br><span class="line">        &lt;configuration&gt;</span><br><span class="line">          &lt;source&gt;1.6&lt;/source&gt;</span><br><span class="line">          &lt;target&gt;1.6&lt;/target&gt;</span><br><span class="line">        &lt;/configuration&gt;</span><br><span class="line">      &lt;/plugin&gt;</span><br><span class="line"></span><br><span class="line">    &lt;/plugins&gt;</span><br><span class="line">  &lt;/build&gt;</span><br><span class="line">&lt;/project&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677742192878.png" alt="img"><br>右击pom.xml -&gt; Maven -&gt; Reload project<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677742199829.png" alt="img"></p>
<p>注意：文件下载的依赖项较多，需要一定的时间，请耐心等待完成。</p>
<h3 id="4-3-编写代码"><a href="#4-3-编写代码" class="headerlink" title="4.3 编写代码"></a>4.3 编写代码</h3><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677742213136.png" alt="img"><br>右击 “cn.cstor”-&gt; New -&gt; java class -&gt; Next。<br>输入类名 “SecondarySortApp” -&gt; 回车。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677742220224.png" alt="img"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.cstor;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaSparkContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SecondarySortApp</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">//创建 sparkcontext 对象，sparkcontext 是程序的唯一入口</span></span><br><span class="line"></span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>()</span><br><span class="line">                .setAppName(<span class="string">&quot;SecondarySortApp&quot;</span>)</span><br><span class="line">                .setMaster(<span class="string">&quot;local&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line">        <span class="comment">//去掉不必要的输出信息</span></span><br><span class="line">        sc.setLogLevel(<span class="string">&quot;WARN&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调用 textFile() 方法，读取日志文件，这里指定本地磁盘文件，您也可以指定是 HDFS 上的文件</span></span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; tf = sc.textFile(</span><br><span class="line">                <span class="string">&quot;G:\\root/access_20170504.log&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 调用 mapTfRDD2Pair 方法 将 tf 映射为键值对</span></span><br><span class="line">        JavaPairRDD&lt;String, cmbInfo&gt; tfPairRDD =</span><br><span class="line">                mapTfRDD2Pair(tf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取每个手机号的总上行流量、总下行流量、最早报告时间戳</span></span><br><span class="line">        JavaPairRDD&lt;String, cmbInfo&gt; agTfPairRDD =</span><br><span class="line">                aggregateByDeviceID(tfPairRDD);</span><br><span class="line">        <span class="comment">//聚合，封装的 RDD 作为 key,手机号作为值</span></span><br><span class="line">        JavaPairRDD&lt;SecondarySortKey, String&gt; tfSortRDD =</span><br><span class="line">                mapRDDKey2SortKey(agTfPairRDD);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 依次按照上行流量、下行流量以及报告时间戳倒序排序</span></span><br><span class="line">        JavaPairRDD&lt;SecondarySortKey ,String&gt; sortedTfRDD =</span><br><span class="line">                tfSortRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line">        <span class="comment">// 根据您的需要获得输出，这里仅显示前 8 行</span></span><br><span class="line">        List&lt;Tuple2&lt;SecondarySortKey, String&gt;&gt; dtTop =</span><br><span class="line">                sortedTfRDD.take(<span class="number">8</span>);</span><br><span class="line">        System.out.println(<span class="string">&quot;============ result ============&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span>(Tuple2&lt;SecondarySortKey, String&gt; dt : dtTop) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;phoneNum &quot;</span>+ dt._2 + <span class="string">&quot;, &quot;</span> + dt._1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 记住执行结束关闭资源</span></span><br><span class="line">        sc.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//mapTfRDD2Pair 方法，封装键值对</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> JavaPairRDD&lt;String, cmbInfo&gt; <span class="title function_">mapTfRDD2Pair</span><span class="params">(</span></span><br><span class="line"><span class="params">            JavaRDD&lt;String&gt; accessLogRDD)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> accessLogRDD.mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;String, String, cmbInfo&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, cmbInfo&gt; <span class="title function_">call</span><span class="params">(String lines)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// 根据数据格式进行切分</span></span><br><span class="line">                String[] split = lines.split(<span class="string">&quot;\t&quot;</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 获取切分的字段</span></span><br><span class="line">                <span class="type">long</span> <span class="variable">reportTime</span> <span class="operator">=</span> Long.valueOf(split[<span class="number">0</span>]);</span><br><span class="line">                <span class="type">String</span> <span class="variable">phoneNum</span> <span class="operator">=</span> split[<span class="number">1</span>];</span><br><span class="line">                <span class="type">long</span> <span class="variable">upPayLoad</span> <span class="operator">=</span> Long.valueOf(split[<span class="number">2</span>]);</span><br><span class="line">                <span class="type">long</span> <span class="variable">downPayload</span> <span class="operator">=</span> Long.valueOf(split[<span class="number">3</span>]);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 创建 cmbInfo 对象，有参构造 ，将上行流量、下行流量，报告时间戳封装为自定义的可序列化对象</span></span><br><span class="line">                <span class="type">cmbInfo</span> <span class="variable">dataInfo</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">cmbInfo</span>(reportTime,</span><br><span class="line">                        upPayLoad, downPayload);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, cmbInfo&gt;(phoneNum, dataInfo);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//根据手机号进行聚合，依次按照上行流量、下行流量以及报告时间戳倒序排序</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> JavaPairRDD&lt;String, cmbInfo&gt; <span class="title function_">aggregateByDeviceID</span><span class="params">(</span></span><br><span class="line"><span class="params">            JavaPairRDD&lt;String, cmbInfo&gt; accessLogPairRDD)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> accessLogPairRDD.reduceByKey(<span class="keyword">new</span> <span class="title class_">Function2</span>&lt;cmbInfo, cmbInfo, cmbInfo&gt;() &#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> cmbInfo <span class="title function_">call</span><span class="params">(cmbInfo d1, cmbInfo d2)</span></span><br><span class="line">                    <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="type">long</span> <span class="variable">reportTime</span> <span class="operator">=</span> d1.getReportTime()&lt; d2.getReportTime() ?</span><br><span class="line">                        d1.getReportTime() : d2.getReportTime();</span><br><span class="line">                <span class="type">long</span> <span class="variable">upPayLoad</span> <span class="operator">=</span> d1.getUpPayLoad() + d2.getUpPayLoad() ;</span><br><span class="line">                <span class="type">long</span> <span class="variable">downPayload</span> <span class="operator">=</span> d1.getDownPayload() + d2.getDownPayload();</span><br><span class="line"></span><br><span class="line">                <span class="type">cmbInfo</span> <span class="variable">accessLogInfo</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">cmbInfo</span>();</span><br><span class="line">                accessLogInfo.setReportTime(reportTime);</span><br><span class="line">                accessLogInfo.setUpPayLoad(upPayLoad);</span><br><span class="line">                accessLogInfo.setDownPayload(downPayload);</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> accessLogInfo;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//二次排序，手机号作为值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> JavaPairRDD&lt;SecondarySortKey, String&gt; <span class="title function_">mapRDDKey2SortKey</span><span class="params">(</span></span><br><span class="line"><span class="params">            JavaPairRDD&lt;String, cmbInfo&gt; aggrAccessLogPairRDD)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> aggrAccessLogPairRDD.mapToPair(</span><br><span class="line"></span><br><span class="line">                <span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String,cmbInfo&gt;, SecondarySortKey, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line"></span><br><span class="line">                    <span class="meta">@Override</span></span><br><span class="line">                    <span class="keyword">public</span> Tuple2&lt;SecondarySortKey, String&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                            Tuple2&lt;String, cmbInfo&gt; tuple)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="comment">// 获取元祖里的数据</span></span><br><span class="line">                        <span class="type">String</span> <span class="variable">phoneNum</span> <span class="operator">=</span> tuple._1;</span><br><span class="line">                        <span class="type">cmbInfo</span> <span class="variable">accessLogInfo</span> <span class="operator">=</span> tuple._2;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 封装为二次排序 key</span></span><br><span class="line">                        <span class="type">SecondarySortKey</span> <span class="variable">accessLogSortKey</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SecondarySortKey</span>(</span><br><span class="line">                                accessLogInfo.getUpPayLoad(),</span><br><span class="line">                                accessLogInfo.getDownPayload(),</span><br><span class="line">                                accessLogInfo.getReportTime());</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 返回新的 Tuple</span></span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;SecondarySortKey, String&gt;(accessLogSortKey, phoneNum);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意：数据文件路径为刚才下载的路径<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677742237493.png" alt="img"><br>选中 cn.cstor 包，同理继续创建 SecondarySortKey.java，代码如下：<br>SecondarySortKey.java 代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.cstor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> scala.math.Ordered;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">cmbInfo</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">5749943279909593929L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> reportTime;        <span class="comment">// 时间戳</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upPayLoad;        <span class="comment">// 上行流量</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downPayload;    <span class="comment">// 下行流量</span></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">cmbInfo</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>();</span><br><span class="line">        <span class="comment">// TODO Auto-generated constructor stub</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">cmbInfo</span><span class="params">(<span class="type">long</span> reportTime, <span class="type">long</span> upPayLoad, <span class="type">long</span> downPayload)</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>();</span><br><span class="line">        <span class="built_in">this</span>.reportTime = reportTime;</span><br><span class="line">        <span class="built_in">this</span>.upPayLoad = upPayLoad;</span><br><span class="line">        <span class="built_in">this</span>.downPayload = downPayload;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getReportTime</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> reportTime;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setReportTime</span><span class="params">(<span class="type">long</span> reportTime)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.reportTime = reportTime;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getUpPayLoad</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upPayLoad;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setUpPayLoad</span><span class="params">(<span class="type">long</span> upPayLoad)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upPayLoad = upPayLoad;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getDownPayload</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> downPayload;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDownPayload</span><span class="params">(<span class="type">long</span> downPayload)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.downPayload = downPayload;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">long</span> <span class="title function_">getSerialversionuid</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> serialVersionUID;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SecondarySortKey</span> <span class="keyword">implements</span> <span class="title class_">Ordered</span>&lt;SecondarySortKey&gt;, Serializable &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">3702442700882342403L</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> upPayLoad;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> downPayLoad;</span><br><span class="line">    <span class="keyword">private</span> <span class="type">long</span> reportTime;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">SecondarySortKey</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>();</span><br><span class="line">        <span class="comment">// TODO Auto-generated constructor stub</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">SecondarySortKey</span><span class="params">(<span class="type">long</span> upPayLoad, <span class="type">long</span> downPayLoad, <span class="type">long</span> reportTime)</span> &#123;</span><br><span class="line">        <span class="built_in">super</span>();</span><br><span class="line">        <span class="built_in">this</span>.upPayLoad = upPayLoad;</span><br><span class="line">        <span class="built_in">this</span>.downPayLoad = downPayLoad;</span><br><span class="line">        <span class="built_in">this</span>.reportTime = reportTime;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getUpPayLoad</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> upPayLoad;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setUpPayLoad</span><span class="params">(<span class="type">long</span> upPayLoad)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.upPayLoad = upPayLoad;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getDownPayLoad</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> downPayLoad;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setDownPayLoad</span><span class="params">(<span class="type">long</span> downPayLoad)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.downPayLoad = downPayLoad;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getReportTime</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> reportTime;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">setReportTime</span><span class="params">(<span class="type">long</span> reportTime)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.reportTime = reportTime;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">long</span> <span class="title function_">getSerialversionuid</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> serialVersionUID;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> $greater(SecondarySortKey other) &#123;</span><br><span class="line">        <span class="keyword">if</span>(upPayLoad &gt; other.upPayLoad) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(upPayLoad == other.upPayLoad &amp;&amp;</span><br><span class="line">                downPayLoad &gt; other.downPayLoad) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(upPayLoad == other.upPayLoad &amp;&amp;</span><br><span class="line">                downPayLoad == other.downPayLoad &amp;&amp;</span><br><span class="line">                        reportTime &gt; other.reportTime) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> $greater$eq(SecondarySortKey other) &#123;</span><br><span class="line">        <span class="keyword">if</span>($greater(other)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(upPayLoad == other.upPayLoad &amp;&amp;</span><br><span class="line">                downPayLoad == other.downPayLoad &amp;&amp;</span><br><span class="line">                        reportTime == other.reportTime) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> $less(SecondarySortKey other) &#123;</span><br><span class="line">        <span class="keyword">if</span>(upPayLoad &lt; other.upPayLoad) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(upPayLoad == other.upPayLoad &amp;&amp;</span><br><span class="line">                downPayLoad &lt; other.downPayLoad) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(upPayLoad == other.upPayLoad &amp;&amp;</span><br><span class="line">                downPayLoad == other.downPayLoad &amp;&amp;</span><br><span class="line">                        reportTime &lt; other.reportTime) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> $less$eq(SecondarySortKey other) &#123;</span><br><span class="line">        <span class="keyword">if</span>($less(other)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(upPayLoad == other.upPayLoad &amp;&amp;</span><br><span class="line">                downPayLoad == other.downPayLoad &amp;&amp;</span><br><span class="line">                        reportTime == other.reportTime) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compare</span><span class="params">(SecondarySortKey other)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(upPayLoad - other.upPayLoad != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="type">int</span>) (upPayLoad - other.upPayLoad);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(downPayLoad - other.downPayLoad != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="type">int</span>) (downPayLoad - other.downPayLoad);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(reportTime - other.reportTime != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="type">int</span>) (reportTime - other.reportTime);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">compareTo</span><span class="params">(SecondarySortKey other)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span>(upPayLoad - other.upPayLoad != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="type">int</span>) (upPayLoad - other.upPayLoad);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(downPayLoad - other.downPayLoad != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="type">int</span>) (downPayLoad - other.downPayLoad);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(reportTime - other.reportTime != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="type">int</span>) (reportTime - other.reportTime);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">hashCode</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">final</span> <span class="type">int</span> <span class="variable">prime</span> <span class="operator">=</span> <span class="number">31</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">result</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">        result = prime * result + (<span class="type">int</span>) (downPayLoad );</span><br><span class="line">        result = prime * result + (<span class="type">int</span>) (reportTime );</span><br><span class="line">        result = prime * result + (<span class="type">int</span>) (upPayLoad );</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">boolean</span> <span class="title function_">equals</span><span class="params">(Object obj)</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">this</span> == obj)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">if</span> (obj == <span class="literal">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (getClass() != obj.getClass())</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="type">SecondarySortKey</span> <span class="variable">other</span> <span class="operator">=</span> (SecondarySortKey) obj;</span><br><span class="line">        <span class="keyword">if</span> (downPayLoad != other.downPayLoad)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (reportTime != other.reportTime)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">if</span> (upPayLoad != other.upPayLoad)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">toString</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;SortResult： [上行流量： &quot;</span> + upPayLoad + <span class="string">&quot;, 下行流量：  &quot;</span> + downPayLoad + <span class="string">&quot;, 报告时间戳：&quot;</span></span><br><span class="line">                + reportTime + <span class="string">&quot;]&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-4-运行代码"><a href="#4-4-运行代码" class="headerlink" title="4.4 运行代码"></a>4.4 运行代码</h3><p>右键点击 SecondarySortApp -&gt; Run ‘SecondarySortApp.main()’<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677744820290.png" alt="img"><br>观察 console 控制台输出结果：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677744829367.png" alt="img"></p>
<h2 id="28-Spark实验：Spark实现黑名单实时过滤"><a href="#28-Spark实验：Spark实现黑名单实时过滤" class="headerlink" title="28.Spark实验：Spark实现黑名单实时过滤"></a>28.Spark实验：Spark实现黑名单实时过滤</h2><blockquote>
<h3 id="目的-7"><a href="#目的-7" class="headerlink" title="目的"></a>目的</h3><p> 1 本节课主要讲解 Spark 的 RDD 操作，让您对 Spark 算子的特性快速了解。通过演示案例实时黑名单过滤，让您切身体会到 RDD 的强大功能，然后学以致用。</p>
<h3 id="要求-7"><a href="#要求-7" class="headerlink" title="要求"></a>要求</h3><p> 1 实验结束后，要求学生能学会使用nc命令、学会使用SparkStreaming实时计算、学会使用Spark RDD。</p>
<h3 id="原理-7"><a href="#原理-7" class="headerlink" title="原理"></a>原理</h3><h4 id="3-1弹性分布式数据集-RDD"><a href="#3-1弹性分布式数据集-RDD" class="headerlink" title="3.1弹性分布式数据集(RDD)"></a>3.1弹性分布式数据集(RDD)</h4><p>（1）提到 Spark Transformation，不得不说 Spark RDD，那么 RDD 是什么?<br>弹性分布式数据集(RDD)是 Spark 框架中的核心概念。它是由数据组成的不可变分布式集合，其主要进行两个操作：transformation 和 action。Spark 将数据存储在不同分区上的 RDD 之中，RDD 可以帮助重新安排计算并优化数据处理过程，此外，它还具有容错性，因为 RDD  知道如何重新创建和重新计算数据集。Transformation 是类似在 RDD上 做 filter()、map()或 union()  以生成另一个 RDD 的操作，而 action 则是 count()、first()、take(n)、collect()  等触发一个计算操作。Transformations 一般都是 lazy 的，直到 action 执行后才会被执行。Spark  Master&#x2F;Driver 会保存 RDD 上的 Transformations。这样一来，如果某个 RDD  丢失，它可以快速和便捷地转换到集群中存活的主机上。这也就是 RDD 的弹性所在。<br>（2）RDD 支持两种类型的操作：<br>变换（Transformation）变换的返回值是一个新的 RDD 集合，而不是单个值。调用一个变换方法，不会有任何求值计算，它只获取一个 RDD 作为参数，然后返回一个新的 RDD。  变换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，pipe 和  coalesce。<br>行动（Action）行动操作计算并返回一个新的值。当在一个 RDD  对象上调用行动函数时，会在这一时刻计算全部的数据处理查询并返回结果值。  行动操作包括：reduce，collect，count，first，take，countByKey 以及 foreach。</p>
<h4 id="3-2-transformation-操作"><a href="#3-2-transformation-操作" class="headerlink" title="3.2 transformation 操作"></a>3.2 transformation 操作</h4><p>map(func): 对调用 map 的 RDD 数据集中的每个 element 都使用 func，然后返回一个新的 RDD，这个返回的数据集是分布式的数据集。<br>filter(func): 对调用 filter 的 RDD 数据集中的每个元素都使用 func，然后返回一个包含使 func 为 true 的元素构成的 RDD。<br>flatMap(func): 和 map 差不多，但是 flatMap 生成的是多个结果。<br>mapPartitions(func): 和 map 很像，但是 map 是每个 element，而 mapPartitions 是每个 partition。<br>mapPartitionsWithSplit(func): 和 mapPartitions 很像，但是 func 作用的是其中一个 split 上，所以 func 中应该有 index。<br>sample(withReplacement,faction,seed): 抽样。<br>union(otherDataset)： 返回一个新的 dataset，包含源 dataset 和给定 dataset 的元素的集合。<br>distinct([numTasks]): 返回一个新的 dataset，这个 dataset 含有的是源 dataset 中的 distinct 的 element。<br>join(otherDataset,[numTasks]): 当有两个 KV 的 dataset(K,V)和(K,W)，返回的是(K,(V,W))的 dataset,numTasks 为并发的任务数。<br>cogroup(otherDataset,[numTasks]): 当有两个 KV 的 dataset(K,V)和(K,W)，返回的是(K,Seq[V],Seq[W])的 dataset，numTasks 为并发的任务数。<br>cartesian(otherDataset)： 笛卡尔积简单说就是 m*n。<br>groupByKey(numTasks): 返回(K,Seq[V])，也就是 hadoop 中 reduce 函数接受的 key-valuelist。<br>reduceByKey(func,[numTasks]): 就是用一个给定的 reduce func再作用在groupByKey产生的(K,Seq[V]),比如求和，求平均数。<br>sortByKey([ascending],[numTasks]): 按照 key 来进行排序，是升序还是降序，ascending 是 boolean 类型。<br>leftOuterJoin: leftOuterJoin 类似于 SQL 中的左外关联 left outer join，返回结果以前面的 RDD 为主，关联不上的记录为空。只能用于两个 RDD 之间的关联，如果要多个 RDD 关联，多关联几次即可。</p>
<h4 id="3-3-action-操作"><a href="#3-3-action-操作" class="headerlink" title="3.3 action 操作"></a>3.3 action 操作</h4><p>count(): 返回的是 dataset 中的 element 的个数。<br>first(): 返回的是 dataset 中的第一个元素。<br>take(n): 返回前 n 个 elements，这个是driver program 返回的。<br>takeSample(withReplacement，num，seed)： 抽样返回一个 dataset 中的 num 个元素，随机种子 seed。<br>reduce(func)： 说白了就是聚集，但是传入的函数是两个参数输入返回一个值，这个函数必须是满足交换律和结合律的。<br>collect()： 一般在 filter 或者足够小的结果的时候，再用 collect 封装返回一个数组。<br>saveAsTextFile（path）： 把 dataset 写到一个 text file 中，或者 hdfs，或者 hdfs 支持的文件系统中，spark 把每条记录都转换为一行记录，然后写到 file 中。<br>saveAsSequenceFile(path): 只能用在 key-value 对上，然后生成 SequenceFile 写到本地或者 hadoop 文件系统。<br>countByKey()： 返回的是 key 对应的个数的一个 map，作用于一个 RDD。<br>foreach(func): 对 dataset 中的每个元素都使用 func。</p>
</blockquote>
<p>在一些企业中，比如小额贷款公司，需要做好风控，对那些信誉不好的用户，我们需要设置黑名单，只要是黑名单中的用户，我们就给过滤掉，禁止提供贷款服务。既然是实时，必然用到 SparkStreaming，这里采用 socketTextStream，结合 nc  命令使用。由于是实验，不方面提供真实数据，这里一再简化，可以简单模拟一份数据，进行测试，原理是相同的。</p>
<h4 id="4-1监听端口"><a href="#4-1监听端口" class="headerlink" title="4.1监听端口"></a>4.1监听端口</h4><p>首先，我们连接我们终端，然后，使用 nc 启动一个监听端口9999。<br>由于我们要使用 socketTextStream传入一个端口及对应的主机名，不先启动端口会报错。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># nc -l -p 9999</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741000652.png" alt="img"></p>
<h4 id="4-2启动spark-shell"><a href="#4-2启动spark-shell" class="headerlink" title="4.2启动spark-shell"></a>4.2启动spark-shell</h4><p>我们再连接一个终端，启动 spark-shell。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark-shell</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741010557.png" alt="img"><br>使用 import 导入依赖。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkConf</span><br><span class="line">import org.apache.spark.streaming.StreamingContext</span><br><span class="line">import org.apache.spark.streaming.Seconds</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741019481.png" alt="img"><br>注意:如果是 spark-shell，那么在进入 spark-shell 的时候，spark-shell 自动创建了一个 SparkContext  为sc，那么创建 StreamingContext 只需要用 sc 来 new 就可以了，这样就不会出现多个sc 的冲突问题，否则会报错。<br>创建StreamingContext，设置每一秒刷新一次。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val ssc = new StreamingContext(sc, Seconds(2))</span><br></pre></td></tr></table></figure>

<p>设置需要过滤的黑名单，这里设置两个名字，您也可设置多个。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val bl = Array((&#x27;Jim&#x27;, true),(&#x27;hack&#x27;, true))</span><br></pre></td></tr></table></figure>

<p>设置并行度，这里指定为3。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val blRdd = ssc.sparkContext.parallelize(bl, 3)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741035861.png" alt="img"><br>设置主机名，端口号。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val st = ssc.socketTextStream(&#x27;localhost&#x27;, 9999)</span><br></pre></td></tr></table></figure>

<p>对输入数据进行转换，(id, user) &#x3D;&gt; (user, id user) ,以便对每个批次RDD，与之前定义好的黑名单进行leftOuterJoin操作。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val users = st.map &#123; l =&gt; (l.split(&#x27; &#x27;)(1),l) &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741045516.png" alt="img"></p>
<h4 id="4-3调用左外连接"><a href="#4-3调用左外连接" class="headerlink" title="4.3调用左外连接"></a>4.3调用左外连接</h4><p>调用左外连接操作leftOuterJoin，进行黑名单匹配，过滤掉。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">val validRddDS = users.transform(ld =&gt; &#123;</span><br><span class="line">      val ljoinRdd = ld.leftOuterJoin(blRdd)</span><br><span class="line">      val fRdd = ljoinRdd.filter(tuple =&gt; &#123;</span><br><span class="line">        if(tuple._2._2.getOrElse(false)) &#123;  </span><br><span class="line">          false</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          true</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;)</span><br><span class="line">      val validRdd = fRdd.map(tuple =&gt; tuple._2._1) </span><br><span class="line">      validRdd</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741054300.png" alt="img"></p>
<h4 id="4-4打印白名单"><a href="#4-4打印白名单" class="headerlink" title="4.4打印白名单"></a>4.4打印白名单</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#打印白名单</span><br><span class="line">validRddDS.print()</span><br><span class="line">#执行    </span><br><span class="line">ssc.start()</span><br><span class="line">#等待完成</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>此刻，你会看到每隔一秒不断刷新。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741062902.png" alt="img"><br>接下来，打开的之前的终端，即监听 9999 端口。 输入以下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0001 marry</span><br><span class="line">0003 hack</span><br><span class="line">0002 tom</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741073997.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741082687.png" alt="img"><br>同理继续输入一下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">0004 Jim</span><br><span class="line">0005 John</span><br><span class="line">0006 shiro</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741093690.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677741099929.png" alt="img"><br>经过两次的输入，发现对黑名单里的数据都过滤掉，实验完毕。由于设置的刷新间隔太短您可能，看得不是很清楚，可以将间隔设置大一点，方便观察。</p>
<h2 id="29-Spark实验：Spark流式计算商品关注度"><a href="#29-Spark实验：Spark流式计算商品关注度" class="headerlink" title="29.Spark实验：Spark流式计算商品关注度"></a>29.Spark实验：Spark流式计算商品关注度</h2><blockquote>
<h3 id="目的-8"><a href="#目的-8" class="headerlink" title="目的"></a>目的</h3><p> 1 使用 Scoket 来模拟用户浏览商品产生实时数据，数据包括用户当前浏览的商品以及浏览商品的次数和停留时间和是否收藏该商品。<br>2 使用 Spark Streaming 构建实时数据处理系统，来计算当前电商平台最受人们关注的商品是哪些。</p>
<h3 id="要求-8"><a href="#要求-8" class="headerlink" title="要求"></a>要求</h3><p> 1本次试验后，要求学生能掌握Java Scoket 编程的基本原理、Spark Streaming 应用的基本实现、DStream 的基本操作、updateStateByKey 的使用</p>
<h3 id="原理-8"><a href="#原理-8" class="headerlink" title="原理"></a>原理</h3><p> 处于网络时代的我们，随着 O2O  的营销模式的流行，越来越多的人开始做起了电商。与此同时也产生了许多网络数据，然而这些数据有什么用呢。比如说一个电商公司可以根据一个商品被用户点击了多少次，用户停留时间是多久，用户是否收藏了该商品。这些都是可以被记录下来的。通过这些数据我们就能分析出这段时间内哪些商品最受普遍人们的关注。同时也可以针对这些数据进行用户商品推荐。<br>通过搜集用户浏览信息，通过网络传输，实时处理用户浏览数据。进行必要的处理，经过计算得到商品的关注度。现在网络上很多人都是使用得 Scala 语言进行 Spark 得实现。因为它书写简洁，快速。但是好多学校并没有开设这一个课程。好多都是学习得 Java 知识，但是网上有关 Java 方式得实现少之又少。这次的项目我就是使用 Java 得方式进行 Spark Streaming 的实现。<br>实验流程图：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427695311.png" alt="img"><br>我们先来了解一些关于 Streaming 的基础知识。<br>（1）什么是 Streaming<br>Spark Streaming 它实现了对实时流数据的高吞吐量，低容错率的流处理。数据可以有许多来源，如 Kafka、Flume、TCP  套接字，可以使用一些逻辑算法来处理分析这些数据，并将这些数据持久化到数据库。比如说 Hbase、Spark SQL、HDFS 等等。如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427701708.png" alt="img"><br>其内部工作原理如下图所示，<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427708573.png" alt="img"><br>(2) 关于 DStream 的操作<br>关于 DStream 的了解均来自于 Spark 官方网站。有什么不足，还请多多谅解。<br>DStream 是 Spark Streaming  提供的基本抽象。它代表了一个连续的数据流，或者从源端接收到的，或通过将输入流中产生处理后的数据流。在内部，它是由 RDDS 的连续序列表示。在  DStream 中每个 RDD 包含数据从某一个时间间隔，如下图。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427716335.png" alt="img"><br>说白了他其实还是 RDD。和 Spark RDD 的操作是差不多的，在 DStream 上面的任何操作都会转化为底层的 RDDS 操作。如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427724998.png" alt="img"><br>有关 DStream 的算子，在这里就不一一罗列了。大家可以参考官网上的说明。<br>链接地址：<a target="_blank" rel="noopener" href="http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html">http://spark.apache.org/docs/1.6.1/streaming-programming-guide.html</a></p>
</blockquote>
<h3 id="4-1项目结构图："><a href="#4-1项目结构图：" class="headerlink" title="4.1项目结构图："></a>4.1项目结构图：</h3><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427739697.png" alt="img"></p>
<h3 id="4-2Java-工程创建"><a href="#4-2Java-工程创建" class="headerlink" title="4.2Java 工程创建"></a>4.2Java 工程创建</h3><p>等待 Eclipse 启动成功后，依次点击左上角工具栏的 File -&gt; New -&gt; Project：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427745839.png" alt="img"><br>选择 Java ，然后选择 Next：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427752635.png" alt="img"><br>在 Project name 中填写 StreamingProject，然后选择 Finish：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427760105.png" alt="img"><br>到此我们就创建好了一个成功的名为 StreamingProject 的 Java 工程。<br>目录结构为：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427766813.png" alt="img"></p>
<h3 id="4-3Spark-assembly-工具包的引入"><a href="#4-3Spark-assembly-工具包的引入" class="headerlink" title="4.3Spark-assembly 工具包的引入"></a>4.3Spark-assembly 工具包的引入</h3><p>现在下载需要用到的jars，我们选择资料工具 -&gt; 软件下载 -&gt; Spark -&gt; 2.2.2<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427779271.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427785111.png" alt="img"><br>单击File -&gt; Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427790954.png" alt="img"><br>选择 Libraries -&gt; ➕ -&gt;java<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427796801.png" alt="img"><br>选择 &#x2F;spark-2.2.2-bin-hadoop2.7&#x2F;jars 路径下的所有包：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427802890.png" alt="img"><br>这里就可以看到我们导入的jar包<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427808825.png" alt="img"><br>右键 src，选择 New -&gt; Package：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676427817431.png" alt="img"><br>在 Java Package 中的 Name 下填写 cn.cstor.simulator，然后回车<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429772014.png" alt="img"><br>右键 cn.cstor.simulator，选择 New -&gt;Java Class：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429778428.png" alt="img"><br>在 Java Class 中的 Name 下填写 SimulatorSocket，然后回车：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429785073.png" alt="img"><br>按照同样的方式创建 StreamingGoods.java 文件，最后的效果如下所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429791541.png" alt="img"></p>
<h3 id="4-4代码实现"><a href="#4-4代码实现" class="headerlink" title="4.4代码实现"></a>4.4代码实现</h3><p>SimulatorSocket.java的实现比较简单，使用了简单的 Java Socket 知识，以及线程来控制它发送消息的频率。StreamingGoods.java实现了 Streaming 程序。通过接收 socket 数据，对数据进行拆分，计算。</p>
<h4 id="4-4-1-StreamingGoods-java-类的具体实现"><a href="#4-4-1-StreamingGoods-java-类的具体实现" class="headerlink" title="4.4.1 StreamingGoods.java 类的具体实现"></a>4.4.1 StreamingGoods.java 类的具体实现</h4><p>（1）初始化 StreamingContext<br>首先使用 Java 初始化 Streaming 程序，需要先定义一个 JavaStreamingContext，它是所有的 Java Streaming 程序的切入点。<br>实现一个 JavaStreamingContext 需要先定义一个 SparkConf。实现代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SparkConf sparkConf = new SparkConf().setAppName(&#x27;StreamingGoods&#x27;).setMaster(&#x27;local[2]&#x27;);</span><br><span class="line">// AppName 自然就是当前 Job 的名字，Master 就是 Spark 的主机地址，这里采用的是本地模式</span><br><span class="line">JavaStreamingContext jsc = new JavaStreamingContext(sparkConf,new Duration(5000));</span><br><span class="line">// new Duration(5000) 窗口时间，单位毫秒</span><br></pre></td></tr></table></figure>

<p>（2）获取模拟数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JavaReceiverInputDStream&lt;String&gt; jds = jsc.socketTextStream(&#x27;127.0.0.1&#x27;, 9999);</span><br></pre></td></tr></table></figure>

<p>（3）消息处理：<br>商品关注度怎么计算呢，这个可能需要一个约定，就是说浏览次数和浏览时间以及购买力度和是否收藏该商品都有一个权重，可能不同的公司觉得不同的选项权重不一样，可能你觉得浏览时间更重要，另一人觉得浏览次数更重要，所以我们事先约定好这个计算公式。我们约定浏览次数的权重为 0.8，浏览时间权重为 0.6，是否收藏和购买力度都为 1。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// mapToPair 就是将 rdd 转换为键值对 rdd，与 map 不同的是添加了一个 key</span><br><span class="line">JavaPairDStream&lt;String, Double&gt; splitMess = jds.mapToPair(new PairFunction&lt;String,String,Double&gt;()&#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            public Tuple2&lt;String, Double&gt; call(String line) throws Exception &#123;</span><br><span class="line">                // TODO Auto-generated method stub.</span><br><span class="line">                String[] lineSplit = line.toString().split(&#x27;::&#x27;);</span><br><span class="line">                Double followValue = Double.parseDouble(lineSplit[1])*0.8+Double.parseDouble(lineSplit[2])*0.6+Double.parseDouble(lineSplit[3])*1+Double.parseDouble(lineSplit[4])*1;</span><br><span class="line">                return new Tuple2&lt;String, Double&gt;(lineSplit[0], followValue);</span><br><span class="line">            &#125;&#125;);</span><br></pre></td></tr></table></figure>

<p>（4）更新关注度值：<br>由于是流式数据，数据每分每秒都在产生，那么计算的关注值也在变化，那么就需要更新这个状态值。使用 updateStateByKey 来进行操作。这也是这里相对比较难的知识点。<br>对初始化的 DStream 进行 Transformation 级别的处理，例如 map、filter  等高阶函数等的编程，来进行具体的数据计算，在这里是通过 updateStateByKey 来以 Batch Interval  为单位来对历史状态进行更新，在这里需要使用 checkPoint，用于保存父 RDD 的值。在 Spark1.6.X 之后也可以尝试使用  mapWithState 来进行更新值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">JavaPairDStream&lt;String, Double&gt; UpdateFollowValue = splitMess.updateStateByKey(new Function2&lt;List&lt;Double&gt;,Optional&lt;Double&gt;,Optional&lt;Double&gt;&gt;()&#123;</span><br><span class="line"></span><br><span class="line">            public Optional&lt;Double&gt; call(List&lt;Double&gt; newValues,</span><br><span class="line">                    Optional&lt;Double&gt; statValue) throws Exception &#123;</span><br><span class="line">                // 对相同的 key 进行 value 统计，实现累加</span><br><span class="line">                Double updateValue = statValue.or(0.0);</span><br><span class="line">                for (Double values : newValues) &#123;</span><br><span class="line">                    updateValue += values;</span><br><span class="line">                &#125;</span><br><span class="line">                return Optional.of(updateValue);</span><br><span class="line">            &#125;&#125;,new HashPartitioner(jsc.sparkContext().defaultParallelism()));</span><br></pre></td></tr></table></figure>

<p>（5）输出关注度值：<br>结果输出，并将里面的商品进行关注度排序，降序排序，只显示关注度最高的十个商品。实现思想，由于原 RDD  UpdateFollowValue 的值可以知道是 &lt;String,Double&gt; 的形式，我们使用 sortByKey  是不能这样进行排序的，因为它并不是按照关注度排序。我们需要将其转化为 &lt;Double,String&gt; 的形式，然后再按照  sortByKey 来进行排序，然后进行输出。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">UpdateFollowValue.foreachRDD(new VoidFunction&lt;JavaPairRDD&lt;String,Double&gt;&gt;()&#123;</span><br><span class="line">            private static final long serialVersionUID = 1L;</span><br><span class="line">            public void call(JavaPairRDD&lt;String, Double&gt; followValue) throws Exception &#123;</span><br><span class="line">                // TODO Auto-generated method stub</span><br><span class="line">                JavaPairRDD&lt;Double,String&gt; followValueSort = followValue.mapToPair(new PairFunction&lt;Tuple2&lt;String,Double&gt;,Double,String&gt;()&#123;</span><br><span class="line"></span><br><span class="line">                    public Tuple2&lt;Double, String&gt; call(</span><br><span class="line">                            Tuple2&lt;String, Double&gt; valueToKey) throws Exception &#123;</span><br><span class="line">                        // TODO Auto-generated method stub</span><br><span class="line">                        return new Tuple2&lt;Double,String&gt;(valueToKey._2,valueToKey._1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).sortByKey(false);</span><br><span class="line">                List&lt;Tuple2&lt;String,Double&gt;&gt; list = followValueSort.mapToPair(new PairFunction&lt;Tuple2&lt;Double,String&gt;,String, Double&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    public Tuple2&lt;String, Double&gt; call(</span><br><span class="line">                            Tuple2&lt;Double, String&gt; arg0) throws Exception &#123;</span><br><span class="line">                        // TODO Auto-generated method stub</span><br><span class="line">                        return new Tuple2&lt;String,Double&gt;(arg0._2,arg0._1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).take(10);</span><br><span class="line">                for (Tuple2&lt;String,Double&gt; tu : list) &#123;</span><br><span class="line">                    System.out.println(&#x27;商品ID: &#x27;+tu._1+&#x27;  关注度: &#x27;+tu._2);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;&#125;);</span><br></pre></td></tr></table></figure>

<h3 id="4-5-完整代码"><a href="#4-5-完整代码" class="headerlink" title="4.5 完整代码"></a>4.5 完整代码</h3><p>SimulatorSocket.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.cstor.simulator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.OutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.PrintWriter;</span><br><span class="line"><span class="keyword">import</span> java.net.ServerSocket;</span><br><span class="line"><span class="keyword">import</span> java.net.Socket;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">SimulatorSocket</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="comment">// 创建一个线程来启动模拟器</span></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">Thread</span>(<span class="keyword">new</span> <span class="title class_">SimulatorSocketLog</span>()).start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimulatorSocketLog</span> <span class="keyword">implements</span> <span class="title class_">Runnable</span>&#123;</span><br><span class="line">    <span class="comment">// 假设一共有 200 个商品</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">GOODSID</span> <span class="operator">=</span> <span class="number">200</span>;</span><br><span class="line">    <span class="comment">// 随机发送消息的条数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">MSG_NUM</span> <span class="operator">=</span> <span class="number">30</span>;</span><br><span class="line">    <span class="comment">// 假设用户浏览该商品的次数</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">BROWSE_NUM</span> <span class="operator">=</span> <span class="number">5</span>;</span><br><span class="line">    <span class="comment">// 假设用户浏览商品停留的时间</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span> <span class="variable">STAY_TIME</span> <span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    <span class="comment">// 用来体现用户是否收藏，收藏为 1，不收藏为 0，差评为 -1</span></span><br><span class="line">    <span class="type">int</span>[] COLLECTION = <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;-<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>&#125;;</span><br><span class="line">    <span class="comment">// 用来模拟用户购买商品的件数，0 比较多是为了增加没有买的概率，毕竟不买的还是很多的，很多用户都只是看看</span></span><br><span class="line">    <span class="keyword">private</span> <span class="type">int</span>[] BUY_NUM = <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">        <span class="type">Random</span> <span class="variable">r</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             *创建一个服务器端，监听9999端口，客户端就是 Streaming，通过看源码才知道，Streaming *socketTextStream 其实就是相当于一个客户端</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="type">ServerSocket</span> <span class="variable">sScoket</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ServerSocket</span>(<span class="number">9999</span>);</span><br><span class="line">            System.out.println(<span class="string">&#x27;成功开启数据模拟模块，去运行Streaming程序吧！&#x27;</span>);</span><br><span class="line">            <span class="keyword">while</span>(<span class="literal">true</span>)&#123;</span><br><span class="line">                <span class="comment">// 随机消息数</span></span><br><span class="line">                <span class="type">int</span> <span class="variable">msgNum</span> <span class="operator">=</span> r.nextInt(MSG_NUM)+<span class="number">1</span>;</span><br><span class="line">                <span class="comment">// 开始监听</span></span><br><span class="line">                <span class="type">Socket</span> <span class="variable">socket</span> <span class="operator">=</span> sScoket.accept();</span><br><span class="line">                <span class="comment">// 创建输出流</span></span><br><span class="line">                <span class="type">OutputStream</span> <span class="variable">os</span> <span class="operator">=</span> socket.getOutputStream();</span><br><span class="line">                <span class="comment">// 包装输出流</span></span><br><span class="line">                <span class="type">PrintWriter</span> <span class="variable">pw</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">PrintWriter</span>(os);</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; msgNum; i++) &#123;</span><br><span class="line">                    <span class="comment">// 消息格式：商品 ID::浏览次数::停留时间::是否收藏::购买件数</span></span><br><span class="line">                    <span class="type">StringBuffer</span> <span class="variable">sb</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringBuffer</span>();</span><br><span class="line">                    sb.append(<span class="string">&#x27;goodsID-&#x27;</span>+(r.nextInt(GOODSID)+<span class="number">1</span>));</span><br><span class="line">                    sb.append(<span class="string">&#x27;::&#x27;</span>);</span><br><span class="line">                    sb.append(r.nextInt(BROWSE_NUM)+<span class="number">1</span>);</span><br><span class="line">                    sb.append(<span class="string">&#x27;::&#x27;</span>);</span><br><span class="line">                    sb.append(r.nextInt(STAY_TIME)+r.nextFloat());</span><br><span class="line">                    sb.append(<span class="string">&#x27;::&#x27;</span>);</span><br><span class="line">                    sb.append(COLLECTION[r.nextInt(<span class="number">2</span>)]);</span><br><span class="line">                    sb.append(<span class="string">&#x27;::&#x27;</span>);</span><br><span class="line">                    sb.append(BUY_NUM[r.nextInt(<span class="number">9</span>)]);</span><br><span class="line">                    System.out.println(sb.toString());</span><br><span class="line">                    <span class="comment">// 发送消息</span></span><br><span class="line">                    pw.write(sb.toString()+<span class="string">&#x27;\n&#x27;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">                pw.flush();</span><br><span class="line">                pw.close();</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">                &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                    <span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">                    System.out.println(<span class="string">&#x27;thread sleep failed&#x27;</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            <span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">            System.out.println(<span class="string">&#x27;port used&#x27;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>StreamingGoods.java</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.cstor.simulator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.HashPartitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.SparkConf;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.JavaPairRDD;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.Function2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.PairFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.function.VoidFunction;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.Duration;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaPairDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaReceiverInputDStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.api.java.JavaStreamingContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.Tuple2;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.api.java.Optional;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">StreamingGoods</span> <span class="keyword">implements</span> <span class="title class_">Serializable</span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">    <span class="comment">// 定义一个文件夹，用于保存上一个 RDD 的数据。该文件夹会自动创建，不需要提前创建</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">String</span> <span class="variable">checkpointDir</span> <span class="operator">=</span> <span class="string">&#x27;checkDir&#x27;</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException&#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">sparkConf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setAppName(<span class="string">&#x27;StreamingGoods&#x27;</span>).setMaster(<span class="string">&#x27;local[2]&#x27;</span>);</span><br><span class="line">        <span class="type">JavaStreamingContext</span> <span class="variable">jsc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaStreamingContext</span>(sparkConf,<span class="keyword">new</span> <span class="title class_">Duration</span>(<span class="number">5000</span>));</span><br><span class="line">        jsc.checkpoint(checkpointDir);</span><br><span class="line">        JavaReceiverInputDStream&lt;String&gt; jds = jsc.socketTextStream(<span class="string">&#x27;127.0.0.1&#x27;</span>, <span class="number">9999</span>);</span><br><span class="line">        JavaDStream&lt;String&gt; mess = jds.map(<span class="keyword">new</span> <span class="title class_">Function</span>&lt;String,String&gt;()&#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="keyword">public</span> String <span class="title function_">call</span><span class="params">(String arg0)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">                <span class="keyword">return</span> arg0;</span><br><span class="line">            &#125;&#125;);</span><br><span class="line">        mess.print();</span><br><span class="line">        JavaPairDStream&lt;String, Double&gt; splitMess = jds.mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;String,String,Double&gt;()&#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Double&gt; <span class="title function_">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// TODO Auto-generated method stub.</span></span><br><span class="line">                String[] lineSplit = line.toString().split(<span class="string">&#x27;::&#x27;</span>);</span><br><span class="line">                <span class="type">Double</span> <span class="variable">followValue</span> <span class="operator">=</span> Double.parseDouble(lineSplit[<span class="number">1</span>])*<span class="number">0.8</span>+Double.parseDouble(lineSplit[<span class="number">2</span>])*<span class="number">0.6</span>+Double.parseDouble(lineSplit[<span class="number">3</span>])*<span class="number">1</span>+Double.parseDouble(lineSplit[<span class="number">4</span>])*<span class="number">1</span>;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String, Double&gt;(lineSplit[<span class="number">0</span>], followValue);</span><br><span class="line">            &#125;&#125;);</span><br><span class="line">        JavaPairDStream&lt;String, Double&gt; UpdateFollowValue = splitMess.updateStateByKey(<span class="keyword">new</span> <span class="title class_">Function2</span>&lt;List&lt;Double&gt;,Optional&lt;Double&gt;,Optional&lt;Double&gt;&gt;()&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">public</span> Optional&lt;Double&gt; <span class="title function_">call</span><span class="params">(List&lt;Double&gt; newValues,</span></span><br><span class="line"><span class="params">                                         Optional&lt;Double&gt; statValue)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">                <span class="type">Double</span> <span class="variable">updateValue</span> <span class="operator">=</span> statValue.or(<span class="number">0.0</span>);</span><br><span class="line">                <span class="keyword">for</span> (Double values : newValues) &#123;</span><br><span class="line">                    updateValue += values;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">return</span> Optional.of(updateValue);</span><br><span class="line">            &#125;&#125;,<span class="keyword">new</span> <span class="title class_">HashPartitioner</span>(jsc.sparkContext().defaultParallelism()));</span><br><span class="line">        UpdateFollowValue.foreachRDD(<span class="keyword">new</span> <span class="title class_">VoidFunction</span>&lt;JavaPairRDD&lt;String,Double&gt;&gt;()&#123;</span><br><span class="line">            <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">serialVersionUID</span> <span class="operator">=</span> <span class="number">1L</span>;</span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">(JavaPairRDD&lt;String, Double&gt; followValue)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">                JavaPairRDD&lt;Double,String&gt; followValueSort = followValue.mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String,Double&gt;,Double,String&gt;()&#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">public</span> Tuple2&lt;Double, String&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                            Tuple2&lt;String, Double&gt; valueToKey)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;Double,String&gt;(valueToKey._2,valueToKey._1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).sortByKey(<span class="literal">false</span>);</span><br><span class="line">                List&lt;Tuple2&lt;String,Double&gt;&gt; list = followValueSort.mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Double,String&gt;,String, Double&gt;() &#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">public</span> Tuple2&lt;String, Double&gt; <span class="title function_">call</span><span class="params">(</span></span><br><span class="line"><span class="params">                            Tuple2&lt;Double, String&gt; arg0)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                        <span class="comment">// TODO Auto-generated method stub</span></span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;String,Double&gt;(arg0._2,arg0._1);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;).take(<span class="number">10</span>);</span><br><span class="line">                <span class="keyword">for</span> (Tuple2&lt;String,Double&gt; tu : list) &#123;</span><br><span class="line">                    System.out.println(<span class="string">&#x27;商品ID: &#x27;</span>+tu._1+<span class="string">&#x27;  关注度: &#x27;</span>+tu._2);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;&#125;);</span><br><span class="line"></span><br><span class="line">        jsc.start();</span><br><span class="line">        jsc.awaitTermination();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-6程序运行说明"><a href="#4-6程序运行说明" class="headerlink" title="4.6程序运行说明"></a>4.6程序运行说明</h3><p>（1）先启动模拟数据。相当于服务端。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429820468.png" alt="img"><br>成功运行结果：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429826974.png" alt="img"><br>（2）现在用同样的方式去运行 Streaming 程序。<br>开始发送数据：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429832696.png" alt="img"><br>Streaming 接收到的数据：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429838563.png" alt="img"><br>Streaming 计算结果：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429844073.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429850071.png" alt="img"><br>（3）我们刷新工程，看是否创建好 checkDir：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429856536.png" alt="img"><br>从下图我们可以看到，已经自动创建了 checkDir 这个文件夹：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676429863886.png" alt="img"></p>
<h2 id="30-Spark实验：决策树预测森林植被"><a href="#30-Spark实验：决策树预测森林植被" class="headerlink" title="30.Spark实验：决策树预测森林植被"></a>30.Spark实验：决策树预测森林植被</h2><blockquote>
<h3 id="目的-9"><a href="#目的-9" class="headerlink" title="目的"></a>目的</h3><p> 1 了解决策树算法。<br>2 在决策树算法基础上运用数据集。</p>
<h3 id="要求-9"><a href="#要求-9" class="headerlink" title="要求"></a>要求</h3><p> 1要求实验结束后，可以掌握决策树算法的应用。</p>
<h3 id="原理-9"><a href="#原理-9" class="headerlink" title="原理"></a>原理</h3><h4 id="3-1-决策树算法"><a href="#3-1-决策树算法" class="headerlink" title="3.1 决策树算法"></a>3.1 决策树算法</h4><p>决策树算法就是通过对已有明确结果的历史数据进行分析，寻找数据中的特征，并以此为依据对新产生的数据结果进行预测。决策树主要由下面三部分组成:<br>（1）决策节点：每个决策节点表示一个待分类的数据类别或属性，其中最顶部的是根决策节点。<br>（2）分支：每一个分支都有一个新的决策节点。<br>（3）叶子节点：每个叶子节点表示一种结果。<br>本节课主要介绍 k-means 算法，并介绍一个简单案例。下面介绍贴近生活的一个决策树例子，方便大家理解。<br>小明最近由于用钱紧张，又想购买 新款iPhone 手机，通过某种渠道发现某公司可以办理分期购，于是联系上这家公司。这家公司员工需要对小明的情况进行进一步审核。下面这张图大致可以表示该员工的审核过程。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745980208.png" alt="img"><br>这张图看似是一颗二叉树，只不过没有具体的量化值，就像 Java 基本语法的选择判断语句 if else：如果 A，那么 B，否则 C。<br>说它“基本可以算”是因为图中的判定条件没有量化，如收入高中低等等，还不能算是严格意义上的决策树，如果将所有条件量化，则就变成真正的决策树了。<br>下面给出决策树的定义：<br>决策树是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。</p>
<h4 id="3-2-数据集介绍"><a href="#3-2-数据集介绍" class="headerlink" title="3.2 数据集介绍"></a>3.2 数据集介绍</h4><p>Covtype 数据集介绍页面：<br><a target="_blank" rel="noopener" href="http://archive.ics.uci.edu/ml/datasets/Covertype">http://archive.ics.uci.edu/ml/datasets/Covertype</a><br>在介绍页面点击 Data Set Description 可以查看对于数据集的字段解释，如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745988786.png" alt="img"><br>对其简要解释如下：</p>
<table>
<thead>
<tr>
<th>字段名称</th>
<th>数据类型</th>
<th>量度单位</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Elevation</td>
<td>定量数据</td>
<td>米</td>
<td>海拔高度</td>
</tr>
<tr>
<td>Aspect</td>
<td>定量数据</td>
<td>度</td>
<td>方位角</td>
</tr>
<tr>
<td>Slope</td>
<td>定量数据</td>
<td>度</td>
<td>坡度</td>
</tr>
<tr>
<td>Horizontal_Distance_To_Hydrology</td>
<td>定量数据</td>
<td>米</td>
<td>与最近水文特征的水平距离</td>
</tr>
<tr>
<td>Vertical_Distance_To_Hydrology</td>
<td>定量数据</td>
<td>米</td>
<td>与最近水文特征的垂直距离</td>
</tr>
<tr>
<td>Horizontal_Distance_To_Roadways</td>
<td>定量数据</td>
<td>米</td>
<td>与最近道路的水平距离</td>
</tr>
<tr>
<td>Hillshade_9am</td>
<td>定量数据</td>
<td>0 至 255 的索引</td>
<td>早上 9:00 光的投射度（夏至）</td>
</tr>
<tr>
<td>Hillshade_Noon</td>
<td>定量数据</td>
<td>0 至 255 的索引</td>
<td>正午光的投射度（夏至）</td>
</tr>
<tr>
<td>Hillshade_3pm</td>
<td>定量数据</td>
<td>0 至 255 的索引</td>
<td>下午 3:00 光的投射度（夏至）</td>
</tr>
<tr>
<td>Horizontal_Distance_To_Fire_Points</td>
<td>定量数据</td>
<td>米</td>
<td>与最近野火燃起点的距离</td>
</tr>
<tr>
<td>Wilderness_Area （4 个二元列）</td>
<td>定性数据</td>
<td>0 或 1（缺失&#x2F;存在）</td>
<td>荒野地区等级</td>
</tr>
<tr>
<td>Soil_Type （40 个二元列）</td>
<td>定性数据</td>
<td>0 或 1（缺失&#x2F;存在）</td>
<td>土壤类型等级</td>
</tr>
<tr>
<td>Cover_Type （7 种）</td>
<td>整数</td>
<td>0 至 7</td>
<td>森林覆盖类型等级</td>
</tr>
<tr>
<td>数据集中共有 12 个特征，由 54 列数据组成。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>covtype.data 记录了美国科罗拉多州不同地块森林植被特征：海拔、坡度、到水源的距离、遮阳情况和土壤类型，并且随同给出了地块的已知森林植被类型，有 581012 个样本。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Spark MLib 将特征向量抽象为 LabeledPoint，它由一个包含对个特征值的 Spark MLib Vector  和一个称为标号（label）的目标值组成。该目标为 Double 类型，而 Vector 本质上是对多个 Double 类型值的抽象。这说明  LabeledPoint 只适用于数值型特征。但只要经过适当编码，LabeledPoint 也可用于类别型特征。</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>其中一种编码是 one-hot 或 1-of-n 编码。在这种编码中，一个有 N 个不同取值的类别型特征可以变成 N 个数值型特征，变换后的每个数值型特征的取值为 0 或 1。在这 N 个特征中，有且只有一个取值为 1，其他特征取值都为 0。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h4 id="3-3-实验流程图"><a href="#3-3-实验流程图" class="headerlink" title="3.3 实验流程图"></a>3.3 实验流程图</h4><p>本实验的整体流程如下图所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746002052.png" alt="img"></p>
<h4 id="3-4-算法详解"><a href="#3-4-算法详解" class="headerlink" title="3.4 算法详解"></a>3.4 算法详解</h4><p>决策树最重要的是决策树的构造。<br>所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。构造决策树的关键步骤是分裂属性。所谓分裂属性就是在某个节点处按照某一特征属性的不同划分构造不同的分支，其目标是让各个分裂子集尽可能地“纯”。决策树帮助我们把复杂的数据表示转换成相对简单的直观的结构。<br>决策树学习算法主要由三部分构成：<br>（1）决策树生成<br>（2）特征选择<br>（3）决策树的剪枝<br>这里主要讲解决策树生成。<br>决策树的生成算法有很多变形，这里介绍几种经典的实现算法：ID3 算法、C4.5 算法和 CART 算法。这些算法的主要区别在于分类结点上特征选择的选取标准不同。下面详细了解一下算法的具体实现过程。<br>ID3 算法（Iterative Dichotomiser 3 迭代二叉树 3 代）是一个由 Ross Quinlan 发明的用于决策树的算法。  这个算法是建立在奥卡姆剃刀的基础上：越是小型的决策树越优于大的决策树（简单理论）。尽管如此，该算法也不是总是生成最小的树形结构，而是一个启发式算法。奥卡姆剃刀阐述了一个信息熵的概念：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746009782.png" alt="img"><br>这个 ID3 算法可以归纳为以下几点：<br>（1）使用所有没有使用的属性并计算与之相关的样本熵值。<br>（2）选取其中熵值最小的属性。<br>（3）生成包含该属性的节点。<br>C4.5 算法是由 Ross Quinlan 开发的用于产生决策树的算法。该算法是对 Ross Quinlan 之前开发的 ID3 算法的一个扩展。C4.5 算法产生的决策树可以被用作分类目的，因此该算法也可以用于统计分类。<br>相对于其他数据挖掘算法，决策树拥有的优势：<br>（1）易于理解和实现。<br>（2）人们在通过解释后都有能力去理解决策树所表达的意义。<br>（3）在相对短的时间内能够对大型数据源做出可行且效果良好的结果。<br>（4）对于决策树，数据的准备往往是简单或者是不必要的。<br>（5)能够同时处理数据型和常规型属性。</p>
</blockquote>
<h3 id="4-1-数据准备"><a href="#4-1-数据准备" class="headerlink" title="4.1 数据准备"></a>4.1 数据准备</h3><p>covtype.data数据路径：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/data/spark/vegetation/covtype.data</span><br></pre></td></tr></table></figure>

<p>使用 head 命令显示前 10 行内容。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># head covtype.data</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746022967.png" alt="img"><br>使用 wc -l 命令显示行数，可以看到有 581012 行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># wc -l covtype.data</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746030886.png" alt="img"></p>
<h3 id="4-2-启动-Spark-Shell"><a href="#4-2-启动-Spark-Shell" class="headerlink" title="4.2 启动 Spark Shell"></a>4.2 启动 Spark Shell</h3><p>请在终端中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark-shell</span><br></pre></td></tr></table></figure>

<p>Spark Shell 启动耗时较长，请耐心等候。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746039506.png" alt="img"><br>进入到 Spark 的 REPL 环境，相当于就是 Spark 的 Console。</p>
<h3 id="4-3-导入数据并转换数据"><a href="#4-3-导入数据并转换数据" class="headerlink" title="4.3 导入数据并转换数据"></a>4.3 导入数据并转换数据</h3><h4 id="4-3-1-导入数据"><a href="#4-3-1-导入数据" class="headerlink" title="4.3.1 导入数据"></a>4.3.1 导入数据</h4><p>拓展：Spark textFile 进行数据的导入，将外部数据导入到 Spark 中来，并将其转换成 RDD。<br>键入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val dataset = sc.textFile(&quot;file:///root/data/spark/vegetation/covtype.data&quot;)     #根据自己的文件路径位置</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746048741.png" alt="img"><br>从图中我们可以看到我们引入了 &#x2F;root&#x2F;data&#x2F;spark&#x2F;vegetation&#x2F;下面的 covtype.data 文件。在最后，我们也了解到了当前的数据格式为  RDD[String] 类型的数据。在我们可以看看当前 dataset 的数据样式。我们查看 dataset 的前十行数据。<br>拓展：这里使用到 RDD Action 操作，take(num) 函数，take 的作用就是获取 RDD 中下标 0 到 num-1 的元素。foreach 遍历，并打印。</p>
<p>键入命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.take(10).foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746055878.png" alt="img"><br>可以看到和我们之前用的 head 命令结果一致。</p>
<h4 id="4-3-2-数据转换"><a href="#4-3-2-数据转换" class="headerlink" title="4.3.2 数据转换"></a>4.3.2 数据转换</h4><p>使用 import 命令导入依赖：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.linalg._</span><br><span class="line">import org.apache.spark.mllib.regression._</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746064245.png" alt="img"><br>然后我们将这些数据以逗号分割开，拆分数据，将其转换 Vector 格式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val data = dataset.map &#123; line =&gt;</span><br><span class="line">        val values = line.split(&#x27;,&#x27;).map(_.toDouble)</span><br><span class="line">        val featureVector = Vectors.dense(values.init)</span><br><span class="line">        val label = values.last - 1</span><br><span class="line">        LabeledPoint(label, featureVector)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746071267.png" alt="img"><br>将数据分割为训练集（占 80% ）、交叉检验集（CV，占 10% ）和测试集（占 10% ），用 cache() 缓存常用的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val Array(trainData, cvData, testData) = data.randomSplit(Array(0.8, 0.1, 0.1))</span><br><span class="line">trainData.cache()</span><br><span class="line">cvData.cache()</span><br><span class="line">testData.cache()</span><br></pre></td></tr></table></figure>

<p>结果如下图：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746078207.png" alt="img"></p>
<h3 id="4-4-DecisionTreeModel-模型建立"><a href="#4-4-DecisionTreeModel-模型建立" class="headerlink" title="4.4 DecisionTreeModel 模型建立"></a>4.4 DecisionTreeModel 模型建立</h3><p>DecisionTree 实现也有几个超参数，我们需要为它们选择值。和之前一样，训练集和 CV  集用于给这些超参数选择一个合适值。这里第三个数据集，也就是测试集，用于对基于选定超参数的模型期望准确度做无偏估计。模型在交叉检验集上的准确度往往有点过于乐观，不是无偏差的。<br>接下来我们开始建立模型.</p>
<h4 id="4-4-1-引入-mllib-tree-包"><a href="#4-4-1-引入-mllib-tree-包" class="headerlink" title="4.4.1 引入 mllib.tree._ 包"></a>4.4.1 引入 mllib.tree._ 包</h4><p>键入命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.evaluation._</span><br><span class="line">import org.apache.spark.mllib.tree._</span><br><span class="line">import org.apache.spark.mllib.tree.model._</span><br><span class="line">import org.apache.spark.rdd._</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746087566.png" alt="img"><br>我们先来试试在训练集上构造一个 DecisionTreeModel 模型，参数采用默认值，并用 CV 集来计算结果模型的指标.<br>输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def getMetrics(model: DecisionTreeModel, data: RDD[LabeledPoint]):</span><br><span class="line"></span><br><span class="line">    MulticlassMetrics = &#123;</span><br><span class="line">        val predictionsAndLabels = data.map(example =&gt;</span><br><span class="line">        (model.predict(example.features), example.label)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    new MulticlassMetrics(predictionsAndLabels)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下图：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746094296.png" alt="img"></p>
<h4 id="4-4-2-创建-DecisionTreeModel-模型"><a href="#4-4-2-创建-DecisionTreeModel-模型" class="headerlink" title="4.4.2 创建 DecisionTreeModel 模型"></a>4.4.2 创建 DecisionTreeModel 模型</h4><p>输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val model = DecisionTree.trainClassifier( trainData, 7, Map[Int,Int](), &quot;gini&quot;, 4, 100)</span><br></pre></td></tr></table></figure>

<p>在调用 trainClassifier 方法时，除训练集以外，填入的参数分别是：<br>7：设定分类数目。<br>Map<a target="_blank" rel="noopener" href="http://10.131.2.101/#/course/laboratory?courseId=9beb8dc3ec64423898817968062fbf26&expId=dbcbeb3d9e5849169f74ae7260b26017&recordId=faffc39768104c3eb2810b0c6f60f756">Int, Int</a>：设定输入数据的格式。<br>‘gini’：设定信息增益计算方式，此处使用了 gini 不纯度。<br>4：设定树的高度（Depth）。<br>100：设定分裂数据集（Bin）数量。<br>创建模型后最后会有如下输出：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746101838.png" alt="img"><br><strong>注意</strong>：这里我们使用 trainClassifier，而不是 trainRegressor，trainClassifier 指示每个 LabeledPoint 里的目标应该当做不同的类别指标，而不是数值型特征。<br>接下来用 CV 集来计算结果模型的指标。<br>输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val metrics = getMetrics(model, cvData)</span><br></pre></td></tr></table></figure>

<p>结果如下：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746109181.png" alt="img"><br>查看混淆矩阵，计算过程需要一点时间，请耐心等待。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">metrics.confusionMatrix</span><br></pre></td></tr></table></figure>

<p>结果如下：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746116961.png" alt="img"><br><strong>注意</strong>：你得到的值可能稍有不同，构造决策树过程中的一些随机选项会导致分类结果稍有不同。<br>矩阵每一行对应一个实际的正确类别值，矩阵每一列按序对应预测值。<br>第 i 行第 j 列的元素代表一个正确类别为 i 的样本被预测为类别为 j 的次数。<br>因此，对角线上的元素代表预测正确的次数，而其他元素则代表预测错误的次数。对角线上的次数大多是好的，但也出现了一些分类错误的情况，比如上面结果中没将任何一个样本类别预测为 7。<br>查看预测准确度：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">metrics.precision</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746124221.png" alt="img"><br>每个类别相对其他类别的精确度：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(0 until 7).map(</span><br><span class="line">    p =&gt; (metrics.precision(p), metrics.recall(p))</span><br><span class="line">).foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746134419.png" alt="img"><br>可以看到每个类型准确度各有不同。</p>
<h4 id="4-4-3-增加参数再次测试准确度"><a href="#4-4-3-增加参数再次测试准确度" class="headerlink" title="4.4.3 增加参数再次测试准确度"></a>4.4.3 增加参数再次测试准确度</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def classPro(data: RDD[LabeledPoint]): Array[Double] = &#123;</span><br><span class="line">    val cb = data.map(_.label).countByValue()</span><br><span class="line"></span><br><span class="line">    // 排序</span><br><span class="line">    val random = cb.toArray.sortBy(_._1).map(_._2)</span><br><span class="line"></span><br><span class="line">    // 取样本数</span><br><span class="line">    random.map(_.toDouble / random.sum)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746142610.png" alt="img"><br>把训练集和 CV 集中的某个类别的概率结成对，相乘然后相加：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val trainPro = classPro(trainData)</span><br><span class="line"></span><br><span class="line">val cvPro = classPro(cvData)</span><br><span class="line"></span><br><span class="line">// 把训练集和 CV 集中的某个类别的概率结成对，相乘然后相加</span><br><span class="line">trainPro.zip(cvPro).map&#123;</span><br><span class="line">    case (trainProb, cvProb) =&gt; trainProb * cvProb</span><br><span class="line">&#125;.sum</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746221545.png" alt="img"></p>
<h4 id="4-4-4-准确度对比"><a href="#4-4-4-准确度对比" class="headerlink" title="4.4.4 准确度对比"></a>4.4.4 准确度对比</h4><p>项目实现节中的创建 DecisionTreeModel模型节的预测准确度远高于增加参数再次测试准确度节预测准确度，因为创建  DecisionTreeModel 模型节将数据分割为训练集（占 80%）、交叉检验集（CV，占 10%）和测试集（占  10%），而增加参数再次测试准确度节是随机的。如果感兴趣的话，您可以在创建  DecisionTreeModel模型节的基础上取不同的参数进行调优，或者调整数据切分比例，进行再次测试，准确度应该还会提高。</p>
<h4 id="4-4-5-对测试集进行预测"><a href="#4-4-5-对测试集进行预测" class="headerlink" title="4.4.5 对测试集进行预测"></a>4.4.5 对测试集进行预测</h4><p>检验分类模型的方式便是通过该模型对测试集进行预测，因为测试集中的样本并没有参与训练过程。<br>此处我们直接使用由创建  DecisionTreeModel  模型节部分调整得到的模型参数，但在输入训练集时，我们将交叉验证集也加入进来。你也可以重复增加参数再次测试准确度节的过程，通过迭代地更改信息增益计算方式、树的最大高度和分裂数据集数量等参数来获得更优的模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val model = DecisionTree.trainClassifier(trainData.union(cvData), 7, Map[Int,Int](), &quot;entropy&quot;, 4, 100)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746232749.png" alt="img"><br>最后利用测试集对该模型进行验证。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val metrics = getMetrics(model, testData)</span><br><span class="line">metrics.precision</span><br></pre></td></tr></table></figure>

<p>得到的预测准确率如下图所示（仅供参考）：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746240607.png" alt="img"><br><strong>注意</strong>：由于内存限制，此处的参数并不是最优值，故得到的预测准确率不是最高值，仅供参考过程。通过持续调优和足够的计算条件，该测试集的预测准确率可以达到 68%。</p>
<h2 id="31-Spark实验：出租车数据分析"><a href="#31-Spark实验：出租车数据分析" class="headerlink" title="31.Spark实验：出租车数据分析"></a>31.Spark实验：出租车数据分析</h2><blockquote>
<h3 id="目的-10"><a href="#目的-10" class="headerlink" title="目的"></a>目的</h3><p>出租车是我们生活中经常乘坐的一种交通工具，但打车难的问题也限制了我们更好地利用这种交通方式。在哪些地方出租车更容易打到？在什么时候更容易打到出租车？本课程将基于某市的出租车行驶轨迹数据，带你学习如何应用 Spark SQL 和机器学习相关技巧，并且通过数据可视化手段展现分析结果。</p>
<h3 id="要求-10"><a href="#要求-10" class="headerlink" title="要求"></a>要求</h3><p>1、本次试验后，要求学生能掌握Spark DataFrame 操作、Spark SQL 的 API 查询、<br>Spark MLlib 的 KMeans 算法应用</p>
<h3 id="原理-10"><a href="#原理-10" class="headerlink" title="原理"></a>原理</h3><h4 id="3-1-K-Means-聚类算法简介"><a href="#3-1-K-Means-聚类算法简介" class="headerlink" title="3.1 K-Means 聚类算法简介"></a>3.1 K-Means 聚类算法简介</h4><p>我们常说“物以类聚，人以群分”，将物理或抽象对象的集合分成由类似的对象组成的多个类的过程，则被称为聚类。<br>聚类的用途有很多，比如：<br>在商务上，聚类能帮助市场分析人员从客户基本库中发现不同的客户群，并且用购买模式来刻画不同的客户群的特征。<br>在生物学上，聚类能用于推导植物和动物的分类，对基因进行分类，获得对种群中固有结构的认识。<br>在房屋租售方面，可以根据房子的类型、价值和地理位置对一个城市中房屋的分组，从而更好地进行租售信息和价格水平的维护。<br>在保险业，可以通过识别可能的欺诈行为，找出一段时间内索赔支出很高的投保人，通过拒保等措施保护保险公司的正当权益。<br>而 K-Means 算法是一个迭代型的聚类算法。迭代是指需要一个或者多个往复的阶段才能完成，结束的条件是找到最优的簇。<br>算法的过程可以描述如下：<br>1、给定簇的数量 K 和一个数据集合（包含 N 个点，点可以是多维的）。在数据集合中，随机产生 K 个初始化的均值（本例中 K &#x3D; 3）作为质心。下图中被标记为彩色的点即为初始质心。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745074780.png" alt="img"><br>2、计算集合内各个点与这 K 个中心点的距离，并且将各个点分配到与它距离最近的质心。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745081570.png" alt="img"><br>3、每个 K 集群的均值成为新的质心。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745089065.png" alt="img"><br>当所有的点都被分配之后，再重新计算 K 个质心的位置，然后重复上述 2 、3 步骤，直到质心不再改变时，该算法结束。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745095894.png" alt="img"></p>
<h4 id="3-2-K-Means-算法实现"><a href="#3-2-K-Means-算法实现" class="headerlink" title="3.2 K-Means 算法实现"></a>3.2 K-Means 算法实现</h4><p>Spark 框架在 MLlib 库中为广大的使用者提供了一些常用的机器学习算法的实现。MLlib 中包含的机器学习算法主要有：<br>回归<br>二元分类<br>聚类<br>协同过滤<br>梯度下降优化<br>K-Means 就是聚类算法中的一种，它可以将数据点进行分组。在 Spark MLlib 中， 实现了 K-Means 算法的并行化版本。因此我们在 Spark 中进行此类问题的分析，能够获得相比于 R 语言、Matlab 等分析工具的更好的运算速度。<br>在 Spark 的 Github 页面 可以看到 K-Means 算法在 Spark 中的实现细节，如果你有兴趣的话不妨花点时间去阅读它的代码。代码中给出了较为详细的注释，阅读算法的实现细节能够帮助你更加深刻地理解其原理和各项参数的意义。</p>
<h4 id="3-3数据解析"><a href="#3-3数据解析" class="headerlink" title="3.3数据解析"></a>3.3数据解析</h4><p>本数据集为四川省成都市的出租车 GPS 记录数据集。该数据集已提前清洗完成，仅提取了原始数据集中某一天的部分数据，并且去除了时间段在 0  点至 6 点之间的较少数据。数据记录了成都市部分出租车在载客时的 GPS 位置和时间等信息，数据记录的格式为 CSV 格式。<br>已清洗的数据仅供本课程学习使用，有一定的模拟性质。如需要更多的信息，则需要从原始数据按照相应的目的进行清洗。<br>该数据集中的一条记录如下所示：<br>1,30.624806,104.136604,211846<br>对各个字段逐个解释如下：<br>TID：出租车的 ID。每辆出租车的 TID 都是唯一的。<br>Lat：出租车状态为载客时的纬度。<br>Lon：出租车状态为载客时的经度。<br>Time：该条记录的时间戳。如 211846 代表 21 点 18 分 46 秒。<br>CSV 格式是数据分析工作中常见的一种数据格式。CSV 意为逗号分隔值（Comma-Separated  Values），其文件以纯文本形式存储表格数据（数字和文本）。每行只有一条记录，每条记录被逗号分隔符分隔为字段，并且每条记录都有同样的字段序列。CSV 格式能被大多数应用程序所支持，广泛用于在不同的系统之间转移数据，是一种容易被兼容的格式。实验楼中大量的数据分析类课程都使用了 CSV  格式的数据集，不仅如此，我们也推荐你在今后的数据分析工作中应用此格式来存储数据。</p>
</blockquote>
<p>本次实验使用的数据文件在以下路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/data/spark/taxi</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745599446.png" alt="img"></p>
<h3 id="4-1启动-Spark-Shell"><a href="#4-1启动-Spark-Shell" class="headerlink" title="4.1启动 Spark Shell"></a>4.1启动 Spark Shell</h3><p>为了更好地处理 CSV 格式的数据集，我们可以直接使用由 DataBricks 公司提供的第三方 Spark CSV 解析库来读取。启动  Spark Shell。在启动的同时，附上参数–packages com.databricks:spark-csv_2.11:1.1.0<br>请在终端中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --packages com.databricks:spark-csv_2.11:1.1.0</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745608340.png" alt="img"><br><strong>注意</strong>：该操作需要联网权限。**如果遇到网络访问较慢，或者是您当前不具备访问互联网的权限时，请参考文末的常见问题“无法访问外网时，应如何通过加载 CSV 解析库的方式进入 Spark Shell”，问题解答中提供了解决方案。</p>
<h3 id="4-2-导入数据"><a href="#4-2-导入数据" class="headerlink" title="4.2 导入数据"></a>4.2 导入数据</h3><h4 id="4-2-1加载实验所需的包"><a href="#4-2-1加载实验所需的包" class="headerlink" title="4.2.1加载实验所需的包"></a>4.2.1加载实验所需的包</h4><p>首先我们需要加载本节实验所需要的包。这些包主要有：<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.VectorAssembler</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.clustering.KMeans</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745651911.png" alt="img"></p>
<h4 id="4-2-2-定义字段格式"><a href="#4-2-2-定义字段格式" class="headerlink" title="4.2.2 定义字段格式"></a>4.2.2 定义字段格式</h4><p>在其他大多数的数据分析类课程中，用到的 CSV  格式的数据集都在首行标记了各个字段的名称。但本次实验中用到的数据集却没有这个关键信息，如果我们直接创建 DataFrame  的话，就不能够很好地去定位到各个列。因此在导入数据之前，我们需要先定义数据的字段格式（Schema）。<br>在学习 Spark SQL 时，我们已经知道： Spark SQL 支持两种不同的方式来将现有的 RDD 转换为数据框（DataFrame）。第一个方法是使用反射机制（Relection），另一个则是通过编程的方式指明字段格式。<br>注：在 2.0 版本及之后的 Spark 中，提出了一个 DataSet 的概念来取代 DataFrame。请关注 Spark 官网提供的 API 文档和编程指南，以适应这个变化，敬请注意。<br>请在 Spark Shell 中输入以下代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 利用 StructType 定义字段格式，与数据集中各个字段一一映射。</span><br><span class="line">// StructField 中的的三个参数分别为字段名称、字段数据类型和是否不允许为空。</span><br><span class="line">val fieldSchema = StructType(Array(</span><br><span class="line">  StructField(&#x27;TID&#x27;, StringType, true),</span><br><span class="line">  StructField(&#x27;Lat&#x27;, DoubleType, true),</span><br><span class="line">  StructField(&#x27;Lon&#x27;, DoubleType, true),</span><br><span class="line">  StructField(&#x27;Time&#x27;, StringType, true)</span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745660761.png" alt="img"></p>
<h4 id="4-2-3-读取数据"><a href="#4-2-3-读取数据" class="headerlink" title="4.2.3 读取数据"></a>4.2.3 读取数据</h4><p>定义好字段格式之后，调用了 sqlContext 提供的 read 接口，指定加载格式 format 为第三方库中定义的格式  com.databricks.spark.csv 。因为本次课程使用的数据集中首行没有各列的字段名称，因此需要设置读取选项 header 为  false。最后，在 load 方法中 指明待读取的数据集文件的路径。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745689043.png" alt="img"><br>请在 Spark Shell 中输入以下代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line">val taxiDF = sqlContext.read.format(&#x27;com.databricks.spark.csv&#x27;).option(&#x27;header&#x27;, &#x27;false&#x27;).schema(fieldSchema).load(&#x27;file:///root/data/spark/taxi/taxi.csv&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745677064.png" alt="img"><br>可以看到读取后的数据已经映射到各个定义好的字段中了。</p>
<h4 id="4-2-4-检查已导入的数据"><a href="#4-2-4-检查已导入的数据" class="headerlink" title="4.2.4 检查已导入的数据"></a>4.2.4 检查已导入的数据</h4><p>读取数据之后，通常会使用 printSchema() 方法打印出 DataFrame 的字段格式。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taxiDF.printSchema()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745699574.png" alt="img"><br>使用 show() 方法打印出前 20 条记录，查看数据是否正常。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taxiDF.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745706055.png" alt="img"></p>
<h3 id="4-3对出租车数据进行聚类"><a href="#4-3对出租车数据进行聚类" class="headerlink" title="4.3对出租车数据进行聚类"></a>4.3对出租车数据进行聚类</h3><h4 id="4-3-1定义特征数组"><a href="#4-3-1定义特征数组" class="headerlink" title="4.3.1定义特征数组"></a>4.3.1定义特征数组</h4><p>为了能够使用 Spark 框架实现的 K-Means 算法，我们需要将数据的特征按照这类机器学习算法的要求进行转换。<br>在机器学习算法应用过程中，经常面临的工作便是为机器学习算法提供的 API 的各项参数 “制备” 数据。<br>转换的目标格式是特征向量，它用数字代表每种类型的特征值（例如用 1 代表位置 A，用 2 代表位置 B  ）。这样做的好处是为了简化数据本身不同特征类型（字符型、数值型、日期时间型等）之间带来的复杂度提升。我们可以在 Spark 中利用  VectorAssembler 这个 API 来进行转换。 它会返回在单个列向量中包含所有特征列的 DataFrame 。<br>对于出租车数据，我们在聚类这一步中主要对其位置进行分析，因此只需要将纬度和经度这两个特征转换为一个特征数组即可。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745722927.png" alt="img"><br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val columns = Array(&#x27;Lat&#x27;, &#x27;Lon&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745730550.png" alt="img"><br>然后创建一个向量装配器 VectorAssembler 对象，并设置相关的属性。并利用向量装配器对象的 transform() 方法对导入的数据（taxiData）进行转化。VectorAssembler 是一个能够将多个列合并到单个列向量中的特征转化器。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 设置参数</span><br><span class="line">val va = new VectorAssembler().setInputCols(columns).setOutputCol(&#x27;features&#x27;)</span><br><span class="line"></span><br><span class="line">// 将数据集按照指定的特征向量进行转化</span><br><span class="line">val taxiDF2 = va.transform(taxiDF)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745740244.png" alt="img"><br>按照惯例，仍然需要对转化后的 DataFrame 中的数据进行检查。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taxiDF2.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745747423.png" alt="img"></p>
<h4 id="4-3-2进行-K-Means-计算"><a href="#4-3-2进行-K-Means-计算" class="headerlink" title="4.3.2进行 K-Means 计算"></a>4.3.2进行 K-Means 计算</h4><p>由于 K-Means 算法是迭代算法，需要对数据多次操作，因此 Spark 官方文档中建议我们将其缓存下来以加速计算。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taxiDF2.cache()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745754896.png" alt="img"><br>聚类之前，我们先把数据集按照合适的比例划分为训练集和测试集。此处我们设置 DataFrame 中 70% 的数据为训练集， 30% 的数据为测试集。同时，使用 randomSplit() 方法对其进行划分。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 设置训练集与测试集的比例</span><br><span class="line">val trainTestRatio = Array(0.7, 0.3)</span><br><span class="line"></span><br><span class="line">// 对数据集进行随机划分，randomSplit 的第二个参数为随机数的种子</span><br><span class="line">val Array(trainingData, testData) = taxiDF2.randomSplit(trainTestRatio, 2333)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745762832.png" alt="img"><br>数据准备好之后，就可以创建 KMeans 对象并对指定的数据进行训练。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745770040.png" alt="img"><br>涉及到的 API 主要有：<br>1.setK()：是一个 “Parameter setter”，用于设置聚类的簇数量。<br>2.setFeaturesCol()：设置数据集中的特征列所在的字段名称。<br>3.setPredictionCol：设置生成预测值时使用的字段名称。<br>4.fit()：将 KMeans 对象对指定数据的特征进行匹配适应，训练模型。<br>请在 Spark Shell 中输入以下代码。注意：此步骤比较耗时，请耐心等待。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 设置模型的参数</span><br><span class="line">val km = new KMeans().setK(10).setFeaturesCol(&#x27;features&#x27;).setPredictionCol(&#x27;prediction&#x27;)</span><br><span class="line"></span><br><span class="line">// 训练 KMeans 模型，此步骤比较耗时</span><br><span class="line">val kmModel = km.fit(taxiDF2)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745779344.png" alt="img"><br>获取 KMeans 模型的聚类中心。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val kmResult = kmModel.clusterCenters</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745787860.png" alt="img"><br>可以看到输出的结果中，便是我们设定数量为 10 的聚类结果。<br>请将当前这一步得到的结果保存到文件中，我们在下一节的数据可视化实验中会用到这些数据。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 先将结果转化为 RDD 以便于保存</span><br><span class="line">val kmRDD1 = sc.parallelize(kmResult)</span><br><span class="line"></span><br><span class="line">// 保存前将经纬度进行位置上的交换</span><br><span class="line">val kmRDD2 = kmRDD1.map(x =&gt; (x(1), x(0)))</span><br><span class="line"></span><br><span class="line">// 调用 saveAsTextFile 方法保存到文件中，</span><br><span class="line">kmRDD2.saveAsTextFile(&#x27;file:///root/kmResult&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745813703.png" alt="img"></p>
<h4 id="4-3-3-对测试集进行聚类"><a href="#4-3-3-对测试集进行聚类" class="headerlink" title="4.3.3 对测试集进行聚类"></a>4.3.3 对测试集进行聚类</h4><p>接下来，我们使用已经训练好的 KMeans 模型，对测试集中的数据进行聚类，产生预测的结果，并对聚类结果进行深入分析。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745820599.png" alt="img"><br>首先是调用模型的 transform() 方法对测试数据进行聚类。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val predictions = kmModel.transform(testData)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745829797.png" alt="img"><br>然后查看预测结果。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745836120.png" alt="img"></p>
<h3 id="4-4分析聚类的预测结果"><a href="#4-4分析聚类的预测结果" class="headerlink" title="4.4分析聚类的预测结果"></a>4.4分析聚类的预测结果</h3><h4 id="4-4-1每天哪个时段的出租车最繁忙？"><a href="#4-4-1每天哪个时段的出租车最繁忙？" class="headerlink" title="4.4.1每天哪个时段的出租车最繁忙？"></a>4.4.1每天哪个时段的出租车最繁忙？</h4><p>为了回答这个问题，需要在预测结果中提取出 Time 字段的前 2 位作为一天之中的小时数，同时提取 prediction 字段为后续的步骤做准备。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/* 使用 select 方法选取字段，</span><br><span class="line">*  substring 用于提取时间的前 2 位作为小时，</span><br><span class="line">*  alias 方法是为选取的字段命名一个别名，</span><br><span class="line">*  选择字段时用符号 $ ，</span><br><span class="line">*  groupBy 方法对结果进行分组。</span><br><span class="line">*/</span><br><span class="line">val tmpQuery = predictions.select(substring($&#x27;Time&#x27;,0,2).alias(&#x27;hour&#x27;), $&#x27;prediction&#x27;).groupBy(&#x27;hour&#x27;, &#x27;prediction&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745895457.png" alt="img"><br>接着，我们基于上述查询的结果，对每个小时不同预测类型的数量进行统计。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/* agg 是聚集函数，count 为其中的一种实现，</span><br><span class="line">*  用于统计某个字段的数量。</span><br><span class="line">*  最后的结果按照预测命中数来降序排列（Desc）。</span><br><span class="line">*/</span><br><span class="line">val predictCount = tmpQuery.agg(count(&#x27;prediction&#x27;).alias(&#x27;count&#x27;)).orderBy(desc(&#x27;count&#x27;))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745903773.png" alt="img"><br>最后，输入前 20 条记录，以查看预测的结果。<br>请在 Spark Shell 中输入以下代码。注意：此步骤比较耗时，请耐心等待。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictCount.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745911379.png" alt="img"><br>结果中每个时段的出租车服务次数按照降序进行了排列。可以看到 14 点 至 21 点这个时段里，聚类区域为 4 号区域内的出租车载客次数是最多的，为 5552 次。同时，从总体上来看，聚类区域为 4  号的区域前 20 条记录内占绝大多数，我们可以大胆推测在该区域内打车的人比较多，并且在 14 点、 15 点 和 21 点  最难打到车（真的是这样吗？根据自己的生活经验想一下是否合理）。</p>
<h4 id="4-4-2-每天哪个区域的出租车最繁忙？"><a href="#4-4-2-每天哪个区域的出租车最繁忙？" class="headerlink" title="4.4.2 每天哪个区域的出租车最繁忙？"></a>4.4.2 每天哪个区域的出租车最繁忙？</h4><p>其实我们在回答前一个问题的时候，这个问题的答案已经隐藏在其中了，我们只需要一个查询语句便可得到。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val busyZones = predictions.groupBy(&#x27;prediction&#x27;).count()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745921737.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">busyZones.show()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677745929464.png" alt="img"><br>结果中可以看出来4号区域的出租车最繁忙，依次是7号区域、6号区域。</p>
<h2 id="32-Spark实验：K均值聚类评估足球比赛"><a href="#32-Spark实验：K均值聚类评估足球比赛" class="headerlink" title="32.Spark实验：K均值聚类评估足球比赛"></a>32.Spark实验：K均值聚类评估足球比赛</h2><blockquote>
<h3 id="目的-11"><a href="#目的-11" class="headerlink" title="目的"></a>目的</h3><p>K-means 算法采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。</p>
<p>该算法认为类簇是由距离靠近的对象组成的，因此把得到紧凑且独立的簇作为最终目标。它是数据点到原型的某种距离作为优化的目标函数，利用函数求极值的方法得到迭代运算的调整规则。</p>
<p>K-means 算法以欧式距离作为相似度测度，它是求对应某一初始聚类中心向量 V 最优分类，使得评价指标最小。算法采用误差平方和准则函数作为聚类准则函数。本节课首先讲解 Kmeans 算法思想，然后再用 Kmeans 分析足球赛事。</p>
<h3 id="要求-11"><a href="#要求-11" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能：<br>Scala 基础编程<br>Spark Mlib 的 Kmeans 算法应用</p>
<h3 id="原理-11"><a href="#原理-11" class="headerlink" title="原理"></a>原理</h3><h3 id="3-1问题引入"><a href="#3-1问题引入" class="headerlink" title="3.1问题引入"></a>3.1问题引入</h3><p>K-Means 算法主要解决的问题如下图所示。图中有一些散乱的点，用肉眼隐约可以看到大致有三个点集。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746812991.png" alt="img"></p>
<p>但是我们怎么通过计算机程序找出这几个点群来呢？于是就出现了我们的 K-Means 算法。</p>
<p>我们希望根据这些不同的点的分布，分为几个簇，把这些点就近划分到不同的簇，这些簇就是接下来我们说的 K 值，下图是我用圆圈标上了这些点可能属于的簇集，更醒目一些，当然这里 K 值取值不是唯一的如图：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746819351.png" alt="img"></p>
<h3 id="3-2-算法思想"><a href="#3-2-算法思想" class="headerlink" title="3.2 算法思想"></a>3.2 算法思想</h3><p>通过不断的迭代来寻找 k 值，形成一种划分方式，使得用这 k 个类簇的均值来代表相应各类样本时所得的总体误差最小。同一聚类中的对象相似度较高;而不同聚类中的对象相似度较小。k-means 算法的基础是最小误差平方和准则， 其函数是：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677746826039.png" alt="img"></p>
<p>上式中，μc(i) 表示第 i 个聚类的均值。划分到各类簇内的样本越相似，其与该类均值间的误差平方越小，然后对所有类计算所得到的误差平方再次累加求和，即我们希望 J 值越小越好。</p>
<h3 id="3-3-算法实现步骤"><a href="#3-3-算法实现步骤" class="headerlink" title="3.3 算法实现步骤"></a>3.3 算法实现步骤</h3><p>k-means 算法是将样本聚类成 k 个簇中心，这里的 k 值是我们给定的，也就是我们希望把数据分成几个类别。<br>具体算法描述如下：<br>1、为需要聚类的数据，随机选取 k 个聚类质心点；<br>2、求每个点到聚类质心点的距离，计算其应该属于的类，迭代直到收敛于某个值。<br>对于每一个样例 i：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677747218031.png" alt="img"></p>
<p>对于每一个类 <code>j</code>，重新计算该类的质心，从而确定新的簇心，一直迭代到某个值或达到要求：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677747228192.png" alt="img"></p>
<p>接下来，我们进入实现阶段，进行实际操作给大家讲解相应的知识。</p>
</blockquote>
<h3 id="4-1数据集介绍"><a href="#4-1数据集介绍" class="headerlink" title="4.1数据集介绍"></a>4.1数据集介绍</h3><p>实验中数据参考根据张洋的算法杂货铺，并做适当的修改。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">序号    国别  2006年世界杯    2007年亚洲杯    2010年世界杯</span><br><span class="line">1       韩国       17           3               15</span><br><span class="line">2       沙特       28           2               40</span><br><span class="line">3       卡塔尔     50           9               40</span><br><span class="line">4       泰国       50           9               50 </span><br><span class="line">5       越南       50           5               50</span><br><span class="line">6       中国       50           9               50</span><br><span class="line">7       巴林       40           9               40 </span><br><span class="line">8       阿联酋     50           9               40 </span><br><span class="line">9       伊朗       25           5               40 </span><br><span class="line">10      日本       28           4               9</span><br></pre></td></tr></table></figure>

<p>根据图片可以得知这 15 支球队在 2006 年~ 2010  年的比赛情况，其中包括两次世界杯和一次亚洲杯。图片中的数据做了如下预处理：对于亚洲杯，前四名取其排名，十六强赋予 9，八强赋予  5，预选赛没出现的赋予 17。对于世界杯，进入决赛圈则取其最终排名，没有进入决赛圈的，打入预选赛十强赛赋予 40，预选赛小组未出线的赋予  50。这样做方便我们接下来使用数据。</p>
<p>根据图片中的数据，我们可以把图片上的数据存储为 data.txt</p>
<p>使用 vim 命令创建并编辑 data.txt。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim data.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748261998.png" alt="img"></p>
<p>添加如下内容，字段之间用空格分隔并保存。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">17 3 15</span><br><span class="line">28 2 40</span><br><span class="line">50 9 40</span><br><span class="line">50 9 50</span><br><span class="line">50 5 50</span><br><span class="line">50 9 50</span><br><span class="line">40 9 40</span><br><span class="line">50 9 40</span><br><span class="line">25 5 40</span><br><span class="line">28 4 9</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748268839.png" alt="img"></p>
<p>使用 more 命令显示刚刚输入的数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># more data.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748276108.png" alt="img"></p>
<p>我们再将data.txt文件上传到HDFS 上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir /root</span><br><span class="line"># hdfs dfs -put /root/data.txt /root/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748283013.png" alt="img"></p>
<h3 id="4-2-启动-spark-shell"><a href="#4-2-启动-spark-shell" class="headerlink" title="4.2 启动 spark shell"></a>4.2 启动 spark shell</h3><p>请在终端中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark-shell</span><br></pre></td></tr></table></figure>

<p>耐心等待 spark-shell 启动</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748301408.png" alt="img"></p>
<p>进入到 Spark 的 REPL 环境，相当于就是 Spark 的 Console。</p>
<h3 id="4-3-导入数据"><a href="#4-3-导入数据" class="headerlink" title="4.3 导入数据"></a>4.3 导入数据</h3><p>拓展：Spark textFile 进行数据的导入，将外部数据导入到 Spark 中来，并将其转换成 RDD。 键入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val dataset = sc.textFile(&#x27;/root/data.txt&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748308478.png" alt="img"></p>
<p>从图中我们可以看到我们引入了 &#x2F;root下面的 data.txt 文件。在最后我们也了解到了当前的数据格式为 RDD[ String ] 类型的数据。<br>现在我们可以看看当前 dataset 的数据样式。我们查看 dataset 的前十五行数据。<br>拓展：这里使用到 RDD Action 操作，take(num)函数，take 的作用就是获取 RDD 中下标 0 到 num-1 的元素。foreach 遍历，并打印。</p>
<p>键入命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.take(10).foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748316258.png" alt="img"></p>
<h3 id="4-4使用import-命令导入依赖"><a href="#4-4使用import-命令导入依赖" class="headerlink" title="4.4使用import 命令导入依赖"></a>4.4使用import 命令导入依赖</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.linalg.Vectors</span><br><span class="line">import org.apache.spark.mllib.clustering.KMeans</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748326183.png" alt="img"></p>
<p>然后我们将这些数据以空格分割开，拆分数据，将其转换 Vector 格式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val data = dataset.map  &#123;</span><br><span class="line">           l =&gt;</span><br><span class="line">             Vectors.dense(l.split(&#x27; &#x27;).map(_.toDouble))</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748333198.png" alt="img"></p>
<p>由于需要多次使用该数据，用 cache 进行缓存，用 collect 查看数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.cache()</span><br><span class="line">data.collect</span><br></pre></td></tr></table></figure>

<p>结果如下图：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748355368.png" alt="img"></p>
<p>划分子集，及迭代次数，这里选择 3 次，迭代次数根据机器状况决定，这里选择 100 次。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val km = KMeans.train(data,3,100)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748362215.png" alt="img"></p>
<p>分别打印这三个子集的质心</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">km.clusterCenters.foreach &#123; println &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748367952.png" alt="img"></p>
<h3 id="4-5打印数据及对应的子集"><a href="#4-5打印数据及对应的子集" class="headerlink" title="4.5打印数据及对应的子集"></a>4.5打印数据及对应的子集</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.foreach &#123;</span><br><span class="line">           p =&gt;</span><br><span class="line">             println(p + &#x27; belongs to subset: &#x27; + km.predict(p))</span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748403136.png" alt="img"></p>
<p>由质心，数据及对应的子集，知道 3 个子集的索引分别是 0、1、2，如果对数据排序， 应该是 2、0、1。显然在子集[ 2 ]中有 2  个国家，分别是沙特，伊朗，这两个国家足球水平较好点，接着在子集[ 0 ]中也有两个国家，分别为韩国，日本，这两个国家足球水平一般，最后在在子集[ 1 ]剩下 6 个国家，其中包含中国[ 50, 50, 9 ]，说明中国足球有点差，不太理想，需要加油，至此实验就结束啦。</p>
<p>注意:本实验仅仅为了演示，实验结果不代表任何政治立场。</p>
<h2 id="33-Spark实验：自定义UDF分析Uber数据"><a href="#33-Spark实验：自定义UDF分析Uber数据" class="headerlink" title="33.Spark实验：自定义UDF分析Uber数据"></a>33.Spark实验：自定义UDF分析Uber数据</h2><blockquote>
<h3 id="目的-12"><a href="#目的-12" class="headerlink" title="目的"></a>目的</h3><p>在本课程中使用 SQL Context 及相关的 API 进行统计分析，最后还将通过一个 Spark 分析旅游数据的实例，进一步学习如何利用 Spark SQL 分析数据。</p>
<h3 id="要求-12"><a href="#要求-12" class="headerlink" title="要求"></a>要求</h3><p>1、SQL Context<br>2、自定义 UDF 函数<br>3、SQL 语句</p>
<h3 id="原理-12"><a href="#原理-12" class="headerlink" title="原理"></a>原理</h3><p>介绍如何使用 Spark UDF 函数分析 uber 数据，希望帮助您掌握了 SparkSQL 的概念，并学会使用自定义 UDF，因为 spark 的函数有时候并不符合我们的需求。<br>本次实验数据在以下路径：<br>&#x2F;root&#x2F;data&#x2F;spark&#x2F;uber<br>该数据集共有4个字段，每个字段的具体含义如下：<br>字段： 字段类型： 描述信息 dispatching_base_number：String 区域编号 date：String 日期 active_vehicles：Int 使用的机动车数量 trips: Int 旅游次数<br>请输入以下命令，查看行数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># wc -l uber</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433151743.png" alt="img"><br>请输入以下命令，查看前行内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># head -n 10 uber</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433158582.png" alt="img"><br>注意，第一行是各字段的描述信息，即表头并非数据，稍后数据处理时，需要排除掉此行</p>
</blockquote>
<h3 id="4-1-启动-spark-shell"><a href="#4-1-启动-spark-shell" class="headerlink" title="4.1 启动 spark shell"></a>4.1 启动 spark shell</h3><p>接下来，我们将使用 spark shell 来编写，请在终端中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433167690.png" alt="img"><br>进入到 spark 的 REPL 环境，相当于就是 spark 的 Console。</p>
<h3 id="4-2-导入数据并转换数据"><a href="#4-2-导入数据并转换数据" class="headerlink" title="4.2 导入数据并转换数据"></a>4.2 导入数据并转换数据</h3><p>下面我们就进入到实验的关键位置，进行数据的导入，并进行必要的数据格式处理。<br>键入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val data= sc.textFile(&#x27;file:///root/data/spark/uber/uber &#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433191462.png" alt="img"><br>调用 first ()取第一行数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.first</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433198617.png" alt="img"><br>可以看到与之前用 linux 命令查到的结果相同，接下来，过滤掉第一行数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val hd = data.first()</span><br><span class="line"></span><br><span class="line">val datafiltered = data.filter(line =&gt; line != hd)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433212290.png" alt="img"><br>检查行数，显示 354 行，第一行已经过滤掉。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datafiltered.count</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433221162.png" alt="img"><br>首先定义一个样例类，封装数据字段。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">case class uber(dispatching_base_number:String ,date:String,active_vehicles:Int,trips:Int)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433231219.png" alt="img"><br>通过 case class 将数据转换为 dataframe</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df = datafiltered.map(x=&gt;x.split(&#x27;,&#x27;)).map(x =&gt; uber(x(0).toString,x(1),x(2).toInt,x(3).toInt)).toDF</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433238844.png" alt="img"><br>将数据对象存成临时表，便于后续的查询操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.registerTempTable(&#x27;uber&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433246764.png" alt="img"></p>
<h3 id="4-3-自定义-UDF-分析数据"><a href="#4-3-自定义-UDF-分析数据" class="headerlink" title="4.3 自定义 UDF 分析数据"></a>4.3 自定义 UDF 分析数据</h3><p>自定义 UDF，解析日期列，UDF 函数其实与 Scala 函数没区别，传几个参数决定几个列，这里我们只传递了一个参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def</span><br><span class="line">pas = (s: String) =&gt; &#123;</span><br><span class="line">val format = new java.text.SimpleDateFormat(&#x27;MM/dd/yyyy&#x27;)</span><br><span class="line">var days =Array(&#x27;Sun&#x27;,&#x27;Mon&#x27;,&#x27;Tue&#x27;,&#x27;Wed&#x27;,&#x27;Thu&#x27;,&#x27;Fri&#x27;,&#x27;Sat&#x27;)</span><br><span class="line">val split = days(format.parse(s).getDay).toString</span><br><span class="line">split</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676433347222.png" alt="img"><br>我们编写的 UDF 采用字符串格式的日期，使用 java 的 SimpleDataFormat 类进行解析，getDay  函数将返回一周的日期数。所以我们创建了一个存储周的名字的数组。根据 getDay  函数的返回值，返回数组的相应索引值。所以最后我们会得到一周中的一天。<br>最后别忘记将我们编写的 UDF 注册到 Spark SQL 中，register()函数第二个参数是我们自己定义的 UDF 函数，第一个只不过是我们起的别名。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line">sqlContext.udf.register(&#x27;ppa&#x27;,pas)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441837003.png" alt="img"></p>
<h3 id="4-4-查询各区域编号的旅游次数"><a href="#4-4-查询各区域编号的旅游次数" class="headerlink" title="4.4 查询各区域编号的旅游次数"></a>4.4 查询各区域编号的旅游次数</h3><p>查找每一个 dispatching_base_number(区域编号)在每一天的旅游次数之和，并且按照星期降序排序。<br>利用自定义的 UDF 解析日期，它返回每天基于日期的行程次数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val rep = sqlContext.sql(&#x27;select dispatching_base_number as dis, ppa(date) as dt ,sum(trips) as cnt from uber group by dispatching_base_number,ppa(date) order by cnt desc&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441845664.png" alt="img"><br>打印结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rep.collect</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441851892.png" alt="img"><br>所有这些操作，在 spark 的 web 界面都可以看到，双击打开浏览器，输入master节点ip地址+端口号4040(如10.30.196.3:4040)。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441899822.png" alt="img"><br>可以看到有 4 个 Jobs 已经完成，我的 RDD 操作多一点，不一定与你的相同，继续点击任意一个 jobs（我选择第一个）。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441908313.png" alt="img"><br>接下来点击 DAG Visualization，可以看到具体的执行计划。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441886740.png" alt="img"><br>当然通过浏览器看到的更直观，还有很多其他细节就不一一举例了，您可以多了解下，直观的感受下 spark 的强大。</p>
<h3 id="4-5-查询各区域编号的机动车的使用"><a href="#4-5-查询各区域编号的机动车的使用" class="headerlink" title="4.5 查询各区域编号的机动车的使用"></a>4.5 查询各区域编号的机动车的使用</h3><p>在上步骤可以紧接着查询每一个 dispatching_base_number(区域编号)在每一天的机动车的数量使用情况，并且按照星期降序排序。 利用自定义的 UDF 解析日期，它返回每天基于日期的机动车使用次数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val rep = sqlContext.sql(&#x27;select dispatching_base_number as dis, ppa(date) as dt ,sum(active_vehicles) as cnt from uber group by dispatching_base_number,ppa(date) order by cnt desc&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441921735.png" alt="img"><br>打印结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rep.collect</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441929912.png" alt="img"><br>请输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:quit</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676441937917.png" alt="img"></p>
<h2 id="34-Spark实验：分析银行营销数据"><a href="#34-Spark实验：分析银行营销数据" class="headerlink" title="34.Spark实验：分析银行营销数据"></a>34.Spark实验：分析银行营销数据</h2><blockquote>
<h3 id="目的-13"><a href="#目的-13" class="headerlink" title="目的"></a>目的</h3><p>存款营销是银行吸收存款的主要经营模式，通过现有数据建立模型来判断客户是否订阅存款业务，从而帮助商业银行更好的分配人力资源，提高业务量，以满足现阶段营销活动对提高营销成功率的期望。<br>本实验会使用 spark 机器学习中的逻辑回归算法，分析银行营销数据，按照机器学习开发步骤，建立逻辑回归模型，预测客户是否会存款，并评估预测模型的精确度。</p>
<h3 id="要求-13"><a href="#要求-13" class="headerlink" title="要求"></a>要求</h3><p>1本次试验后，要求学生能掌握Spark 机器学习开发的常规步骤、Spark 框架提供的特征转换算法–StringIndex、Spark  框架提供的特征转换算法–OneHotEncoder、Spark 提供的 API 对预测结果准确度进行评估、Spark SQL 在 Spark  机器学习中的用法。</p>
<h3 id="原理-13"><a href="#原理-13" class="headerlink" title="原理"></a>原理</h3><h3 id="3-1-逻辑回归-Logistic-Regression"><a href="#3-1-逻辑回归-Logistic-Regression" class="headerlink" title="3.1 逻辑回归(Logistic Regression)"></a>3.1 逻辑回归(Logistic Regression)</h3><p>机器学习算法分为监督学习算法和无监督学习算法，在监督学习算法中，算法有目标变量(即预测值)和特征变量等两种变量，根据目标变量的值是离散值还是连续数值，分为分类算法(目标变量的值为离散值，如 是&#x2F;否、0&#x2F;1)和回归算法(目标变量为连续值)。<br>逻辑回归是一个分类算法而不是回归算法。通常是利用已知的特征变量来预测一个离散型目标变量的值(如  0&#x2F;1，是&#x2F;否，真&#x2F;假)。通过拟合一个逻辑函数来预测一个事件发生的概率，预测值是一个概率值(0-100%),根据概率值的大小,映射为目标变量的分类值,如:概率值大于等于 50%,映射目标变量分类值为 1,概率值小于 50%,映射目标变量分类值为 0。</p>
<h3 id="3-2-逻辑回归与线性回归"><a href="#3-2-逻辑回归与线性回归" class="headerlink" title="3.2 逻辑回归与线性回归"></a>3.2 逻辑回归与线性回归</h3><p>为了更好理解逻辑回归算法，可以与线性回归算法做个对比。线性回归对多维空间中存在的样本点，用特征的线性组合去拟合多维空间中点的分布和轨迹。线性回归的公式如下：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677808500477.png" alt="img"><br>对于逻辑回归来说，其思想也是基于线性回归。其公式如下：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677808508106.png" alt="img"><br> 其中, <img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677808517539.png" alt="img">被称作 sigmoid 函数，Logistic Regression 算法将线性函数的结果映射到了 sigmoid 函数中。sigmoid 的函数图形如下：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677808530754.png" alt="img"><br> sigmoid 的函数输出是介于（0，1）之间的，中间值是 0.5，之前的公式 hθ(x)输出是介于（0，1）之间，表明某一样本数据属于某一类别的概率，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hθ(x)&lt;0.5 则说明当前样本数据属于A类；</span><br><span class="line">hθ(x)&gt;0.5 则说明当前样本数据属于B类。</span><br></pre></td></tr></table></figure>

<p>所以可以将 sigmoid 函数看成样本数据的概率函数。Spark 提供的逻辑回归算法，对样本数据的预测结果中，有一列存储了概率值。本实验中将此概率值进行了输出。</p>
<h3 id="3-3-特征工程"><a href="#3-3-特征工程" class="headerlink" title="3.3 特征工程"></a>3.3 特征工程</h3><p>特征工程是最大限度地从原始数据中提取特征以供算法和模型使用的过程。特征工程包括特征提取、特征转换、降维等操作。 Spark 提供了多种特征工程算法，详细内容可查看官方文档。<br>本实验中，我们使用 StringIndex 和 OneHotEncoder 两种特征转换算法。</p>
<h4 id="3-3-1-StringIndex"><a href="#3-3-1-StringIndex" class="headerlink" title="3.3.1 StringIndex"></a>3.3.1 StringIndex</h4><p>StringIndexer 是指把一组字符型标签编码成一组数值型标签索引，索引的范围为 0 到标签数量，索引构建的顺序为标签的频率，优先编码频率较大的标签，所以出现频率最高的标签为 0 号。如果输入的是数值型的，会转成字符型，再对其进行编码。<br>Spark 官方实例： 下列数据，包括 id 和 category 两列：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">id | category</span><br><span class="line">----|----------</span><br><span class="line">0  | a</span><br><span class="line">1  | b</span><br><span class="line">2  | c</span><br><span class="line">3  | a</span><br><span class="line">4  | a</span><br><span class="line">5  | c</span><br></pre></td></tr></table></figure>

<p>使用 StringIndexer 特征转换算法，对上述数据 category 列进行特征转换,设置生成新列名为 categoryIndex，结果为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> id | category | categoryIndex</span><br><span class="line">----|----------|---------------</span><br><span class="line"> 0  | a        | 0.0</span><br><span class="line"> 1  | b        | 2.0</span><br><span class="line"> 2  | c        | 1.0</span><br><span class="line"> 3  | a        | 0.0</span><br><span class="line"> 4  | a        | 0.0</span><br><span class="line"> 5  | c        | 1.0</span><br></pre></td></tr></table></figure>

<p>上述结果中，由于 a 出现了 3 次，c 出现了 2 次,b 出现了 1 次，按照出现频率由高到低，从 0 开始编码，所以 a 的编码为 0.0,c 的编码为 1.0,b 的编码为 2.0。</p>
<h4 id="3-3-2-OneHotEncoder"><a href="#3-3-2-OneHotEncoder" class="headerlink" title="3.3.2 OneHotEncoder"></a>3.3.2 OneHotEncoder</h4><p>OneHot 编码将已经转换为数值型的类别特征，映射为一个稀疏向量对象,对于某一个类别映射的向量中只有一位有效,即只有一位数字是 1，其他数字位都是 0。如下面的例子，有如下两个特征属性：<br>婚姻状况:[“已婚”,”单身”,”离异”,”未知”]<br>有无房贷:[“有房贷”,”无房贷”]<br>对于某一个样本，如[“已婚”，”无房贷”]，因为机器学习算法不接收字符型的特征值，我们需要将这个分类值的特征数字化，最直接的方法，可以采用序列化的方式：[0，1]。但是这样的特征处理并不能直接放入机器学习算法中。对于这个问题，婚姻状况是 4 维的，有无房贷是 2 维的，这样，我们可以采用 One-Hot  编码的方式对上述的样本[“已婚”，”无房贷”]编码，”已婚”对应[1，0，0，0]，”无房贷”对应[0，1]，则完整的特征数字化的结果为：[1，0，0，0，0，1]，这样做结果就是数据会变得连续，但也会非常的稀疏，所以在 Spark 中，使用了稀疏向量来表示这个结果。<br>逻辑回归算法的分类器需要连续数值作为特征输入。<br>实验将采用如下步骤进行回归机器学习算法开发：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677808543262.png" alt="img"></p>
</blockquote>
<h3 id="4-1-获取银行营销数据"><a href="#4-1-获取银行营销数据" class="headerlink" title="4.1 获取银行营销数据"></a>4.1 获取银行营销数据</h3><p>在本实验中所用到的实验数据都存储在<br>&#x2F;root&#x2F;data&#x2F;spark&#x2F;bank&#x2F;bank_marketing_data.csv<br> 据结构描述<br>在数据 bank_marketing_data.csv 中，包含 4 万多条记录和 21 个字段，本次实验中，我们使用其中 10 个字段作为因变量，1 个字段作为目标变量，进行分析预测。11 个字段的名称和中文含义如下：</p>
<table>
<thead>
<tr>
<th>字段名称</th>
<th>中文含义</th>
</tr>
</thead>
<tbody><tr>
<td>age</td>
<td>客户年龄</td>
</tr>
<tr>
<td>job</td>
<td>客户职业</td>
</tr>
<tr>
<td>marital</td>
<td>婚姻状况</td>
</tr>
<tr>
<td>default</td>
<td>是否有信用违约</td>
</tr>
<tr>
<td>housing</td>
<td>是否有住房贷款</td>
</tr>
<tr>
<td>loan</td>
<td>是否有住房贷款</td>
</tr>
<tr>
<td>duration</td>
<td>最后一次联系持续时间(秒)</td>
</tr>
<tr>
<td>previous</td>
<td>之前活动中与用户联系次数</td>
</tr>
<tr>
<td>poutcome</td>
<td>之前市场营销活动的结果</td>
</tr>
<tr>
<td>empvarrate</td>
<td>就业变化速率</td>
</tr>
<tr>
<td>y</td>
<td>目标变量，本次活动实施结果：是否同意存款</td>
</tr>
</tbody></table>
<h3 id="4-2编写程序查看数据结构"><a href="#4-2编写程序查看数据结构" class="headerlink" title="4.2编写程序查看数据结构"></a>4.2编写程序查看数据结构</h3><p>我们首先导入Hadoop相关外部依赖。导入Hadoop相关外部依赖。</p>
<p>现在下载需要用到的jars，我们选择资料工具 -&gt; 软件下载 -&gt; Spark -&gt; 2.2.2<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677808980012.png" alt="img"><br>单击File -&gt; Project Structure<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677808986425.png" alt="img"><br>选择 Libraries -&gt; ➕ -&gt;java<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677808992085.png" alt="img"><br>选择 &#x2F;spark-2.2.2-bin-hadoop2.7&#x2F;jars 路径下的所有包：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809024071.png" alt="img"><br>打开开发环境：IDEA，创建一个 Scala 项目：package:cn.cstor.algorithm用来存放机器学习算法相关的程序。创建Scala Object：LogisiticRegressionTest。<br>在LogisiticRegressionTest Object中创建main方法，此时的项目结构如下：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809037285.png" alt="img"><br><em>注意</em>：path路径以数据下载路径为主<br>在 main 方法中编写如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">//实例化SparkSession对象</span><br><span class="line">    val spark=SparkSession.builder().appName(&#x27;Logistic_Prediction&#x27;).master(&#x27;local&#x27;).getOrCreate()</span><br><span class="line">    //设置日志级别，减少日志输出，便于查看运行结果</span><br><span class="line">    spark.sparkContext.setLogLevel(&#x27;WARN&#x27;)</span><br><span class="line">    //导入隐式转换包，方便使用转换函数</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    //读取数据，传入数据路径/opt/train/bank_marketing_data.csv</span><br><span class="line">    val bank_Marketing_Data=spark.read</span><br><span class="line">                                 .option(&#x27;header&#x27;, true)</span><br><span class="line">                                 .option(&#x27;inferSchema&#x27;, &#x27;true&#x27;)</span><br><span class="line">                                 .csv(&#x27;/bank_marketing_data.csv&#x27;)这里是自己本地的地址</span><br><span class="line">    //查看营销数据的前5条记录，包括所有字段</span><br><span class="line">    println(&#x27;all columns data:&#x27;)</span><br><span class="line">    bank_Marketing_Data.show(5)</span><br><span class="line">    //读取营销数据指定的11个字段，并将age、duration、previous三个字段的类型从Integer类型转换为Double类型</span><br><span class="line">    val selected_Data=bank_Marketing_Data.select(&#x27;age&#x27;,</span><br><span class="line">                                                &#x27;job&#x27;,</span><br><span class="line">                                                &#x27;marital&#x27;,</span><br><span class="line">                                                &#x27;default&#x27;,</span><br><span class="line">                                                &#x27;housing&#x27;,</span><br><span class="line">                                                &#x27;loan&#x27;,</span><br><span class="line">                                                &#x27;duration&#x27;,</span><br><span class="line">                                                &#x27;previous&#x27;,</span><br><span class="line">                                                &#x27;poutcome&#x27;,</span><br><span class="line">                                                &#x27;empvarrate&#x27;,</span><br><span class="line">                                                &#x27;y&#x27;)</span><br><span class="line">                                                .withColumn(&#x27;age&#x27;, bank_Marketing_Data(&#x27;age&#x27;).cast(DoubleType))</span><br><span class="line">                                                .withColumn(&#x27;duration&#x27;, bank_Marketing_Data(&#x27;duration&#x27;).cast(DoubleType))</span><br><span class="line">                                                .withColumn(&#x27;previous&#x27;, bank_Marketing_Data(&#x27;previous&#x27;).cast(DoubleType))</span><br><span class="line">     //显示前5条记录，只包含指定的11个字段</span><br><span class="line">     println(&#x27;11 columns data:&#x27;)</span><br><span class="line">     selected_Data.show(5)</span><br><span class="line">     //显示营销数据的数据量</span><br><span class="line">     println(&#x27;data count:&#x27;+selected_Data.count())</span><br></pre></td></tr></table></figure>

<p>上述代码,首先实例化SparkSession对象,接着读取营销数据bank_marketing_data.csv,将其中age,duration,previous三列由Integer转换成Double类型。查看数据信息,对要分析的数据有一个大概了解。需要注意的是：我们只分析数据中选中的 11 列，选中的数据存储在了selected_Data变量中。<br>运行程序，查看运行结果：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809056650.png" alt="img"><br>运行结果包括三部分：包含所有列的前 5 条记录、包含指定 11 列的前 5 条记录和数据总条数：41188 条。<br>上述操作可执行完整代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">package com.shiyanlou.algorithm</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.types.DoubleType</span><br><span class="line"></span><br><span class="line">object LogisiticRegressionTest &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark=SparkSession.builder().appName(&#x27;Logistic_Prediction&#x27;).master(&#x27;local&#x27;).getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(&#x27;WARN&#x27;)</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val bank_Marketing_Data=spark.read</span><br><span class="line">                                 .option(&#x27;header&#x27;, true)</span><br><span class="line">                                 .option(&#x27;inferSchema&#x27;, &#x27;true&#x27;)</span><br><span class="line">                                 .csv(&#x27;/bank_marketing_data.csv&#x27;)</span><br><span class="line">// 这里是自己本地的地址</span><br><span class="line">    println(&#x27;all columns data:&#x27;)</span><br><span class="line">    bank_Marketing_Data.show(5)</span><br><span class="line">    val selected_Data=bank_Marketing_Data.select(&#x27;age&#x27;,</span><br><span class="line">                                                &#x27;job&#x27;,</span><br><span class="line">                                                &#x27;marital&#x27;,</span><br><span class="line">                                                &#x27;default&#x27;,</span><br><span class="line">                                                &#x27;housing&#x27;,</span><br><span class="line">                                                &#x27;loan&#x27;,</span><br><span class="line">                                                &#x27;duration&#x27;,</span><br><span class="line">                                                &#x27;previous&#x27;,</span><br><span class="line">                                                &#x27;poutcome&#x27;,</span><br><span class="line">                                                &#x27;empvarrate&#x27;,</span><br><span class="line">                                                &#x27;y&#x27;)</span><br><span class="line">                                                .withColumn(&#x27;age&#x27;, bank_Marketing_Data(&#x27;age&#x27;).cast(DoubleType))</span><br><span class="line">                                                .withColumn(&#x27;duration&#x27;, bank_Marketing_Data(&#x27;duration&#x27;).cast(DoubleType))</span><br><span class="line">                                                .withColumn(&#x27;previous&#x27;, bank_Marketing_Data(&#x27;previous&#x27;).cast(DoubleType))</span><br><span class="line">     println(&#x27;11 columns data:&#x27;)</span><br><span class="line">     selected_Data.show(5)</span><br><span class="line"></span><br><span class="line">     println(&#x27;data count:&#x27;+selected_Data.count())</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-3-概要分析数据字段内容"><a href="#4-3-概要分析数据字段内容" class="headerlink" title="4.3 概要分析数据字段内容"></a>4.3 概要分析数据字段内容</h3><p>机器学习算法接收的数据都是向量对象，向量对象中的元素值都是数值型的。由于我们所分析的数据中存在非数值型的字段列，我们需要对其进行分析，尤其是包含分类值的字段列，需要统计出分类值和分类数量，为下一步的特征转换做准备。</p>
<h4 id="4-3-1-在-main方法中继续编写如下代码："><a href="#4-3-1-在-main方法中继续编写如下代码：" class="headerlink" title="4.3.1 在 main方法中继续编写如下代码："></a>4.3.1 在 main方法中继续编写如下代码：</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//对数据进行概要统计</span><br><span class="line">     val summary=selected_Data.describe()</span><br><span class="line">     println(&#x27;Summary Statistics:&#x27;)</span><br><span class="line">     //显示概要统计信息</span><br><span class="line">     summary.show()</span><br><span class="line"></span><br><span class="line">     //查看每一列所包含的不同值数量</span><br><span class="line">     val columnNames=selected_Data.columns</span><br><span class="line">     val uniqueValues_PerField=columnNames.map &#123; field =&gt; field+&#x27;:&#x27;+selected_Data.select(field).distinct().count() &#125;</span><br><span class="line">     println(&#x27;Unique Values For each Field:&#x27;)</span><br><span class="line">     uniqueValues_PerField.foreach(println)</span><br></pre></td></tr></table></figure>

<h4 id="4-3-2-运行程序，查看运行结果："><a href="#4-3-2-运行程序，查看运行结果：" class="headerlink" title="4.3.2 运行程序，查看运行结果："></a>4.3.2 运行程序，查看运行结果：</h4><p>概要统计信息<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809070600.png" alt="img"><br>统计信息输出，包括 12 列: 第 1 列为统计值，总数、平均值、方差值、最小值、最大值; 第 2-12 列为指定的数据字段值。从中可以看出每一列的总数、平均值、方差值、最小值、最大值是多少。如果某列内容不是数值，则某些统计信息会是 null。<br>每一列包含多少个不同值<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809079041.png" alt="img"><br>从输出可以看出，age 列有 78 个不同的值，job 列有 12 个不同的值，后续的输出可以以此类推…  需要注意：job、marital、default、housing、poutcome、loan 这 6 列的值是分类值，如 marital  列:婚姻状况，有已婚、单身、离异、未知等分类值。<br>完整代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">package cn.cstor.algorithm</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.types.DoubleType</span><br><span class="line"></span><br><span class="line">object LogisiticRegressionTest &#123;</span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    val spark=SparkSession.builder().appName(&#x27;Logistic_Prediction&#x27;).master(&#x27;local&#x27;).getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(&#x27;WARN&#x27;)</span><br><span class="line">    import spark.implicits._</span><br><span class="line">    val bank_Marketing_Data=spark.read</span><br><span class="line">                                 .option(&#x27;header&#x27;, true)</span><br><span class="line">                                 .option(&#x27;inferSchema&#x27;, &#x27;true&#x27;)</span><br><span class="line">                                 .csv(&#x27;/bank_marketing_data.csv&#x27;)这里是自己本地的地址</span><br><span class="line">    println(&#x27;all columns data:&#x27;)</span><br><span class="line">    bank_Marketing_Data.show(5)</span><br><span class="line">    val selected_Data=bank_Marketing_Data.select(&#x27;age&#x27;,</span><br><span class="line">                                                &#x27;job&#x27;,</span><br><span class="line">                                                &#x27;marital&#x27;,</span><br><span class="line">                                                &#x27;default&#x27;,</span><br><span class="line">                                                &#x27;housing&#x27;,</span><br><span class="line">                                                &#x27;loan&#x27;,</span><br><span class="line">                                                &#x27;duration&#x27;,</span><br><span class="line">                                                &#x27;previous&#x27;,</span><br><span class="line">                                                &#x27;poutcome&#x27;,</span><br><span class="line">                                                &#x27;empvarrate&#x27;,</span><br><span class="line">                                                &#x27;y&#x27;)</span><br><span class="line">                                                .withColumn(&#x27;age&#x27;, bank_Marketing_Data(&#x27;age&#x27;).cast(DoubleType))</span><br><span class="line">                                                .withColumn(&#x27;duration&#x27;, bank_Marketing_Data(&#x27;duration&#x27;).cast(DoubleType))</span><br><span class="line">                                                .withColumn(&#x27;previous&#x27;, bank_Marketing_Data(&#x27;previous&#x27;).cast(DoubleType))</span><br><span class="line">     println(&#x27;11 columns data:&#x27;)</span><br><span class="line">     selected_Data.show(5)</span><br><span class="line"></span><br><span class="line">     println(&#x27;data count:&#x27;+selected_Data.count())</span><br><span class="line"></span><br><span class="line">     val summary=selected_Data.describe()</span><br><span class="line">     println(&#x27;Summary Statistics:&#x27;)</span><br><span class="line">     summary.show()</span><br><span class="line"></span><br><span class="line">     val columnNames=selected_Data.columns</span><br><span class="line">     val uniqueValues_PerField=columnNames.map &#123; field =&gt; field+&#x27;:&#x27;+selected_Data.select(field).distinct().count() &#125;</span><br><span class="line">     println(&#x27;Unique Values For each Field:&#x27;)</span><br><span class="line">     uniqueValues_PerField.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-4-特征工程"><a href="#4-4-特征工程" class="headerlink" title="4.4 特征工程"></a>4.4 特征工程</h3><p>经过对数据的概要分析，我们知道，营销数据中除了数值型的字段(age、duration、previous、empvarrate)，还有一些包含分类值的字符型字段(job、marital、default、housing、poutcome、loan)。本步骤就要利用特征工程,对分类字段进行特征转换，使用 spark 提供的StringIndex和OneHotEncoder算法。</p>
<h4 id="4-4-1-新建一个Scala-Object"><a href="#4-4-1-新建一个Scala-Object" class="headerlink" title="4.4.1 新建一个Scala Object"></a>4.4.1 新建一个Scala Object</h4><p>为了方便测试，我们在项目中在新建一个Scala Object：LogisiticRegressionTest2， main 方法中编写如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">//实例化SparkSession对象</span><br><span class="line">val spark=SparkSession.builder().appName(&#x27;Logistic_Prediction&#x27;).master(&#x27;local&#x27;).getOrCreate()</span><br><span class="line">//设置日志级别，减少日志输出，便于查看运行结果</span><br><span class="line">spark.sparkContext.setLogLevel(&#x27;WARN&#x27;)</span><br><span class="line">//导入隐式转换包，方便使用转换函数</span><br><span class="line">import spark.implicits._</span><br><span class="line">//读取数据，传入数据路径/opt/train/bank_marketing_data.csv</span><br><span class="line">val bank_Marketing_Data=spark.read</span><br><span class="line">                             .option(&#x27;header&#x27;, true)</span><br><span class="line">                             .option(&#x27;inferSchema&#x27;, &#x27;true&#x27;)   .csv(&#x27;/bank_marketing_data.csv&#x27;)这里是自己本地的地址</span><br><span class="line">//查看营销数据的前5条记录，包括所有字段</span><br><span class="line">bank_Marketing_Data.show(5)</span><br><span class="line">//读取营销数据指定的11个字段，并将age、duration、previous三个字段的类型从Integer类型转换为Double类型</span><br><span class="line">val selected_Data=bank_Marketing_Data.select(&#x27;age&#x27;,</span><br><span class="line">                                            &#x27;job&#x27;,</span><br><span class="line">                                            &#x27;marital&#x27;,</span><br><span class="line">                                            &#x27;default&#x27;,</span><br><span class="line">                                            &#x27;housing&#x27;,</span><br><span class="line">                                            &#x27;loan&#x27;,</span><br><span class="line">                                            &#x27;duration&#x27;,</span><br><span class="line">                                            &#x27;previous&#x27;,</span><br><span class="line">                                            &#x27;poutcome&#x27;,</span><br><span class="line">                                            &#x27;empvarrate&#x27;,</span><br><span class="line">                                            &#x27;y&#x27;)</span><br><span class="line">                                            .withColumn(&#x27;age&#x27;, bank_Marketing_Data(&#x27;age&#x27;).cast(DoubleType))</span><br><span class="line">                                            .withColumn(&#x27;duration&#x27;, bank_Marketing_Data(&#x27;duration&#x27;).cast(DoubleType))</span><br><span class="line">                                            .withColumn(&#x27;previous&#x27;, bank_Marketing_Data(&#x27;previous&#x27;).cast(DoubleType))</span><br><span class="line"></span><br><span class="line">val indexer = new StringIndexer().setInputCol(&#x27;job&#x27;).setOutputCol(&#x27;jobIndex&#x27;)</span><br><span class="line">val indexed = indexer.fit(selected_Data).transform(selected_Data)</span><br><span class="line">indexed.printSchema()</span><br><span class="line">indexed.show</span><br><span class="line">val encoder = new OneHotEncoder().setDropLast(false).setInputCol(&#x27;jobIndex&#x27;).setOutputCol(&#x27;jobVec&#x27;)</span><br><span class="line">val encoded = encoder.transform(indexed)</span><br><span class="line">encoded.show()</span><br><span class="line">encoded.printSchema()</span><br></pre></td></tr></table></figure>

<p>首先获取需要分析的数据字段，存入selected_Data变量，此步骤与上一个类LogisiticRegressionTest中的代码相同。 接着将对分类字段 job 使用 StringIndex 数据转换算法，将分类值转换为数值类型，输入列为 job,输出列为 jobIndex,为 OneHot 编码做准备。 然后对 jobIndex 字段使用 OneHotEncoder 数据转换算法，输入列为 StringIndex  数据转换算法的输出列，输出列为 jobVec。<br>输出结果如下：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809124431.png" alt="img"><br>输出结果中的内容分为两部分：数据内容和数据结构。<br>从上方的数据内容可以看出，对 job 列应用数据转换算法(StringIndexer 和 OneHotEncoder)，输出了新列：jobIndex 和 jobVec。  从下方的数据内容可以看出，jobIndex 列为 double 类型，jobVec 列为向量类型。<br>StringIndex 数据转换算法为数据增加了一列，列名为jobIndex,列类型为double,原始列job 并没有删除。由于原始列值不是数值型，无法参与向量运算，所以可以将对应的原始列删除；<br>OneHotEncoder 数据转换算法为数据增加了一列，列名为 jobVec,列类型为 Vector，原始列 jobIndex 并没有删除。<br>LogisiticRegressionTest2 类的完整代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.cstor.algorithm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DoubleType</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.StringIndexer</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.OneHotEncoder</span><br><span class="line"></span><br><span class="line">object LogisticRegressionTest2 &#123;</span><br><span class="line">   def <span class="title function_">main</span><span class="params">(args: Array[String])</span>: Unit = &#123;</span><br><span class="line">    val spark=SparkSession.builder().appName(<span class="string">&#x27;Logistic_Prediction&#x27;</span>).master(<span class="string">&#x27;local&#x27;</span>).getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val bank_Marketing_Data=spark.read</span><br><span class="line">                                 .option(<span class="string">&#x27;header&#x27;</span>, <span class="literal">true</span>)</span><br><span class="line">                                 .option(<span class="string">&#x27;inferSchema&#x27;</span>, <span class="string">&#x27;true&#x27;</span>)</span><br><span class="line">                                 .csv(<span class="string">&#x27;/bank_marketing_data.csv&#x27;</span>)这里是自己本地的地址</span><br><span class="line">    bank_Marketing_Data.show(<span class="number">5</span>)</span><br><span class="line">    val selected_Data=bank_Marketing_Data.select(<span class="string">&#x27;age&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;job&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;marital&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;default&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;housing&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;loan&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;duration&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;previous&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;poutcome&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;empvarrate&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">                                                .withColumn(<span class="string">&#x27;age&#x27;</span>, bank_Marketing_Data(<span class="string">&#x27;age&#x27;</span>).cast(DoubleType))</span><br><span class="line">                                                .withColumn(<span class="string">&#x27;duration&#x27;</span>, bank_Marketing_Data(<span class="string">&#x27;duration&#x27;</span>).cast(DoubleType))</span><br><span class="line">                                                .withColumn(<span class="string">&#x27;previous&#x27;</span>, bank_Marketing_Data(<span class="string">&#x27;previous&#x27;</span>).cast(DoubleType))</span><br><span class="line"></span><br><span class="line">    <span class="type">val</span> <span class="variable">indexer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;job&#x27;</span>).setOutputCol(<span class="string">&#x27;jobIndex&#x27;</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">indexed</span> <span class="operator">=</span> indexer.fit(selected_Data).transform(selected_Data)</span><br><span class="line">    indexed.printSchema()</span><br><span class="line">    indexed.show</span><br><span class="line">    <span class="type">val</span> <span class="variable">encoder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OneHotEncoder</span>().setDropLast(<span class="literal">false</span>).setInputCol(<span class="string">&#x27;jobIndex&#x27;</span>).setOutputCol(<span class="string">&#x27;jobVec&#x27;</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">encoded</span> <span class="operator">=</span> encoder.transform(indexed)</span><br><span class="line">    encoded.show()</span><br><span class="line">    encoded.printSchema()</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="4-4-3继续编写代码："><a href="#4-4-3继续编写代码：" class="headerlink" title="4.4.3继续编写代码："></a>4.4.3继续编写代码：</h4><p> 同理，对 marital、default、housing、poutcome、loan 等 5 个分类列进行 OneHot 编码,在 main 方法中已有代码后面，继续编写代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">val indexer = new StringIndexer().setInputCol(&#x27;job&#x27;).setOutputCol(&#x27;jobIndex&#x27;)</span><br><span class="line">val indexed = indexer.fit(selected_Data).transform(selected_Data)</span><br><span class="line">val encoder = new OneHotEncoder().setDropLast(false).setInputCol(&#x27;jobIndex&#x27;).setOutputCol(&#x27;jobVec&#x27;)</span><br><span class="line">val encoded = encoder.transform(indexed)</span><br><span class="line">//下面代码为新增代码</span><br><span class="line">val maritalIndexer=new StringIndexer().setInputCol(&#x27;marital&#x27;).setOutputCol(&#x27;maritalIndex&#x27;)</span><br><span class="line">//注意：此处所使用的数据是job列应用OneHotEncoder算法后产生的数据encoded</span><br><span class="line">//这里不能使用原始数据selected_Data，因为原始数据中没有jobVec列。</span><br><span class="line">val maritalIndexed=maritalIndexer.fit(encoded).transform(encoded)</span><br><span class="line">val maritalEncoder=new OneHotEncoder().setDropLast(false).setInputCol(&#x27;maritalIndex&#x27;).setOutputCol(&#x27;maritalVec&#x27;)</span><br><span class="line">val maritalEncoded=maritalEncoder.transform(maritalIndexed)</span><br><span class="line"></span><br><span class="line">val defaultIndexer=new StringIndexer().setInputCol(&#x27;default&#x27;).setOutputCol(&#x27;defaultIndex&#x27;)</span><br><span class="line">//注意：此处所使用的数据是对marital列应用OneHotEncoder算法后产生的数据maritalEncoded</span><br><span class="line">val defaultIndexed=defaultIndexer.fit(maritalEncoded).transform(maritalEncoded)</span><br><span class="line">val defaultEncoder=new OneHotEncoder().setDropLast(false).setInputCol(&#x27;defaultIndex&#x27;).setOutputCol(&#x27;defaultVec&#x27;)</span><br><span class="line">val defaultEncoded=defaultEncoder.transform(defaultIndexed)</span><br><span class="line"></span><br><span class="line">val housingIndexer=new StringIndexer().setInputCol(&#x27;housing&#x27;).setOutputCol(&#x27;housingIndex&#x27;)</span><br><span class="line">//注意：此处所使用的数据是对default列应用OneHotEncoder算法后产生的数据defaultEncoded</span><br><span class="line">val housingIndexed=housingIndexer.fit(defaultEncoded).transform(defaultEncoded)</span><br><span class="line">val housingEncoder=new OneHotEncoder().setDropLast(false).setInputCol(&#x27;housingIndex&#x27;).setOutputCol(&#x27;housingVec&#x27;)</span><br><span class="line">val housingEncoded=housingEncoder.transform(housingIndexed)</span><br><span class="line"></span><br><span class="line">val poutcomeIndexer=new StringIndexer().setInputCol(&#x27;poutcome&#x27;).setOutputCol(&#x27;poutcomeIndex&#x27;)</span><br><span class="line">//注意：此处所使用的数据是对housing列应用OneHotEncoder算法后产生的数据housingEncoded</span><br><span class="line">val poutcomeIndexed=poutcomeIndexer.fit(housingEncoded).transform(housingEncoded)</span><br><span class="line">val poutcomeEncoder=new OneHotEncoder().setDropLast(false).setInputCol(&#x27;poutcomeIndex&#x27;).setOutputCol(&#x27;poutcomeVec&#x27;)</span><br><span class="line">val poutcomeEncoded=poutcomeEncoder.transform(poutcomeIndexed)</span><br><span class="line"></span><br><span class="line">val loanIndexer=new StringIndexer().setInputCol(&#x27;loan&#x27;).setOutputCol(&#x27;loanIndex&#x27;)</span><br><span class="line">//注意：此处所使用的数据是对poutcome列应用OneHotEncoder算法后产生的数据poutcomeEncoded</span><br><span class="line">val loanIndexed=loanIndexer.fit(poutcomeEncoded).transform(poutcomeEncoded)</span><br><span class="line">val loanEncoder=new OneHotEncoder().setDropLast(false).setInputCol(&#x27;loanIndex&#x27;).setOutputCol(&#x27;loanVec&#x27;)</span><br><span class="line">val loanEncoded=loanEncoder.transform(loanIndexed)</span><br><span class="line">loanEncoded.show()</span><br><span class="line">loanEncoded.printSchema()</span><br></pre></td></tr></table></figure>

<p>上述代码对多个分类字段(marital、default、housing、poutcome、loan)使用了 StringIndex 和 OneHotEncoder 算法，代码执行流程可以用下图表示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809142994.png" alt="img"></p>
<p>上图初看较复杂，其实很简单！我们对数据进行特征转换，实质上是对数据中的某列进行特征转换，每次应用特征转换算法都会有一个输入列和一个输出列。上图中，菱形表示应用算法后，增加了输出列的数据，正方形表示算法，正方形前后的箭头表示输入列和输出列。转换的具体步骤如下：<br>selected_Data 是我们选择了 11 列后的数据，首先对 selected_Data 数据中的 job 数据进行 StringIndexer 转换，算法输出一个新列:jobIndex,对应代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val indexer = new StringIndexer().setInputCol(&#x27;job&#x27;).setOutputCol(&#x27;jobIndex&#x27;)</span><br><span class="line">val indexed = indexer.fit(selected_Data).transform(selected_Data)</span><br></pre></td></tr></table></figure>

<p>indexed 变量中的数据是包含了新列 jobIndex 的数据，原始列 job 的类型为字符型，<br>indexed  变量中的数据是包含了新列 jobIndex 的数据，原始列 job 的类型为字符型，新列 jobIndex 的类型为对应的数值型，将此列输入到  OneHotEncoder 算法中，进行 OneHot 编码，输出一个新列 jobVec，对应代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val encoder = new OneHotEncoder().setDropLast(false).setInputCol(&#x27;jobIndex&#x27;).setOutputCol(&#x27;jobVec&#x27;)</span><br><span class="line">val encoded = encoder.transform(indexed)</span><br></pre></td></tr></table></figure>

<p>encoded 变量中的数据是包含了新列 jobVec 的数据，接着用 encoded 数据中的 marital 列作为 StringIndexer 算法的输入列，算法输出一个新列 maritalIndex,对应代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val maritalIndexer=new StringIndexer().setInputCol(&#x27;marital&#x27;).setOutputCol(&#x27;maritalIndex&#x27;)</span><br><span class="line">val maritalIndexed=maritalIndexer.fit(encoded).transform(encoded)</span><br></pre></td></tr></table></figure>

<p>接着对 maritalIndexed 变量中的 maritalIndex 列进行 OneHotEncoder 编码，其过程与 job 列一样，以此类推……<br>最后对 loan 列进行特征转换，转换结果为 loanEncoded 数据，将 loanEncoded 数据中的内容和 schema 输出。<br>4.4.5 再次运行程序，查看输出结果：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809153652.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809160449.png" alt="img"><br>可以看出，marital、default、housing、poutcome、loan 这 5 个分类的字段增加情况和 job 字段一样，都增加了对应稀疏向量字段。<br>到目前为止，除了目标变量y，所有的字符型列都完成了 OneHot 编码，在对目标变量 y 进行数据转换后，就可以基于编码后的向量字段应用逻辑回归算法进行预测了。 在应用逻辑回归算法之前，我们使用 pipeline 技术对算法和数据进行流水线式组装，然后再应用逻辑回归算法。</p>
<h3 id="4-5-建立逻辑回归模型进行预测"><a href="#4-5-建立逻辑回归模型进行预测" class="headerlink" title="4.5 建立逻辑回归模型进行预测"></a>4.5 建立逻辑回归模型进行预测</h3><h4 id="4-5-1-继续编写代码："><a href="#4-5-1-继续编写代码：" class="headerlink" title="4.5.1 继续编写代码："></a>4.5.1 继续编写代码：</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//实例化一个向量组装器对象，</span></span><br><span class="line"><span class="comment">//将向量类型字段(&#x27;jobVec&#x27;,&#x27;maritalVec&#x27;, &#x27;defaultVec&#x27;,&#x27;housingVec&#x27;,&#x27;poutcomeVec&#x27;,&#x27;loanVec&#x27;)</span></span><br><span class="line"><span class="comment">//和数值型字段(&#x27;age&#x27;,&#x27;duration&#x27;,&#x27;previous&#x27;,&#x27;empvarrate&#x27;)</span></span><br><span class="line"><span class="comment">//形成一个新的字段:features，其中包含了所有的特征值</span></span><br><span class="line"><span class="type">val</span> <span class="variable">vectorAssembler</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">VectorAssembler</span>()</span><br><span class="line">.setInputCols(Array(<span class="string">&#x27;jobVec&#x27;</span>,<span class="string">&#x27;maritalVec&#x27;</span>, <span class="string">&#x27;defaultVec&#x27;</span>,<span class="string">&#x27;housingVec&#x27;</span>,<span class="string">&#x27;poutcomeVec&#x27;</span>,<span class="string">&#x27;loanVec&#x27;</span>,<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;duration&#x27;</span>,<span class="string">&#x27;previous&#x27;</span>,<span class="string">&#x27;empvarrate&#x27;</span>))</span><br><span class="line">.setOutputCol(<span class="string">&#x27;features&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//对目标变量进行StringIndexer特征转换,输出新列:label</span></span><br><span class="line"> <span class="type">val</span> <span class="variable">indexerY</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;y&#x27;</span>).setOutputCol(<span class="string">&#x27;label&#x27;</span>)</span><br><span class="line"> <span class="comment">//将特征算法按顺序进行合并，形成一个算法数组</span></span><br><span class="line"> val transformers=Array(indexer,</span><br><span class="line">                        encoder,</span><br><span class="line">                        maritalIndexer,</span><br><span class="line">                        maritalEncoder,</span><br><span class="line">                        defaultIndexer,</span><br><span class="line">                        defaultEncoder,</span><br><span class="line">                        housingIndexer,</span><br><span class="line">                        housingEncoder,</span><br><span class="line">                        poutcomeIndexer,</span><br><span class="line">                        poutcomeEncoder,</span><br><span class="line">                        loanIndexer,</span><br><span class="line">                        loanEncoder,</span><br><span class="line">                        vectorAssembler,</span><br><span class="line">                        indexerY);</span><br><span class="line"><span class="comment">//将原始数据selected_Data进行8-2分，80%用于训练数据。20%用于测试数据，评估训练模型的精确度。</span></span><br><span class="line"><span class="type">val</span> <span class="variable">splits</span> <span class="operator">=</span> selected_Data.randomSplit(Array(<span class="number">0.8</span>,<span class="number">0.2</span>))</span><br><span class="line"><span class="type">val</span> <span class="variable">training</span> <span class="operator">=</span> splits(<span class="number">0</span>).cache()</span><br><span class="line"><span class="type">val</span> <span class="variable">test</span> <span class="operator">=</span> splits(<span class="number">1</span>).cache()</span><br><span class="line"><span class="comment">//实例化逻辑回归算法</span></span><br><span class="line"><span class="type">val</span> <span class="variable">lr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogisticRegression</span>()</span><br><span class="line"><span class="comment">//将算法数组和逻辑回归算法合并,传入pipeline对象的stages中，然后作用于训练数据，训练模型</span></span><br><span class="line"><span class="type">var</span> <span class="variable">model</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Pipeline</span>().setStages(transformers :+ lr).fit(training)</span><br><span class="line"><span class="comment">//将上一步的训练模型作用于测试数据，返回测试结果</span></span><br><span class="line"><span class="type">var</span> <span class="variable">result</span> <span class="operator">=</span> model.transform(test)</span><br><span class="line"><span class="comment">//显示测试结果集中的真实值、预测值、原始值、百分比字段</span></span><br><span class="line">result.select(<span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;prediction&#x27;</span>,<span class="string">&#x27;rawPrediction&#x27;</span>,<span class="string">&#x27;probability&#x27;</span>).show(<span class="number">10</span>,<span class="literal">false</span>)</span><br><span class="line"><span class="comment">//创建二分类算法评估器，对测试结果进行评估</span></span><br><span class="line"><span class="type">val</span> <span class="variable">evaluator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryClassificationEvaluator</span>()</span><br><span class="line"><span class="type">var</span> <span class="variable">aucTraining</span> <span class="operator">=</span> evaluator.evaluate(result)</span><br><span class="line">println(<span class="string">&#x27;aucTraining = &#x27;</span>+aucTraining)</span><br></pre></td></tr></table></figure>

<h4 id="4-5-2-再次运行程序，查看运行结果："><a href="#4-5-2-再次运行程序，查看运行结果：" class="headerlink" title="4.5.2 再次运行程序，查看运行结果："></a>4.5.2 再次运行程序，查看运行结果：</h4><p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677809179446.png" alt="img"><br>从运行结果中可以看出，输出的测试结果前 10 条数据，其中 probability 列值是[0,1]对应的发生概率，如果某个分类值出现的概率大于 0.5，那么预测值  prediction 就是对应的那个分类值。 对 20%测试集的预测精确度为：0.9136000394632664，即：精确度为 90%  以上，说明我们训练的模型，预测的准确度还是很高的。注意：每次运行的精确度不会完全一样，有稍微的差别。<br>截至此操作完整代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.cstor.algorithm</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.SparkSession</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.DoubleType</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.StringIndexer</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.OneHotEncoder</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.classification.LogisticRegression</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.Pipeline</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.evaluation.BinaryClassificationEvaluator</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.ml.feature.VectorAssembler</span><br><span class="line"></span><br><span class="line">object LogisticRegressionTest2 &#123;</span><br><span class="line">   def <span class="title function_">main</span><span class="params">(args: Array[String])</span>: Unit = &#123;</span><br><span class="line">    val spark=SparkSession.builder().appName(<span class="string">&#x27;Logistic_Prediction&#x27;</span>).master(<span class="string">&#x27;local&#x27;</span>).getOrCreate()</span><br><span class="line">    spark.sparkContext.setLogLevel(<span class="string">&#x27;WARN&#x27;</span>)</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    val bank_Marketing_Data=spark.read</span><br><span class="line">                                 .option(<span class="string">&#x27;header&#x27;</span>, <span class="literal">true</span>)</span><br><span class="line">                                 .option(<span class="string">&#x27;inferSchema&#x27;</span>, <span class="string">&#x27;true&#x27;</span>)</span><br><span class="line">                                 .csv(<span class="string">&#x27;/bank_marketing_data.csv&#x27;</span>)这里是自己本地的地址</span><br><span class="line">    bank_Marketing_Data.show(<span class="number">5</span>)</span><br><span class="line">    val selected_Data=bank_Marketing_Data.select(<span class="string">&#x27;age&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;job&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;marital&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;default&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;housing&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;loan&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;duration&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;previous&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;poutcome&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;empvarrate&#x27;</span>,</span><br><span class="line">                                                <span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">                                                .withColumn(<span class="string">&#x27;age&#x27;</span>, bank_Marketing_Data(<span class="string">&#x27;age&#x27;</span>).cast(DoubleType))</span><br><span class="line">                                                .withColumn(<span class="string">&#x27;duration&#x27;</span>, bank_Marketing_Data(<span class="string">&#x27;duration&#x27;</span>).cast(DoubleType))</span><br><span class="line">                                                .withColumn(<span class="string">&#x27;previous&#x27;</span>, bank_Marketing_Data(<span class="string">&#x27;previous&#x27;</span>).cast(DoubleType))</span><br><span class="line"></span><br><span class="line">    <span class="type">val</span> <span class="variable">indexer</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;job&#x27;</span>).setOutputCol(<span class="string">&#x27;jobIndex&#x27;</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">indexed</span> <span class="operator">=</span> indexer.fit(selected_Data).transform(selected_Data)</span><br><span class="line">    indexed.printSchema()</span><br><span class="line">    indexed.show</span><br><span class="line">    <span class="type">val</span> <span class="variable">encoder</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">OneHotEncoder</span>().setDropLast(<span class="literal">false</span>).setInputCol(<span class="string">&#x27;jobIndex&#x27;</span>).setOutputCol(<span class="string">&#x27;jobVec&#x27;</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">encoded</span> <span class="operator">=</span> encoder.transform(indexed)</span><br><span class="line">    encoded.show()</span><br><span class="line">    encoded.printSchema()</span><br><span class="line">    val maritalIndexer=<span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;marital&#x27;</span>).setOutputCol(<span class="string">&#x27;maritalIndex&#x27;</span>)</span><br><span class="line">    val maritalIndexed=maritalIndexer.fit(encoded).transform(encoded)</span><br><span class="line">    val maritalEncoder=<span class="keyword">new</span> <span class="title class_">OneHotEncoder</span>().setDropLast(<span class="literal">false</span>).setInputCol(<span class="string">&#x27;maritalIndex&#x27;</span>).setOutputCol(<span class="string">&#x27;maritalVec&#x27;</span>)</span><br><span class="line">    val maritalEncoded=maritalEncoder.transform(maritalIndexed)</span><br><span class="line"></span><br><span class="line">    val defaultIndexer=<span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;default&#x27;</span>).setOutputCol(<span class="string">&#x27;defaultIndex&#x27;</span>)</span><br><span class="line">    val defaultIndexed=defaultIndexer.fit(maritalEncoded).transform(maritalEncoded)</span><br><span class="line">    val defaultEncoder=<span class="keyword">new</span> <span class="title class_">OneHotEncoder</span>().setDropLast(<span class="literal">false</span>).setInputCol(<span class="string">&#x27;defaultIndex&#x27;</span>).setOutputCol(<span class="string">&#x27;defaultVec&#x27;</span>)</span><br><span class="line">    val defaultEncoded=defaultEncoder.transform(defaultIndexed)</span><br><span class="line"></span><br><span class="line">    val housingIndexer=<span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;housing&#x27;</span>).setOutputCol(<span class="string">&#x27;housingIndex&#x27;</span>)</span><br><span class="line">    val housingIndexed=housingIndexer.fit(defaultEncoded).transform(defaultEncoded)</span><br><span class="line">    val housingEncoder=<span class="keyword">new</span> <span class="title class_">OneHotEncoder</span>().setDropLast(<span class="literal">false</span>).setInputCol(<span class="string">&#x27;housingIndex&#x27;</span>).setOutputCol(<span class="string">&#x27;housingVec&#x27;</span>)</span><br><span class="line">    val housingEncoded=housingEncoder.transform(housingIndexed)</span><br><span class="line"></span><br><span class="line">    val poutcomeIndexer=<span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;poutcome&#x27;</span>).setOutputCol(<span class="string">&#x27;poutcomeIndex&#x27;</span>)</span><br><span class="line">    val poutcomeIndexed=poutcomeIndexer.fit(housingEncoded).transform(housingEncoded)</span><br><span class="line">    val poutcomeEncoder=<span class="keyword">new</span> <span class="title class_">OneHotEncoder</span>().setDropLast(<span class="literal">false</span>).setInputCol(<span class="string">&#x27;poutcomeIndex&#x27;</span>).setOutputCol(<span class="string">&#x27;poutcomeVec&#x27;</span>)</span><br><span class="line">    val poutcomeEncoded=poutcomeEncoder.transform(poutcomeIndexed)</span><br><span class="line"></span><br><span class="line">    val loanIndexer=<span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;loan&#x27;</span>).setOutputCol(<span class="string">&#x27;loanIndex&#x27;</span>)</span><br><span class="line">    val loanIndexed=loanIndexer.fit(poutcomeEncoded).transform(poutcomeEncoded)</span><br><span class="line">    val loanEncoder=<span class="keyword">new</span> <span class="title class_">OneHotEncoder</span>().setDropLast(<span class="literal">false</span>).setInputCol(<span class="string">&#x27;loanIndex&#x27;</span>).setOutputCol(<span class="string">&#x27;loanVec&#x27;</span>)</span><br><span class="line">    val loanEncoded=loanEncoder.transform(loanIndexed)</span><br><span class="line">    loanEncoded.show()</span><br><span class="line">    loanEncoded.printSchema()</span><br><span class="line"></span><br><span class="line">    <span class="type">val</span> <span class="variable">vectorAssembler</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">VectorAssembler</span>()</span><br><span class="line">    .setInputCols(Array(<span class="string">&#x27;jobVec&#x27;</span>,<span class="string">&#x27;maritalVec&#x27;</span>, <span class="string">&#x27;defaultVec&#x27;</span>,<span class="string">&#x27;housingVec&#x27;</span>,<span class="string">&#x27;poutcomeVec&#x27;</span>,<span class="string">&#x27;loanVec&#x27;</span>,<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;duration&#x27;</span>,<span class="string">&#x27;previous&#x27;</span>,<span class="string">&#x27;empvarrate&#x27;</span>))</span><br><span class="line">    .setOutputCol(<span class="string">&#x27;features&#x27;</span>)</span><br><span class="line"></span><br><span class="line">     <span class="type">val</span> <span class="variable">indexerY</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">StringIndexer</span>().setInputCol(<span class="string">&#x27;y&#x27;</span>).setOutputCol(<span class="string">&#x27;label&#x27;</span>)</span><br><span class="line">     val transformers=Array(indexer,</span><br><span class="line">                            encoder,</span><br><span class="line">                            maritalIndexer,</span><br><span class="line">                            maritalEncoder,</span><br><span class="line">                            defaultIndexer,</span><br><span class="line">                            defaultEncoder,</span><br><span class="line">                            housingIndexer,</span><br><span class="line">                            housingEncoder,</span><br><span class="line">                            poutcomeIndexer,</span><br><span class="line">                            poutcomeEncoder,</span><br><span class="line">                            loanIndexer,</span><br><span class="line">                            loanEncoder,</span><br><span class="line">                            vectorAssembler,</span><br><span class="line">                            indexerY);</span><br><span class="line">    <span class="type">val</span> <span class="variable">splits</span> <span class="operator">=</span> selected_Data.randomSplit(Array(<span class="number">0.8</span>,<span class="number">0.2</span>))</span><br><span class="line">    <span class="type">val</span> <span class="variable">training</span> <span class="operator">=</span> splits(<span class="number">0</span>).cache()</span><br><span class="line">    <span class="type">val</span> <span class="variable">test</span> <span class="operator">=</span> splits(<span class="number">1</span>).cache()</span><br><span class="line">    <span class="type">val</span> <span class="variable">lr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogisticRegression</span>()</span><br><span class="line">    <span class="type">var</span> <span class="variable">model</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Pipeline</span>().setStages(transformers :+ lr).fit(training)</span><br><span class="line">    <span class="type">var</span> <span class="variable">result</span> <span class="operator">=</span> model.transform(test)</span><br><span class="line">    result.select(<span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;prediction&#x27;</span>,<span class="string">&#x27;rawPrediction&#x27;</span>,<span class="string">&#x27;probability&#x27;</span>).show(<span class="number">10</span>,<span class="literal">false</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">evaluator</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BinaryClassificationEvaluator</span>()</span><br><span class="line">    <span class="type">var</span> <span class="variable">aucTraining</span> <span class="operator">=</span> evaluator.evaluate(result)</span><br><span class="line">    println(<span class="string">&#x27;aucTraining = &#x27;</span>+aucTraining)</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="35-Spark实验：D3-js分析航班大数据"><a href="#35-Spark实验：D3-js分析航班大数据" class="headerlink" title="35.Spark实验：D3.js分析航班大数据"></a>35.Spark实验：D3.js分析航班大数据</h2><blockquote>
<h3 id="目的-14"><a href="#目的-14" class="headerlink" title="目的"></a>目的</h3><p>在本课程中，我们将通过 Spark 提供的 DataFrame、 SQL 和机器学习框架等工具，对航班起降的记录数据进行分析，尝试找出造成航班延误的原因，以及对航班延误情况进行预测。</p>
<h3 id="要求-14"><a href="#要求-14" class="headerlink" title="要求"></a>要求</h3><p>1本次试验后，要求学生能掌握Spark DataFrame 操作、Spark SQL 常用操作、Spark MLlib 机器学习框架使用。</p>
<h3 id="原理-14"><a href="#原理-14" class="headerlink" title="原理"></a>原理</h3><p>“我们很抱歉地通知您，您乘坐的由 XX 飞往 XX 的 XXXX  航班延误。”相信很多在机场等待飞行的旅客都不愿意听到这句话。随着乘坐飞机这种交通方式的逐渐普及，航延延误问题也一直困扰着我们。航班延误通常会造成两种结果，一种是航班取消，另一种是航班晚点 。我们将通过 Spark 提供的 DataFrame、 SQL  和机器学习框架等工具，对航班起降的记录数据进行分析，尝试找出造成航班延误的原因，以及对航班延误情况进行预测。<br>在学习过程中，建议手边能够准备纸和笔做相应的记录。代码写起来是非常快的，然而更重要的是如何通过思考去设计这些代码。因此我们会有大量的工作在书写伪代码和记录相关的字段上。这也是数据分析工作中常见的一个习惯。</p>
</blockquote>
<h3 id="4-1数据集简介及准备"><a href="#4-1数据集简介及准备" class="headerlink" title="4.1数据集简介及准备"></a>4.1数据集简介及准备</h3><h4 id="4-1-1数据集简介"><a href="#4-1-1数据集简介" class="headerlink" title="4.1.1数据集简介"></a>4.1.1数据集简介</h4><p>本节实验用到的航班数据集仍然是 2008年 Data Expo 上提供的飞行准点率统计数据。该数据集的各个字段解释如下：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430978186.png" alt="img"><br>此外，我们还会用到一些补充信息。如机场信息数据集等。</p>
<h4 id="4-1-2-获取数据集"><a href="#4-1-2-获取数据集" class="headerlink" title="4.1.2 获取数据集"></a>4.1.2 获取数据集</h4><p>本次实验要用到的数据在一下路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/data/spark/flight/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430988688.png" alt="img"><br>我们先将airports.csv文件用WinSCP下载到本地。</p>
<h3 id="4-2-数据清洗"><a href="#4-2-数据清洗" class="headerlink" title="4.2 数据清洗"></a>4.2 数据清洗</h3><p>由于 airports  数据集中含有一些非常用字符，我们需要对其进行清洗处理，以防止部分记录字符的不能被识别错误引起后续检索的错误。OpenRefine 是  Google 主导开发的一款开源数据清洗工具（路径&#x2F;root&#x2F;data&#x2F;spark下有压缩包）。我们下载安装它：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430997390.png" alt="img"><br>双击运行OpenRefine，会出现以下页面，并会在浏览器中会出现 OpenRefine 的应用网页，如下图所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431004372.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431011267.png" alt="img"><br>我们先修改应用的语言，来方便操作，再选择刚刚下载的机场信息数据集，并点击 下一步 按钮进入下一步。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431019405.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431026041.png" alt="img"><br>在数据解析步骤中，直接点击右上角的 新建项目 按钮创建数据清洗项目。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431033715.png" alt="img"><br>稍作等待，项目创建完成后，就可以对数据进行各种操作。点击 airport 列旁边的下拉菜单按钮，然后在菜单中选择 编辑列 -&gt; 移除该列 选项，以移除 airport 列。具体操作如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431042081.png" alt="img"><br>请按照同样的方法，移除 lat 和 long 列。最后的数据集应只包含 iata 、city、state、country 四列。<br>最后我们点击右上角的 Export 按钮导出数据集。导出选项选择 以逗号分隔的值，即 CSV 文件。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431052461.png" alt="img"><br>清洗完毕之后，我们将文件导出，然后我们用Winscp上传到我们的集群中去，最后关闭浏览器和运行着 OpenRefine 的终端即可。</p>
<h3 id="4-3-启动-Spark-Shell"><a href="#4-3-启动-Spark-Shell" class="headerlink" title="4.3 启动 Spark Shell"></a>4.3 启动 Spark Shell</h3><p>为了更好地处理 CSV 格式的数据集，我们可以直接使用由 DataBricks 公司提供的第三方 Spark CSV 解析库来读取。<br>首先现在hdfs上建立&#x2F;root文件夹</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -mkdir /root</span><br></pre></td></tr></table></figure>

<p>然后是启动 Spark Shell。在启动的同时，附上参数–packages com.databricks:spark-csv_2.11:1.1.0<br>请在终端中输入以下代码。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431064091.png" alt="img"><br>**注意：**该操作需要联网权限。如果遇到网络访问较慢，或者是您当前不具备访问互联网的权限时，请参考文末的常见问题“无法访问外网时，应如何通过加载 CSV 解析库的方式进入 Spark Shell ”，问题解答中提供了解决方案。</p>
<h3 id="4-4导入数据及处理格式"><a href="#4-4导入数据及处理格式" class="headerlink" title="4.4导入数据及处理格式"></a>4.4导入数据及处理格式</h3><p>等待 Spark Shell 启动完成后，输入以下命令来导入数据集。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line">val flightData = sqlContext.read.format(&#x27;com.databricks.spark.csv&#x27;).option(&#x27;header&#x27;,&#x27;true&#x27;).load(&#x27;file:///root/data/spark/flight/2008.csv&#x27;)</span><br></pre></td></tr></table></figure>

<p>在上述命令中，我们调用了 sqlContext 提供的 read 接口，指定加载格式 format 为第三方库中定义的格式  com.databricks.spark.csv 。同时设置了一个读取选项 header 为  true，这表示将数据集中的首行内容解析为字段名称。最后在 load 方法中 指明了待读取的数据集文件为我们刚刚下载的这个数据集。<br>执行结果如下图所示。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431073101.png" alt="img"><br>此时， flightData 的数据类型为 Spark SQL 中常用的 DataFrame。<br>接着将 flightData 其注册为临时表，命令为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flightData.registerTempTable(&#x27;flights&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431083276.png" alt="img"><br>使用相同的方法导入机场信息数据集 airports.csv ，并将其注册为临时表。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val airportData = sqlContext.read.format(&#x27;com.databricks.spark.csv&#x27;).option(&#x27;header&#x27;,&#x27;true&#x27;).load(&#x27;file:///root/airports-csv.csv&#x27;)</span><br><span class="line">airportData.registerTempTable(&#x27;airports&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431091938.png" alt="img"><br>稍后我们将基于这些临时表来做一些 SQL 查询。<br>在探索数据之前，我们已经知道该数据共有 29 个字段。根据出发时间、出发 &#x2F; 抵达延误时间等信息，我们可以大胆地提出下面这些问题：</p>
<h4 id="4-4-1每天航班最繁忙的时间段是哪些"><a href="#4-4-1每天航班最繁忙的时间段是哪些" class="headerlink" title="4.4.1每天航班最繁忙的时间段是哪些"></a>4.4.1每天航班最繁忙的时间段是哪些</h4><p>分析某个问题时，要想办法将解答问题的来源落实在数据集的各个指标上。当问题不够详细时，可以取一些具有代表性的值作为该问题的答案。<br>例如，航班分为到港（Arrive）和离港（Depart）航班，若统计所有机场在每天的某个时间段内离港航班数量，就能在一定程序上反映这个时段的航班是否繁忙。<br>数据集中的每一条记录都朴实地反映了航班的基本情况，但它们并不会直接告诉我们每一天、每一个时段都发生了什么。为了得到后者这样的信息，我们需要对数据进行筛选和统计。<br>于是我们会顺理成章地用到 AVG（平均值）、COUNT（计数）和 SUM（求和）等统计函数。为了分时间段统计航班数量，我们可以大致地将一天的时间分为以下五段：<br>凌晨（00:00 - 06:00）：大部分人在这个时段都在休息，所以我们可以合理假设该时间段内航班数量较少。<br>早上（06:01 - 10:00）：一些早班机会选择在此时间出发，机场也通常从这个时间段起逐渐进入高峰。<br>中午（10:01 - 14:00）：早上从居住地出发的人们通常在这个时候方便抵达机场，因此选择在该时间段出发的航班可能更多。<br>下午（14:01 - 19:00）：同样，在下午出发更为方便，抵达目的地是刚好是晚上，又不至于太晚，方便找到落脚之处。<br>晚上（19:01 - 23:59）：在一天结束之际，接近凌晨的航班数量可能会更少。</p>
<p>这样的分段都是基于一些假设的。如果你认为你有更合理的分段方式，不妨将它们用到后续的分析工作中。不要忘了在草稿纸上记录下这些重要的时间段，在设计代码时会用到它们。<br>当我们所需的数据不是单个离散的数据而是基于一定范围的时候，我们可以用关键字 BETWEEN x AND y 来设置数据的起止范围。<br>有了上述准备，我们可以尝试写出统计离港时间在 0 点 至 6 点 间的航班总数。首先选取的目标是 flights 这张表，即 FROM flights。航班总数可以对 FlightNum  进行统计（使用 COUNT 函数），即 COUNT(FlightNum)。限定的条件是离港时间在 0 （代表 00:00）至 600 （代表  6:00）之间，即 WHERE DepTime BETWEEN 0 AND 600。所以我们要写出的语句是：</p>
<p>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val queryFlightNumResult = sqlContext.sql(&#x27;SELECT COUNT(FlightNum) FROM flights WHERE DepTime BETWEEN 0 AND 600&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431101171.png" alt="img"><br>查看其中 1 条结果。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤比较耗时，请耐心等待计算完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">queryFlightNumResult.take(1)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431107843.png" alt="img"></p>
<p>在此基础上我们可以细化一下，计算出每天的平均离港航班数量，并且每次只选择 1 个月的数据。这里我们选择的时间段为 10:00 至 14:00 。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// COUNT(DISTINCT DayofMonth) 的作用是计算每个月的天数</span><br><span class="line">val queryFlightNumResult1 = sqlContext.sql(&#x27;SELECT COUNT(FlightNum)/COUNT(DISTINCT DayofMonth) FROM flights WHERE Month = 1 AND DepTime BETWEEN 1001 AND 1400&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431118414.png" alt="img"><br>查询得到的结果只有一条，即该月每天的平均离港航班数量。查看一下：<br>请在 Spark Shell 中输入以下代码。 注意：此步骤比较耗时，请耐心等待计算完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">queryFlightNumResult1.take(1)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431126774.png" alt="img"><br>你可以尝试计算出其他时间段的平均离港航班数量，并作记录。</p>
<h4 id="4-4-2-飞哪最准时"><a href="#4-4-2-飞哪最准时" class="headerlink" title="4.4.2 飞哪最准时"></a>4.4.2 飞哪最准时</h4><p>要看飞哪最准时，实际上就是统计航班到港准点率。可以先来查询到港延误时间为 0 的航班都是飞往哪里的。这句话中，有几个信息：<br>要查询的主要信息为目的地代码。<br>信息的来源为 flights 表。<br>查询的条件为到港延误时间（ArrDelay）为 0 。<br>在面对任何一个问题时，我们都可以仿照上面的思路对问题进行拆解，然后将每一条信息转化为对应的 SQL 语句。<br>请尝试根据已提供的信息，完成 SQL 语句的设计。<br>于是最终我们可以得到这样的查询代码：<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val queryDestResult = sqlContext.sql(&#x27;SELECT DISTINCT Dest, ArrDelay FROM flights WHERE ArrDelay = 0&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431135401.png" alt="img"><br>取出其中 5 条结果来看看。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤比较耗时，请耐心等待计算完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">queryDestResult.head(5)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431145540.png" alt="img"><br>在此基础上，我们尝试加入更多的限定条件。<br>我们可以统计出到港航班延误时间为 0 的次数（准点次数），并且最终输出的结果为 [目的地， 准点次数] ，并且按照降序对它们进行排列。这一次我们不再给出信息拆分的提示，请尝试自己完成该步骤，然后与下方的代码进行比对。<br>最后查询的代码为：<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val queryDestResult2 = sqlContext.sql(&#x27;SELECT DISTINCT Dest, COUNT(ArrDelay) AS delayTimes FROM flights where ArrDelay = 0 GROUP BY Dest ORDER BY delayTimes DESC&#x27;)</span><br></pre></td></tr></table></figure>

<p>DISTINCT 关键字的作用是去除重复的结果。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431167450.png" alt="img"><br>查看其中 10 条结果。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤比较耗时，请耐心等待计算完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">queryDestResult2.head(10)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431175649.png" alt="img"><br>在美国，一个州通常会有多个机场。我们在上一步得到的查询结果都是按照目的地的机场代码进行输出的。那么抽象到每一个州都有多少个准点的到港航班呢？<br>我们可以在上一次查询的基础上，再次进行嵌套的查询。并且，我们会用到另一个数据集 airports 中的信息：目的地中的三字代码（Dest）即该数据集中的 IATA  代码（iata），而每个机场都给出了它所在的州的信息（state）。我们可以通过一个联结操作将 airports 表加入到查询中。<br>此处直接给出查询的代码，请尝试理解每一段关键字的作用都是什么。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val queryDestResult3 = sqlContext.sql(&#x27;SELECT DISTINCT state, SUM(delayTimes) AS s FROM (SELECT DISTINCT Dest, COUNT(ArrDelay) AS delayTimes FROM flights WHERE ArrDelay = 0 GROUP BY Dest ) a JOIN airports b ON a.Dest = b.iata GROUP BY state ORDER BY s DESC&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431188575.png" alt="img"><br>查看其中 10 条结果。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤比较耗时，请耐心等待计算完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">queryDestResult3.head(10)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431219359.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431213174.png" alt="img"><br>最后还可以将结果输出为 CSV 格式，保存在用户主目录下。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤比较耗时，请耐心等待计算完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// QueryDestResult.csv只是保存结果的文件夹名</span><br><span class="line">queryDestResult3.rdd.saveAsTextFile(&#x27;/root/QueryDestResult.csv&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431229539.png" alt="img"><br>保存完毕后，我们还需要手动将文件从HDFS上下载到本地，然后我们将其合并为一个文件。新打开一个终端，在终端中输入以下命令来进行文件合并。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431236521.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 进入到结果文件的目录</span><br><span class="line">cd ~/QueryDestResult.csv/</span><br><span class="line"></span><br><span class="line"># 使用通配符将每个part文件中的内容追加到 result.csv 文件中</span><br><span class="line">cat part-* &gt;&gt; result.csv</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431245123.png" alt="img"><br>最后打开 result.csv 文件就能看到最终结果，如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431252486.png" alt="img"><br>请注意，在这一小节中，我们只是统计了每个州的到港准点航班数量。请仿照上面的思路，尝试自己动手计算每个州的航班准点率。<br>提示：到港航班准点率 &#x3D; 到港准点航班数量 &#x2F; 到港航班总数<br>那么，到港航班准点率最高的州是哪个呢？</p>
<h4 id="4-4-3-出发延误的重灾区都有哪些"><a href="#4-4-3-出发延误的重灾区都有哪些" class="headerlink" title="4.4.3 出发延误的重灾区都有哪些"></a>4.4.3 出发延误的重灾区都有哪些</h4><p>解决问题的方式似乎变得越来越简单了。我们继续回答下一个问题：出发延误的重灾区都有哪些？<br>可以大胆地设置查询条件为离港延误时间大于 60 分钟，查询语句从来都不是一次就写好的，尝试多次修改关键字和参数，以得到一个最优的结果。<br>写出查询语句如下：<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val queryOriginResult = sqlContext.sql(&#x27;SELECT DISTINCT Origin, DepDelay FROM flights where DepDelay &gt; 60 ORDER BY DepDelay DESC&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431264498.png" alt="img"><br>因为数据已经按照降序的形式进行排列，所以我们取出前 10 个查询结果即为 2008 年内，延误最严重的十次航班及所在的离港机场。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤比较耗时，请耐心等待计算完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">queryOriginResult.head(10)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431272608.png" alt="img"><br>是时候完全依靠自己了。请你仿照解答“飞哪最准确”这个问题时的解答方式，联结 airports 表，查询哪些机场拥有最高的离港航班晚点率。<br>如果你有足够的能力，可以结合 airports 表中每个机场的经纬度（lat 和 long 字段）和相关的气象数据，得到某些机场是否是因为常年的气象原因（如频繁的飓风、雷暴等)导致了经常发生延误。</p>
<h3 id="4-5-航班延误时间预测"><a href="#4-5-航班延误时间预测" class="headerlink" title="4.5 航班延误时间预测"></a>4.5 航班延误时间预测</h3><h4 id="4-5-1-引入相关的包"><a href="#4-5-1-引入相关的包" class="headerlink" title="4.5.1 引入相关的包"></a>4.5.1 引入相关的包</h4><p>为了使用 Spark ML 的相关功能，我们需要引入下面这些包：<br>请在 Spark Shell 中输入下面的代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark._</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">import org.apache.spark.mllib.util.MLUtils</span><br><span class="line">import org.apache.spark.mllib.linalg.Vectors</span><br><span class="line">import org.apache.spark.mllib.regression.LabeledPoint</span><br><span class="line">import org.apache.spark.mllib.tree.DecisionTree</span><br><span class="line">import org.apache.spark.mllib.tree.model.DecisionTreeModel</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431289106.png" alt="img"></p>
<h4 id="4-5-2-DataFrame-转换为-RDD"><a href="#4-5-2-DataFrame-转换为-RDD" class="headerlink" title="4.5.2 DataFrame 转换为 RDD"></a>4.5.2 DataFrame 转换为 RDD</h4><p>Spark ML 中的操作大部分是基于 RDD （分布式弹性数据集）来进行的。而之前我们读进来的数据集的数据类型为 DataFrame  。在 DataFrame 中的每一条记录即对应于 RDD 中的每一行值。为此，我们需要将 DataFrame 转换为 RDD。<br>首先数据从 DataFrame 类型转换为 RDD 类型。从 row 中取值时是按照数据集中各个字段取出 Flight 类中对应字段的值。例如排在第二的  row(3) 取出的是 DataFrame 中 DayofWeek 字段的值，对应的是 Flight 类中的 dayOfWeek 成员变量。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val tmpFlightDataRDD = flightData.map(row =&gt; row(2).toString+&#x27;,&#x27;+row(3).toString+&#x27;,&#x27;+row(5).toString+&#x27;,&#x27;+row(7).toString+&#x27;,&#x27;+row(8).toString+&#x27;,&#x27;+row(12).toString+&#x27;,&#x27;+ row(16).toString+&#x27;,&#x27;+row(17).toString+&#x27;,&#x27;+row(14).toString+&#x27;,&#x27;+row(15).toString).rdd</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431299737.png" alt="img"><br>接着需要建立一个类，将 RDD 中的部分字段映射到类的成员变量中。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">case class Flight(dayOfMonth:Int, dayOfWeek:Int, crsDepTime:Double, crsArrTime:Double, uniqueCarrier:String, crsElapsedTime:Double, origin:String, dest:String, arrDelay:Int, depDelay:Int, delayFlag:Int)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431309912.png" alt="img"><br>在类 Flight 中，最后一个成员变量为  delayFlag。通过对数据的观察，我们知道部分航班的延误时间仅仅为几分钟（无论是出发还是抵达时），而通常此类延误都是可以容忍的。为了减少待处理的数据量，我们可以将延误定义为出发或抵达的延迟时间大于半个小时（即 30 分钟），从而将抵达延误时间和出发延误时间简化为延误标记 delayFlag。<br>可以先尝试写出伪代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if ArrDelayTime or DepDelayTime &gt; 30</span><br><span class="line">    delayFlag = True</span><br><span class="line">else</span><br><span class="line">    delayFlag = False</span><br></pre></td></tr></table></figure>

<p>接着我们按照上述逻辑，定义一个解析方法。该方法用于将 DataFrame 中的记录转换为 RDD 。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">def parseFields(input: String): Flight = &#123;</span><br><span class="line">    val line = input.split(&#x27;,&#x27;)</span><br><span class="line"></span><br><span class="line">    // 针对可能出现的无效值“NA”进行过滤</span><br><span class="line">    var dayOfMonth = 0</span><br><span class="line">    if(line(0) != &#x27;NA&#x27;)&#123;</span><br><span class="line">        dayOfMonth = line(0).toInt</span><br><span class="line">    &#125;</span><br><span class="line">    var dayOfWeek = 0</span><br><span class="line">    if(line(1) != &#x27;NA&#x27;)&#123;</span><br><span class="line">        dayOfWeek = line(1).toInt</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    var crsDepTime = 0.0</span><br><span class="line">    if(line(2) != &#x27;NA&#x27;)&#123;</span><br><span class="line">        crsDepTime = line(2).toDouble</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    var crsArrTime = 0.0</span><br><span class="line">    if(line(3) != &#x27;NA&#x27;)&#123;</span><br><span class="line">        crsArrTime = line(3).toDouble</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    var crsElapsedTime = 0.0</span><br><span class="line">    if(line(5) != &#x27;NA&#x27;)&#123;</span><br><span class="line">        crsElapsedTime = line(5).toDouble</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    var arrDelay = 0</span><br><span class="line">    if(line(8) != &#x27;NA&#x27;)&#123;</span><br><span class="line">        arrDelay = line(8).toInt</span><br><span class="line">    &#125;</span><br><span class="line">    var depDelay = 0</span><br><span class="line">    if(line(9) != &#x27;NA&#x27;)&#123;</span><br><span class="line">        depDelay = line(9).toInt</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 根据延迟时间决定延迟标志是否为1</span><br><span class="line">    var delayFlag = 0</span><br><span class="line">    if(arrDelay &gt; 30 || depDelay &gt; 30)&#123;</span><br><span class="line">        delayFlag = 1</span><br><span class="line">    &#125;</span><br><span class="line">    Flight(dayOfMonth, dayOfWeek, crsDepTime, crsArrTime, line(4), crsElapsedTime, line(6), line(7), arrDelay, depDelay, delayFlag)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431323739.png" alt="img"><br>解析方法定义完成后，我们就是用 map 操作来解析 RDD 中的各个字段。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val flightRDD = tmpFlightDataRDD.map(parseFields)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431333930.png" alt="img"><br>可以尝试随机取出一个值检查解析是否成功。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flightRDD.take(1)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431350997.png" alt="img"></p>
<h4 id="4-5-3-提取特征"><a href="#4-5-3-提取特征" class="headerlink" title="4.5.3 提取特征"></a>4.5.3 提取特征</h4><p>为了建立分类模型，我们需要提取出航班数据的特征。在刚刚解析数据的一步中，我们设立 delayFlag  的目的就是为了定义两个类用于分类。因此你可以将其称之为标签（Label），这是分类中常用的一个手段。标签有两种，如果 delayFlag 为 1 ，则代表航班有延误；如果为 0 ，则代表没有延误。区分延误与否的标准正如之前所讨论的那样：抵达或出发的延误时间是否超过了 30 分钟。<br>对于数据集中的每条记录，现在它们都包含了标签和特征信息。特征则是每条记录在 Flight 类中对应的所有属性（从 dayOfMonth 一直到 delayFlag）。<br>下面，我们需要将上述的这些特征转换为数值特征。在 Flight 类中，有些属性已经是数值特征，而诸如 crsDepTime 和 uniqueCarrier  等属性还不是数值特征。在这一步中我们都要将它们转换为数值特征。例如，uniqueCarrier 这个特征通常是航空公司代码（ “WN”  等），我们按照先后顺序为它们进行编号，将字符串类型的特征转换为含有唯一 ID 的数值特征（如 “AA” 变成了 0，“AS” 变成了 1  ，以此类推，实际运算时是按照字母先后顺序进行标记的）。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var id: Int = 0</span><br><span class="line">var mCarrier: Map[String, Int] = Map()</span><br><span class="line">flightRDD.map(flight =&gt; flight.uniqueCarrier).distinct.collect.foreach(x =&gt; &#123;mCarrier += (x -&gt; id); id += 1&#125;)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431368181.png" alt="img"><br>计算完成后，我们来查看一下 carrier 中是否已经完成了从表示航空公司代码的字符串到对应的唯一 ID 之间的转换。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤比较耗时，请耐心等待计算完成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mCarrier.toString</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431377969.png" alt="img"><br>按照同样的逻辑，我们要为出发地 Origin 、目的地 Dest 进行字符串到数值的转换。<br>先是对 Origin 进行转换：<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">var id_1: Int = 0</span><br><span class="line">var mOrigin: Map[String, Int] = Map()</span><br><span class="line">// 这里的origin相当于一个“全局”变量，在每次map中我们都在对其进行修改</span><br><span class="line"></span><br><span class="line">flightRDD.map(flight =&gt; flight.origin).distinct.collect.foreach(x =&gt; &#123;mOrigin += (x -&gt; id_1); id_1 += 1&#125;)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431387879.png" alt="img"><br>最后是对 Dest 进行转换，不要忘了我们转换的目的是为了建立数值特征。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var id_2: Int = 0</span><br><span class="line">var mDest: Map[String, Int] = Map()</span><br><span class="line">flightRDD.map(flight =&gt; flight.dest).distinct.collect.foreach(x =&gt; &#123;mDest += (x -&gt; id_2); id_2 += 1&#125;)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431401022.png" alt="img"><br>至此我们就将所有的特征都准备好了。<br>4.5.4 定义特征数组<br>我们在上一步用不同的数字代表了不同的特征，这些特征最后都将放入数组中，可以将其理解为建立了特征向量。<br>接下来，我们将所有的标签（延迟与否）和特征都以数值的形式存储到一个新的 RDD 中，用作机器学习算法的输入。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">val featuredRDD = flightRDD.map(flight =&gt; &#123;</span><br><span class="line">  val vDayOfMonth = flight.dayOfMonth - 1</span><br><span class="line">  val vDayOfWeek = flight.dayOfWeek - 1</span><br><span class="line">  val vCRSDepTime = flight.crsDepTime</span><br><span class="line">  val vCRSArrTime = flight.crsArrTime</span><br><span class="line">  val vCarrierID = mCarrier(flight.uniqueCarrier)</span><br><span class="line">  val vCRSElapsedTime = flight.crsElapsedTime</span><br><span class="line">  val vOriginID = mOrigin(flight.origin)</span><br><span class="line">  val vDestID = mDest(flight.dest)</span><br><span class="line">  val vDelayFlag = flight.delayFlag</span><br><span class="line"></span><br><span class="line">  // 返回值中，将所有字段都转换成Double类型以利于建模时使用相关API</span><br><span class="line">  Array(vDelayFlag.toDouble, vDayOfMonth.toDouble, vDayOfWeek.toDouble, vCRSDepTime.toDouble, vCRSArrTime.toDouble, vCarrierID.toDouble, vCRSElapsedTime.toDouble, vOriginID.toDouble, vDestID.toDouble)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431410836.png" alt="img"><br>经历这个 map 阶段后，我们得到了包含所有信息的特征数组，并且这些特征都是数值类型的。<br>尝试取出其中一个值来查看转换是否成功。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">featuredRDD.take(1)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431420300.png" alt="img"></p>
<h4 id="4-5-5-创建标记点"><a href="#4-5-5-创建标记点" class="headerlink" title="4.5.5 创建标记点"></a>4.5.5 创建标记点</h4><p>此步骤中，我们需要将含有特征数组的 featuredRDD 转换为含有  org.apache.spark.mllib.regression.LabeledPoint 包中定义的标记点 LabeledPoints 的新 RDD 。在分类中，标记点含有两类信息，一是代表了数据点的标记，二是代表了特征向量类。<br>下面我们来完成这个转换。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// Label设定为 DelayFlag，Features设定为其他所有字段的值</span><br><span class="line">val LabeledRDD = featuredRDD.map(x =&gt; LabeledPoint(x(0), Vectors.dense(x(1), x(2), x(3), x(4), x(5), x(6), x(7), x(8))))</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431430491.png" alt="img"><br>尝试取出其中一个值来查看转换是否成功。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LabeledRDD.take(1)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431439213.png" alt="img"><br>回顾一下之前所做的工作：我们得到了含有延误标记 DelayFlag 的数据，所有的航班都可以被标记为延误了或者没有延误。下面我们会将上述数据使用随机划分的方法，划分为训练集和测试集。<br>以下是详细比例说明：<br>在 LabeledRDD 中，数据标记为 DelayFlag &#x3D; 0 的数据为未延迟航班；数据标记为 DelayFlag &#x3D; 1 的数据为已延迟航班。<br>未延迟航班总数的 80% ，将与所有的已延迟航班组成新的数据集。新数据集的 70% 和 30% 将被划分为训练集和测试集。<br>不直接使用 LabeledRDD 中的数据来划分训练集和测试集的目的是：尽可能提高已延迟航班在测试集中的比例，让训练得到的模型能更精确地描述延迟的情况。<br>因此，我们首先来提取 LabeledRDD 中的所有未延迟航班，再随机提取其中的 80% 。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 末尾的(0)是为了取这 80% 的部分</span><br><span class="line">val notDelayedFlights = LabeledRDD.filter(x =&gt; x.label == 0).randomSplit(Array(0.8, 0.2))(0)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431450640.png" alt="img"><br>接着我们提取所有的已延迟航班。<br>请在 Spark Shell 中输入以下代码。<br>val delayedFlights &#x3D; LabeledRDD.filter(x &#x3D;&gt; x.label &#x3D;&#x3D; 1)<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431459291.png" alt="img"><br>将上述二者组合成新的数据集，用于后续划分训练集和测试集。<br>请在 Spark Shell 中输入以下代码。<br>val tmpTTData &#x3D; notDelayedFlights ++ delayedFlights<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431467239.png" alt="img"><br>最后我们将这个数据集按照约定的比例随机划分为训练集和测试集。<br>请在 Spark Shell 中输入以下代码。<br>&#x2F;&#x2F; TT意为Train &amp; Test<br>val TTData &#x3D; tmpTTData.randomSplit(Array(0.7, 0.3))<br>val trainingData &#x3D; TTData(0)<br>val testData &#x3D; TTData(1)</p>
<p>执行过程如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431476440.png" alt="img"></p>
<h4 id="4-5-6-训练模型"><a href="#4-5-6-训练模型" class="headerlink" title="4.5.6 训练模型"></a>4.5.6 训练模型</h4><p>接下来，我们将会从训练集中提取特征（Feature）。这里会用到 Spark MLlib 中的决策树。决策树是一个预测模型，代表的是对象属性与对象值之间的一种映射关系。你可以在百度百科中详细了解决策树的原理。<br>希望在进行接下来的工作之前，你能够利用一些时间了解决策树，以便于更好地理解各项参数设置的含义。<br>在官方文档中，决策树的参数分为三类：<br>问题规格参数（Problem specification parameters）：这些参数描述了待求解问题和数据集。我们需要设置  categoricalFeaturesInfo这一项，它指明了哪些特征是已经明确的，以及这些特征都可以取多少明确的值。返回值是一个 Map  。例如 Map(0 -&gt; 2, 4 -&gt; 10) 表示特征 0 的取值有 2 个（0 和 1），特征 4 的取值有 10 个（从 0 到 9）。<br>停止准则（Stopping criteria）：这些参数决定了树的构造在什么时候停止（即停止添加新节点）。我们需要设置 maxDepth 这一项，它表示树的最大深度。更深的树可能更有表达力，但它也更难训练并且容易过拟合。<br>可调参数（Tunable parameters）：这些参数都是可选的。我们需要设置两个。第一个是 maxBins，表示离散连续特征时的桶信息数量。第二个是 impurity ，表示在选择候选分支时的杂质度。<br>我们要训练的模型是通过建立输入特征与已标记的输出间的联系。要用到的方法是决策树类 DecisionTree 自带的 trainClassifier 方法。通过使用该方法，我们能够得到一个决策树模型。<br>下面来尝试构造训练逻辑。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">// 仿照 API 文档中的提示，构造各项参数</span><br><span class="line">var paramCateFeaturesInfo = Map[Int, Int]()</span><br><span class="line"></span><br><span class="line">// 第一个特征信息：下标为 0 ，表示 dayOfMonth 有 0 到 30 的取值。</span><br><span class="line">paramCateFeaturesInfo += (0 -&gt; 31)</span><br><span class="line"></span><br><span class="line">// 第二个特征信息：下标为 1 ，表示 dayOfWeek 有 0 到 6 的取值。</span><br><span class="line">paramCateFeaturesInfo += (1 -&gt; 7)</span><br><span class="line"></span><br><span class="line">// 第三、四个特征是出发和抵达时间，这里我们不会用到，故省略。</span><br><span class="line"></span><br><span class="line">// 第五个特征信息：下标为 4 ，表示 uniqueCarrier 的所有取值。</span><br><span class="line">paramCateFeaturesInfo += (4 -&gt; mCarrier.size)</span><br><span class="line"></span><br><span class="line">// 第六个特征信息为飞行时间，同样忽略。</span><br><span class="line"></span><br><span class="line">// 第七个特征信息：下标为 6 ，表示 origin 的所有取值。</span><br><span class="line">paramCateFeaturesInfo += (6 -&gt; mOrigin.size)</span><br><span class="line"></span><br><span class="line">// 第八个特征信息：下标为 7， 表示 dest 的所有取值。</span><br><span class="line">paramCateFeaturesInfo += (7 -&gt; mDest.size)</span><br><span class="line"></span><br><span class="line">// 分类的数量为 2，代表已延误航班和未延误航班。</span><br><span class="line">val paramNumClasses = 2</span><br><span class="line"></span><br><span class="line">// 下面的参数设置为经验值</span><br><span class="line">val paramMaxDepth = 9</span><br><span class="line">val paramMaxBins = 7000</span><br><span class="line">val paramImpurity = &#x27;gini&#x27;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431488801.png" alt="img"><br>参数构造完成后，我们调用 trainClassfier 方法进行训练。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤耗时较长，请耐心等候。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val flightDelayModel = DecisionTree.trainClassifier(trainingData, paramNumClasses, paramCateFeaturesInfo, paramImpurity, paramMaxDepth, paramMaxBins)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431532055.png" alt="img"><br>等待训练完成后，我们可以尝试打印出这棵决策树。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val tmpDM = flightDelayModel.toDebugString</span><br><span class="line">print(tmpDM)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示，此处未显示所有的结果。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431545544.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431554243.png" alt="img"><br>决策树的内容看起来大致是多重的分支结构。如果有足够的耐心，你可以在草稿纸上将决策的条件逐一画出来。按照上述这些条件，我们就能对今后的一个输入值作出预测了。当然，预测的结果就是会延误或者不会延误。</p>
<h4 id="4-5-7-测试模型"><a href="#4-5-7-测试模型" class="headerlink" title="4.5.7 测试模型"></a>4.5.7 测试模型</h4><p>在模型训练完成之后，我们还需要检验模型的构造效果。因此，最后一步是使用测试集对模型进行测试。<br>请在 Spark Shell 中输入以下代码。<br>&#x2F;&#x2F; 使用决策树模型的predict方法按照输入进行预测，预测结果临时存放于 tmpPredictResult 中。最后与输入信息的标记组成元祖，作为最终的返回结果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val predictResult = testData.map&#123;flight =&gt;</span><br><span class="line">  val tmpPredictResult = flightDelayModel.predict(flight.features)</span><br><span class="line">  (flight.label, tmpPredictResult)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431587457.png" alt="img"><br>尝试取出 10 组预测结果，看一下效果。<br>请在 Spark Shell 中输入以下代码。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictResult.take(10)</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431595310.png" alt="img"><br>可以看到， 若 Label 的 0.0 与 PredictResult 的 0.0 是对应的，则表明预测结果是正确的。并且不是每一条预测值都是准确的。<br>因此，我们可以按照这个评价标准来统计有多少条预测记录是准确的。<br>请在 Spark Shell 中输入以下代码。 注意：此步骤耗时较长，请耐心等候。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val numOfCorrectPrediction = predictResult.filter&#123;case (label, result) =&gt; (label == result)&#125;.count()</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431607021.png" alt="img"><br>因数据集在每次随机划分过程中均会有差异，此处的正确预测记录数量仅供参考。<br>最后计算预测的正确率：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">请在 Spark Shell 中输入以下代码。 注意：此步骤耗时较长，请耐心等候。</span><br><span class="line">// 使用toDouble是为了提高正确率的精度，否则两个long值相除仍然是long值。</span><br><span class="line">val predictAccuracy = numOfCorrectPrediction/testData.count().toDouble</span><br></pre></td></tr></table></figure>

<p>执行结果如下图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676431619654.png" alt="img"><br>我们得到了该模型的预测正确率约为 59.25% ，可以说在实际的预测中还是有一定的应用价值的。为了提高预测的正确率，你可以考虑使用更多的数据进行模型的训练，并且在建立决策树时将参数调至最优。<br>因数据集在每次随机划分过程中均会有差异，此处的预测正确率仅供参考。</p>
<h2 id="36-Spark实验：实现电影推荐系统"><a href="#36-Spark实验：实现电影推荐系统" class="headerlink" title="36.Spark实验：实现电影推荐系统"></a>36.Spark实验：实现电影推荐系统</h2><blockquote>
<h3 id="目的-15"><a href="#目的-15" class="headerlink" title="目的"></a>目的</h3><p>本节课主要讲解协同过滤推荐算法，之后会实现一个简易的电影推荐系统，让您体会 Spark MLlib 在机器学习领域的强悍，然后学以致用。</p>
<h3 id="要求-15"><a href="#要求-15" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能：<br>协同过滤算法</p>
<h3 id="原理-15"><a href="#原理-15" class="headerlink" title="原理"></a>原理</h3><h3 id="3-1-机器学习概述"><a href="#3-1-机器学习概述" class="headerlink" title="3.1 机器学习概述"></a>3.1 机器学习概述</h3><p>机器学习是从已经存在的数据进行学习来对将来进行数据预测，它是基于输入数据集创建模型做数据驱动决策。<br>常见的几类机器学习模型： 监督学习模型: 监督学习模型对已标记的训练数据集训练出结果，然后对未标记的数据集进行预测。 非监督学习模型:  非监督学习模型是用来从原始数据（无训练数据）中找到隐藏的模式或者关系，因而非监督学习模型是基于未标记数据集的。 半监督学习模型:半监督学习模型用在监督和非监督机器学习中做预测分析，其既有标记数据又有未标记数据。典型的场景是混合少量标记数据和大量未标记数据。半监督学习一般使用分类和回归的机器学习方法。 增强学习模型: 增强学习模型通过不同的行为来寻找目标回报函数最大化。</p>
<h3 id="3-2-基于-Spark-MLlib-平台的协同过滤算法"><a href="#3-2-基于-Spark-MLlib-平台的协同过滤算法" class="headerlink" title="3.2 基于 Spark MLlib 平台的协同过滤算法"></a>3.2 基于 Spark MLlib 平台的协同过滤算法</h3><p>什么是协同过滤 (Collaborative Filtering, 简称 CF)？<br>首先想一个简单的问题，如果你现在想看个电影，但你不知道具体看哪部，你会怎么做？  大部分的人会问问周围的朋友，看看最近有什么好看的电影推荐，而我们一般更倾向于从口味比较类似的朋友那里得到推荐。这就是协同过滤的核心思想。协同过滤算法又分为基于用户的协同过滤算法和基于物品的协同过滤算法。<br>协同过滤算法按照数据使用可以分为：<br>基于用户（UserCF）：通过不同用户对物品的评分来评测用户之间的相似性，基于用户之间的相似性做出推荐。简单来讲，就是给用户推荐和他兴趣相似的其他用户喜欢的物品。<br>基于商品（ItemCF）：通过用户对不同 item 的评分来评测 item 之间的相似性，基于 item 之间的相似性做出推荐。简单来讲，就是给用户推荐和他之前喜欢的物品相似的物品。<br>基于模型（ModelCF）：基于模型的协同过滤推荐就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐。</p>
</blockquote>
<h3 id="4-1-数据准备-1"><a href="#4-1-数据准备-1" class="headerlink" title="4.1 数据准备"></a>4.1 数据准备</h3><p>数据文件在一下目录获取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># cd /root/data/spark/ml-1m</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430749063.png" alt="img"></p>
<p>该数据集，拥有 6000+个用户，3800+部电影，100 多万的评分数据，本次测试数据主要包括四个数据文件（详细的数据描述参见 README 文件）</p>
<p>movies.dat(电影资源数据)，格式为： 电影id::电影名称::电影类型<br>使用tail -f命令查看movies.dat，可以看到有 3800+部电影。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tail -f movies.dat</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430756482.png" alt="img"></p>
<p>ratings.dat(评分数据)，格式为： 用户Id::电影Id::评分::时间<br>使用tail -f命令查看ratings.dat，可以看到 6000+用户。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tail -f ratings.dat</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430766513.png" alt="img"></p>
<p>users.dat(评分数据)，格式为： 用户Id::性别::年龄::职业编号:邮编<br>使用tail -f命令查看users.dat，可以看到 6000+用户。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># tail -f users.dat</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430773295.png" alt="img"></p>
<p>person.txt(注意：这个数据集是我们自己创建在 ml-1m 下的，用户 Id 为 0)，格式为： 我的用户Id::我看过的电影Id::我对该电影的评分::评分的时间戳</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># touch person.txt</span><br><span class="line"># vim person.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430780044.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">0::1210::3::1545646575</span><br><span class="line">0::165::4::1545646575</span><br><span class="line">0::344::3::1545646575</span><br><span class="line">0::231::5::1545646575</span><br><span class="line">0::597::4::1545646575</span><br><span class="line">0::134::4::1545646575</span><br><span class="line">0::593::3::1545646575</span><br><span class="line">0::231::5::1545646575</span><br><span class="line">0::1580::4::1545646575</span><br><span class="line">0::1216::4::1545646575</span><br><span class="line">0::648::5::1545646575</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430788341.png" alt="img"></p>
<p>使用more命令查看person.txt。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># more person.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430794840.png" alt="img"></p>
<h3 id="4-2-整体思路"><a href="#4-2-整体思路" class="headerlink" title="4.2 整体思路"></a>4.2 整体思路</h3><p>我在一个电影网站上看了几部电影，并都为其做了评分操作（0-5 分），这个数据集是我们自己创建的person.txt<br>该电影网站的推荐系统根据person.txt数据集（即我对那几部电影的评分），预测出在该网站的电影资源库中，有哪些电影是适合我的，并推荐给我看。<br>根据我的观影习惯和用户的一个个人信息，预测该网站用户库中，哪些人和我的兴趣爱好是差不多的，并推荐给我认识，预测方法涉及训练数据集，均方根误差等。</p>
<h3 id="4-3-代码实现"><a href="#4-3-代码实现" class="headerlink" title="4.3 代码实现"></a>4.3 代码实现</h3><p>在开始代码前，不妨先来个热身准备，统计得分最高的 10 部电影。<br>进入spark-shell模式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark-shell</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430808426.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#加载数据集</span><br><span class="line">val ratingsRdd = sc.textFile(&#x27;file:///root/data/spark/ml-1m/ratings.dat&#x27;)</span><br><span class="line">#切分，缓存</span><br><span class="line">val ratings = ratingsRdd.map(_.split(&#x27;::&#x27;)).map &#123; x =&gt;</span><br><span class="line">(x(0), x(1), x(2))</span><br><span class="line">&#125;.cache</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430814972.png" alt="img"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#取前10</span><br><span class="line">val topK10ScoreMovie = ratings.map&#123;x =&gt;</span><br><span class="line">(x._2, (x._3.toInt, 1))</span><br><span class="line">&#125;.reduceByKey &#123; (v1, v2) =&gt;</span><br><span class="line">(v1._1 + v2._1, v1._2 + v2._2)</span><br><span class="line">&#125;.map &#123; x =&gt;</span><br><span class="line">(x._2._1.toFloat / x._2._2.toFloat, x._1)</span><br><span class="line">&#125;.sortByKey(false).</span><br><span class="line">take(10).</span><br><span class="line">foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430821789.png" alt="img"></p>
<p>接下来正式进入主题，这里还是用上面的spark-shell，前三个数据文件用于模型训练，第四个数据文件用于测试模型。</p>
<h4 id="4-3-1-首先导入依赖"><a href="#4-3-1-首先导入依赖" class="headerlink" title="4.3.1 首先导入依赖"></a>4.3.1 首先导入依赖</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.log4j.&#123;Level, Logger&#125;</span><br><span class="line">import org.apache.spark.mllib.recommendation.&#123;ALS, MatrixFactorizationModel, Rating&#125;</span><br><span class="line">import org.apache.spark.rdd._</span><br><span class="line">import org.apache.spark.&#123;SparkContext, SparkConf&#125;</span><br><span class="line">import scala.io.Source</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430829284.png" alt="img"></p>
<h4 id="4-3-2-定义-addRatings"><a href="#4-3-2-定义-addRatings" class="headerlink" title="4.3.2 定义 addRatings"></a>4.3.2 定义 addRatings</h4><p>定义 addRatings 有参方法，加载用户评分文件 person.txt。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def addRatings(path:String):Seq[Rating] = &#123;</span><br><span class="line">    val lines = Source.fromFile(path).getLines()</span><br><span class="line">    val ratings = lines.map&#123;</span><br><span class="line">      line =&gt;</span><br><span class="line">        val fields = line.split(&#x27;::&#x27;)</span><br><span class="line">        Rating(fields(0).toInt,fields(1).toInt,fields(2).toDouble)</span><br><span class="line">    &#125;.filter(_.rating &gt; 0.0)</span><br><span class="line">    if(ratings.isEmpty)&#123;</span><br><span class="line">      sys.error(&#x27;No ratings provided.&#x27;)</span><br><span class="line">    &#125;else&#123;</span><br><span class="line">      ratings.toSeq</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">val myRatings = addRatings(&#x27;/root/data/spark/ml-1m/person.txt&#x27;)</span><br><span class="line">val myRatingsRDD = sc.parallelize(myRatings, 1)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430838949.png" alt="img"></p>
<h4 id="4-3-3-加载样本评分数据"><a href="#4-3-3-加载样本评分数据" class="headerlink" title="4.3.3 加载样本评分数据"></a>4.3.3 加载样本评分数据</h4><p>加载样本评分数据，最后一列 Timestamp 取除 10 的余数作为 key，Rating 为值，即(Int，Rating)。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val ratings = sc.textFile(&#x27;file:///root/data/spark/ml-1m/ratings.dat&#x27;).map &#123;</span><br><span class="line">      line =&gt;</span><br><span class="line">        val fields = line.split(&#x27;::&#x27;)</span><br><span class="line">        (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430855828.png" alt="img"></p>
<h4 id="4-3-4-加载-movies-dat"><a href="#4-3-4-加载-movies-dat" class="headerlink" title="4.3.4 加载 movies.dat"></a>4.3.4 加载 movies.dat</h4><p>加载 movies.dat，取字段(movieId, movieName)。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val movies = sc.textFile(&#x27;file:///root/data/spark/ml-1m/movies.dat&#x27;).map &#123;</span><br><span class="line">      line =&gt;</span><br><span class="line">        val fields = line.split(&#x27;::&#x27;)</span><br><span class="line">        (fields(0).toInt, fields(1))</span><br><span class="line">    &#125;.collect().toMap</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430867584.png" alt="img"></p>
<h4 id="4-3-5-统计用户数量和电影数量以及用户对电影的评分数目。"><a href="#4-3-5-统计用户数量和电影数量以及用户对电影的评分数目。" class="headerlink" title="4.3.5 统计用户数量和电影数量以及用户对电影的评分数目。"></a>4.3.5 统计用户数量和电影数量以及用户对电影的评分数目。</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val numRatings = ratings.count()</span><br><span class="line"> val numUsers = ratings.map(_._2.user).distinct().count()</span><br><span class="line"> val numMovies = ratings.map(_._2.product).distinct().count()</span><br><span class="line"> println(&#x27;The data contains &#x27; + numRatings + &#x27; ratings from  &#x27; + numUsers + &#x27; users &#x27; + numMovies + &#x27; movies&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430876451.png" alt="img"></p>
<h4 id="4-3-6-Key值切分"><a href="#4-3-6-Key值切分" class="headerlink" title="4.3.6 Key值切分"></a>4.3.6 Key值切分</h4><p>将样本评分表以 key 值切分成 3 个部分，分别用于训练(55%，并加入用户评分), 校验 (15%), 测试(30%)，该数据在计算过程中要多次应用到，这里用 cache 缓存。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val numPartitions = 4</span><br><span class="line">val training = ratings.filter(x =&gt; x._1 &lt; 6).values.union(myRatingsRDD).repartition(numPartitions).cache()</span><br><span class="line">val validation = ratings.filter(x =&gt; x._1 &gt;= 6 &amp;&amp; x._1 &lt; 8).values.repartition(numPartitions).cache()</span><br><span class="line">val test = ratings.filter(x =&gt; x._1 &gt;= 8).values.cache()</span><br><span class="line">val numTraining = training.count()</span><br><span class="line">val numValidation = validation.count()</span><br><span class="line">val numTest = test.count()</span><br><span class="line">println(&#x27;Training: &#x27; + numTraining + &#x27; validation: &#x27; + numValidation + &#x27; test: &#x27; + numTest)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430886269.png" alt="img"></p>
<h4 id="4-3-7-获取最佳模型"><a href="#4-3-7-获取最佳模型" class="headerlink" title="4.3.7 获取最佳模型"></a>4.3.7 获取最佳模型</h4><p>训练不同参数下的模型，迭代次数,根据机器情况设定,并在校验集中验证，获取最佳参数下的模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val ranks = List(8, 12)</span><br><span class="line">val lambdas = List(0.1, 10.0)</span><br><span class="line">val numIters = List(10, 20)</span><br><span class="line">var bestModel: Option[MatrixFactorizationModel] = None</span><br><span class="line">var bestValidationRmse = Double.MaxValue</span><br><span class="line">var bestRank = 0</span><br><span class="line">var bestLambda = -1.0</span><br><span class="line">var bestNumIter = -1</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430896598.png" alt="img"></p>
<h4 id="4-3-8-定义-compute-函数"><a href="#4-3-8-定义-compute-函数" class="headerlink" title="4.3.8 定义 compute 函数"></a>4.3.8 定义 compute 函数</h4><p>定义 compute 函数校验集预测数据和实际数据之间的均方根误差。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def compute(model:MatrixFactorizationModel,data:RDD[Rating],n:Long):Double = &#123;</span><br><span class="line">val predictions:RDD[Rating] = model.predict((data.map(x =&gt; (x.user,x.product))))</span><br><span class="line">val predictionsAndRatings = predictions.map&#123; x =&gt;((x.user,x.product),x.rating)&#125;</span><br><span class="line">.join(data.map(x =&gt; ((x.user,x.product),x.rating))).values</span><br><span class="line">math.sqrt(predictionsAndRatings.map( x =&gt; (x._1 - x._2) * (x._1 - x._2)).reduce(_+_)/n)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430906432.png" alt="img"></p>
<h4 id="4-3-9-计算最佳模型"><a href="#4-3-9-计算最佳模型" class="headerlink" title="4.3.9 计算最佳模型"></a>4.3.9 计算最佳模型</h4><p>三层嵌套循环，会产生 8 个 ranks ，lambdas ，iters 的组合，每个组合都会产生一个模型，计算 8 个模型的方差，最小的那个记为最佳模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">for (rank &lt;- ranks; lambda &lt;- lambdas; numIter &lt;- numIters) &#123;</span><br><span class="line">   val model = ALS.train(training, rank, numIter, lambda)</span><br><span class="line">   val validRmse = compute(model, validation, numValidation)</span><br><span class="line">   println(&#x27;validation= &#x27; + validRmse + &#x27; for the model trained with rank = &#x27;</span><br><span class="line">   + rank + &#x27;,lambda = &#x27; + lambda + &#x27;,and numIter = &#x27; + numIter + &#x27;.&#x27;)</span><br><span class="line">   if (validRmse &lt; bestValidationRmse) &#123;</span><br><span class="line">   bestModel = Some(model)</span><br><span class="line">   bestValidationRmse = validRmse</span><br><span class="line">   bestRank = rank</span><br><span class="line">   bestLambda = lambda</span><br><span class="line">   bestNumIter = numIter</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430915598.png" alt="img"></p>
<p>注意:迭代过程执行时间可能有点长（4个半小时左右），如超时再次执行，请耐心等待，部分截图如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430926302.png" alt="img"></p>
<h4 id="4-3-10-计算和实际评分之间的均方根误"><a href="#4-3-10-计算和实际评分之间的均方根误" class="headerlink" title="4.3.10 计算和实际评分之间的均方根误"></a>4.3.10 计算和实际评分之间的均方根误</h4><p>用最佳模型预测测试集的评分，并计算和实际评分之间的均方根误。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val tRm = compute(bestModel.get, test, numTest)</span><br><span class="line">    println(&#x27;The best model was trained with rank = &#x27; + bestRank + &#x27; and lambda = &#x27; + bestLambda</span><br><span class="line">      + &#x27;, and numIter = &#x27; + bestNumIter + &#x27;, and its RMSE on the test set is &#x27; + tRm + &#x27;.&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430935326.png" alt="img"></p>
<h4 id="4-3-11-计算最佳模型与原始基础的相比其提升度"><a href="#4-3-11-计算最佳模型与原始基础的相比其提升度" class="headerlink" title="4.3.11 计算最佳模型与原始基础的相比其提升度"></a>4.3.11 计算最佳模型与原始基础的相比其提升度</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val inspopular = training.union(validation).map(_.rating).mean</span><br><span class="line">    val bRm = math.sqrt(test.map(x =&gt; (inspopular - x.rating) * (inspopular - x.rating)).reduce(_ + _) / numTest)</span><br><span class="line">    val improvement = (bRm - tRm) / bRm * 100</span><br><span class="line">    println(&#x27;The best model improves the baseline by &#x27; + &#x27;%1.2f&#x27;.format(improvement) + &#x27;%.&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430944813.png" alt="img"></p>
<h4 id="4-3-12-推荐前-15-部最感兴趣的电影给我，注意要剔除用户-我-已经评分的电影"><a href="#4-3-12-推荐前-15-部最感兴趣的电影给我，注意要剔除用户-我-已经评分的电影" class="headerlink" title="4.3.12 推荐前 15 部最感兴趣的电影给我，注意要剔除用户(我)已经评分的电影"></a>4.3.12 推荐前 15 部最感兴趣的电影给我，注意要剔除用户(我)已经评分的电影</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val myinterestedIds = myRatings.map(_.product).toSet</span><br><span class="line">    val choice = sc.parallelize(movies.keys.filter(!myinterestedIds.contains(_)).toSeq)</span><br><span class="line">      var i = 1</span><br><span class="line"></span><br><span class="line">    println(&#x27;Movies recommended for you:&#x27;)</span><br><span class="line">    bestModel.get.predict(choice.map((0, _))).collect.sortBy(-_.rating).take(15).foreach &#123; r =&gt;</span><br><span class="line">      println(&#x27;%2d&#x27;.format(i) + &#x27;: &#x27; + movies(r.product))</span><br><span class="line">      i += 1</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676430954954.png" alt="img"></p>
<p>这样，一个简单的基于模型的电影推荐应用就算 OK 了。</p>
<p>推荐的最终结果不一定准确，可以调整参数使得预测结果偏优。增加迭代次数，次数越多，lambda 较小，均方差会较小，推荐结果更好，置数据最好随机划分，结果更有说服力，数据量增大时，通过提高并行度，可以减少运行时间。</p>
<h2 id="37-Spark实验：Spark简介与安装部署"><a href="#37-Spark实验：Spark简介与安装部署" class="headerlink" title="37.Spark实验：Spark简介与安装部署"></a>37.Spark实验：Spark简介与安装部署</h2><blockquote>
<h3 id="目的-16"><a href="#目的-16" class="headerlink" title="目的"></a>目的</h3><p>1.了解 Spark 软件体系的构成和基本的部署方法。</p>
<h3 id="要求-16"><a href="#要求-16" class="headerlink" title="要求"></a>要求</h3><p>1.掌握spark 核心概念、生态系统、自行部署spark<br>2.掌握spark基本使用并启动spark服务</p>
<h3 id="原理-16"><a href="#原理-16" class="headerlink" title="原理"></a>原理</h3><h2 id="3-1Spark-简介"><a href="#3-1Spark-简介" class="headerlink" title="3.1Spark 简介"></a>3.1Spark 简介</h2><p>Spark 是 UC Berkeley AMP lab 开发的一个集群计算的框架，类似于  Hadoop，但有很多的区别。最大的优化是让计算任务的中间结果可以存储在内存中，不需要每次都写入 HDFS，更适用于需要迭代的  MapReduce 算法场景中，可以获得更好的性能提升。例如一次排序测试中，对 100TB 数据进行排序，Spark 比 Hadoop  快三倍，并且只需要十分之一的机器。Spark 集群目前最大的可以达到 8000 节点，处理的数据达到 PB 级别，在互联网企业中应用非常广泛。</p>
<h2 id="3-2Spark-的特性"><a href="#3-2Spark-的特性" class="headerlink" title="3.2Spark 的特性"></a>3.2Spark 的特性</h2><p>Hadoop 的核心是分布式文件系统 HDFS 和计算框架 MapReduces。Spark 可以替代 MapReduce，并且兼容 HDFS、Hive 等分布式存储层，良好的融入 Hadoop 的生态系统。</p>
<p>Spark执行的特点<br>• 中间结果输出：Spark 将执行工作流抽象为通用的有向无环图执行计划（DAG），可以将多 Stage 的任务串联或者并行执行。<br>• 数据格式和内存布局：Spark 抽象出分布式内存存储结构弹性分布式数据集 RDD，能够控制数据在不同节点的分区，用户可以自定义分区策略。<br>• 任务调度的开销：Spark 采用了事件驱动的类库 AKKA 来启动任务，通过线程池的复用线程来避免系统启动和切换开销。</p>
<h2 id="3-3Spark-的优势"><a href="#3-3Spark-的优势" class="headerlink" title="3.3Spark 的优势"></a>3.3Spark 的优势</h2><p>• 速度快，运行工作负载快 100 倍。Apache Spark 使用最先进的 DAG 调度器、查询优化器和物理执行引擎，实现了批处理和流数据的高性能。<br>• 易于使用，支持用 Java、Scala、Python、R 和 SQL 快速编写应用程序。Spark 提供了超过 80 个算子，可以轻松构建并行应用程序。您可以从 Scala、Python、R 和 SQL shell 中交互式地使用它。<br>• 普遍性，结合 SQL、流处理和复杂分析。Spark 提供了大量的库，包括 SQL 和 DataFrames、用于机器学习的 MLlib、GraphX 和 Spark 流。您可以在同一个应用程序中无缝地组合这些库。<br>• 各种环境都可以运行，Spark 在 Hadoop、Apache  Mesos、Kubernetes、单机或云主机中运行。它可以访问不同的数据源。您可以使用它的独立集群模式在 EC2、Hadoop  YARN、Mesos 或 Kubernetes 上运行 Spark。访问 HDFS、Apache Cassandra、Apache  HBase、Apache Hive 和数百个其他数据源中的数据。</p>
<h2 id="3-4哪些公司在使用-Spark"><a href="#3-4哪些公司在使用-Spark" class="headerlink" title="3.4哪些公司在使用 Spark"></a>3.4哪些公司在使用 Spark</h2><p>日常为我们所熟知的，在国外就有 IBM Almaden（IBM 研究实验室）、Amazon（亚马逊）等，而在国内有 baidu（百度）、Tencent（腾讯）等等，包括一些其它的公司大部分都使用 Spark 来处理生产过程中产生的大量数据。</p>
<h2 id="3-5Spark-生态系统-BDAS"><a href="#3-5Spark-生态系统-BDAS" class="headerlink" title="3.5Spark 生态系统 BDAS"></a>3.5Spark 生态系统 BDAS</h2><p>目前，Spark 已经发展成为包含众多子项目的大数据计算平台。BDAS 是伯克利大学提出的基于 Spark  的数据分析栈（BDAS）。其核心框架是 Spark，同时涵盖支持结构化数据 SQL 查询与分析的查询引擎 Spark  SQL，提供机器学习功能的系统 MLBase 及底层的分布式机器学习库 MLlib，并行图计算框架 GraphX，流计算框架 Spark  Streaming，近似查询引擎 BlinkDB，内存分布式文件系统 Tachyon，资源管理框架 Mesos 等子项目。这些子项目在  Spark 上层提供了更高层、更丰富的计算范式。</p>
</blockquote>
<h3 id="4-1部署-Spark"><a href="#4-1部署-Spark" class="headerlink" title="4.1部署 Spark"></a>4.1部署 Spark</h3><p>Spark 虽然是大规模的计算框架，但也支持在单机上运行，这里的教程提供的是单机模式安装，方便同学们在自己的电脑上按照相同的操作方式部署安装。<br>添加系统环境变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/profile</span><br></pre></td></tr></table></figure>

<p>在文件的末尾添加以下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SCALA_HOME=/usr/cstor/scala</span><br><span class="line">export PATH=$PATH:$SCALA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>然后执行如下命令使添加后的环境变量生效：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># source /etc/profile</span><br></pre></td></tr></table></figure>

<p>接着，需要设置 Spark 相关的环境变量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim /etc/profile</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676424870215.png" alt="img"></p>
<p>在文件的末尾添加以下内容，其中 $SPARK_HOME&#x2F;bin 是直接添加在已有的 PATH 环境变量之后，$PATH由环境中的实际情况决定。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HOME=/usr/cstor/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure>

<p>编辑完成后保存并退出 VIM 编辑器。在终端中使用 source 命令来激活之前设置的环境变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># source /etc/profile</span><br></pre></td></tr></table></figure>

<p>运行如下的命令打印出 spark-shell 的路径：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># which spark-shell</span><br></pre></td></tr></table></figure>

<p>内容如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/cstor/spark/bin/spark-shell</span><br></pre></td></tr></table></figure>

<p>spark-shell 的路径被打印了出来，证明环境没有错误。<br>我们进入到 spark 的配置目录 &#x2F;usr&#x2F;cstor&#x2F;spark&#x2F;conf&#x2F; 进行配置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/spark/conf/</span><br></pre></td></tr></table></figure>

<p>基于模板创建日志配置文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># cp log4j.properties.template log4j.properties</span><br></pre></td></tr></table></figure>

<p>使用 VIM 编辑文件 log4j.properties：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim log4j.properties</span><br></pre></td></tr></table></figure>

<p>修改 log4j.rootCategory 为 WARN, console ，可避免测试中输出太多信息：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676424882968.png" alt="img"></p>
<p>使用 VIM 编辑文件 spark-env.sh:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># vim spark-env.sh</span><br></pre></td></tr></table></figure>

<p>添加以下内容设置 Spark 的环境变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_161</span><br><span class="line">export SPARK_HOME=/usr/cstor/spark</span><br><span class="line">export SCALA_HOME=/usr/cstor/scala</span><br></pre></td></tr></table></figure>

<p>spark-env.sh脚本会在启动 Spark 时加载，内容包含很多配置选项及说明，在以后的实验中会用到少部分，感兴趣可以仔细阅读这个文件的注释内容。<br>至此，Spark 就已经安装好了，Spark 安装很简单，依赖也很少。<br>本次实验的数据为&#x2F;etc&#x2F;protocols,并将&#x2F;etc&#x2F;protocols上传至hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir /etc/</span><br><span class="line"># hdfs dfs -put /etc/protocols /etc/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676424892173.png" alt="img"></p>
<h3 id="4-2使用Spark-Shell"><a href="#4-2使用Spark-Shell" class="headerlink" title="4.2使用Spark Shell"></a>4.2使用Spark Shell</h3><p>Spark-Shell是 Spark 自带的一个 Scala 交互 Shell ，可以以脚本方式进行交互式执行，类似直接用 Python 及其他脚本语言的 Shell 。<br>进入Spark-Shell只需要执行spark-shell即可：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark-shell   #执行需要等待一小会</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696747883564.png" alt="img"></p>
<p>进入到Spark-Shell后可以使用Ctrl D组合键退出 Shell。<br>在Spark-Shell中我们可以使用 Scala 的语法进行简单的测试，比如我们运行下面几个语句获得文件&#x2F;etc&#x2F;protocols的行数以及第一行的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var file = sc.textFile(&#x27;/etc/protocols&#x27;)</span><br><span class="line">file.count()</span><br><span class="line">file.first()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676424908465.png" alt="img"></p>
<p>上面的操作中创建了一个 RDD file，执行了两个简单的操作：<br>1.count()获取 RDD 的行数<br>2.first()获取第一行的内容<br>我们继续执行其他操作，比如查找有多少行含有tcp和udp字符串：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file.filter(line =&gt; line.contains(&#x27;tcp&#x27;)).count()</span><br><span class="line">file.filter(line =&gt; line.contains(&#x27;udp&#x27;)).count()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676424915929.png" alt="img"></p>
<p>查看一共有多少个不同单词的方法，这里用到 Mapreduce 的思路：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var wordcount = file.flatMap(line =&gt; line.split(&#x27; &#x27;)).map(word =&gt; (word,1)).reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">wordcount.count()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676424926591.png" alt="img"></p>
<p>上面两步骤我们发现，&#x2F;etc&#x2F;protocols中各有一行含有tcp与udp字符串，并且一共有 243 个不同的单词。<br>上面每个语句的具体含义这里不展开，可以结合你阅读的文章进行理解，后续实验中会不断介绍。这里仅仅提供一个简单的例子让大家对 Spark 运算有基本认识。<br>操作完成后，Ctrl D组合键退出 Shell。</p>
<h3 id="4-3Pyspark"><a href="#4-3Pyspark" class="headerlink" title="4.3Pyspark"></a>4.3Pyspark</h3><p>Pyspark 类似 Spark-Shell ，是一个 Python 的交互 Shell 。<br>执行pyspark启动进入 Pyspark：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># pyspark    #执行需要等待一小会</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696747955422.png" alt="img"></p>
<p>退出方法仍然是Ctrl D组合键。<br>在 Pyspark 中，我们可以用 Python 语法执行 Spark-Shell 中的操作，比如下面的语句获得文件&#x2F;etc&#x2F;protocols 的行数以及第一行的内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file = sc.textFile(&#x27;/etc/protocols&#x27;)</span><br><span class="line">file.count()</span><br><span class="line">file.first()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676424946464.png" alt="img"></p>
<p>操作完成后，Ctrl D组合键退出 Shell。</p>
<h3 id="4-4-启动-Spark-服务"><a href="#4-4-启动-Spark-服务" class="headerlink" title="4.4.启动 Spark 服务"></a>4.4.启动 Spark 服务</h3><p>执行下面几条命令启动主节点：<br>进入到spark目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/spark/sbin</span><br></pre></td></tr></table></figure>

<p>启动主节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./start-master.sh</span><br></pre></td></tr></table></figure>

<p>没有报错的话表示 master 已经启动成功，master 默认可以通过 web 访问http:&#x2F;&#x2F;(主节点ip):8080,打开浏览器，访问该链接：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696748242925.png" alt="img"><br>图中所示，master 中暂时还没有一个 worker ，我们启动 worker 时需要 master 的参数，该参数已经在上图中标志出来：spark:&#x2F;&#x2F;master:7077，请在执行后续命令时替换成你自己的参数。<br>启动主节点<br>执行下面的命令启动 slave：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./start-slave.sh spark://master:7077</span><br></pre></td></tr></table></figure>

<p>没有报错表示启动成功，再次刷新浏览器页面可以看到下图所示新的 worker 已经添加：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696748325787.png" alt="img"></p>
<p>也可以用jps命令查看启动的服务，应该会列出Master和Worker。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676424977362.png" alt="img"></p>
<p>使用 spark-shell 连接 master ，注意把 MASTER 参数替换成你实验环境中的实际参数：<br>MASTER&#x3D;spark:&#x2F;&#x2F;master:7077 spark-shell #执行需要等待一小会</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># MASTER=spark://master:7077 spark-shell</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696748506980.png" alt="img"></p>
<p>刷新 master 的 web 页面，可以看到新的Running Applications，如下图所示：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696748534519.png" alt="img"></p>
<p>当退出 spark-shell 时，这个 application 会移动到Completed Applications一栏。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696748606272.png" alt="img"></p>
<p>停止服务的脚本为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ./sbin/stop-all.sh。</span><br><span class="line"># ./stop-all.sh</span><br><span class="line"># Jps</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676425012427.png" alt="img"></p>
<p>通过 jps 可以看到，master 与 worker 进程都已经停止。</p>
<h3 id="4-5Spark-集群部署："><a href="#4-5Spark-集群部署：" class="headerlink" title="4.5Spark 集群部署："></a>4.5Spark 集群部署：</h3><p>1.主节点上配置 spark ，例如 conf&#x2F;spark-env.sh 中的环境变量<br>2.主节点上配置 conf&#x2F;slaves ，添加从节点的主机名，注意需要先把所有主机名输入到 &#x2F;etc&#x2F;hosts 避免无法解析<br>3.把配置好的 spark 目录拷贝到所有从节点，从节点上的目录路径与主节点一致，例如都设置为&#x2F;usr&#x2F;cstor&#x2F;spark<br>4.配置主节点到所有从节点的 SSH 无密码登录，使用ssh-keygen -t rsa和ssh-copy-id两个命令<br>5.启动 spark 集群，在主节点上执行sbin&#x2F;start-all.sh<br>6.进入主节点的 web 界面查看所有 worker 是否成功启动</p>
<h2 id="38-Spark实验：SparkFPGrowth关联学习"><a href="#38-Spark实验：SparkFPGrowth关联学习" class="headerlink" title="38.Spark实验：SparkFPGrowth关联学习"></a>38.Spark实验：SparkFPGrowth关联学习</h2><blockquote>
<h3 id="目的-17"><a href="#目的-17" class="headerlink" title="目的"></a>目的</h3><p>模式挖掘也叫关联规则，其实就是从大量的数据中挖掘出比较有用的数据，挖掘频繁项。比如说超市有大量的购物数据，从而可以根据用户的购物数据找到哪些商品关联性比较大。也可以进行用户推荐。下面我们说一个真实的案例。</p>
<p>我想可能有些同学一定听说过尿布和啤酒的故事吧。就是在一家超市里尿布和啤酒摆在一起出售。但是这个奇怪的举措却使尿布和啤酒的销量双双增加了。这不是一个笑话，而是发生在美国沃尔玛连锁店超市的真实案例，并一直为商家所津津乐道。</p>
<p>沃尔玛拥有世界上最大的数据仓库系统，为了能够准确了解顾客在其门店的购买习惯，沃尔玛对其顾客的购物行为进行购物篮分析，想知道顾客经常一起购买的商品有哪些。沃尔玛数据仓库里集中了其各门店的详细原始交易数据。在这些原始交易数据的基础上，沃尔玛利用数据挖掘方法对这些数据进行分析和挖掘。</p>
<p>一个意外的发现是：”跟尿布一起购买最多的商品竟是啤酒。经过大量实际调查和分析，揭示了一个隐藏在”尿布与啤酒”背后的美国人的一种行为模式：在美国，一些年轻的父亲下班后经常要到超市去买婴儿尿布，而他们中有 30%～ 40%  的人同时也为自己买一些啤酒。产生这一现象的原因是：美国的太太们常叮嘱她们的丈夫下班后为小孩买尿布，而丈夫们在买尿布后又随手带回了他们喜欢的啤酒。</p>
<p>沃尔玛就是根据自己超市里面的售物数据，通过数据挖掘，这里指的就是关联规则算法分析，从而从大量的数据中找到了啤酒和尿布是最搭配的，然后他们就将两者放在一起出售，取得了较好的经济效应。可想而知，数据挖掘在现实生活中还是挺有用的吧。</p>
<h3 id="要求-17"><a href="#要求-17" class="headerlink" title="要求"></a>要求</h3><p>本次试验后，要求学生能：<br>1.Scala 基础编程<br>2.RDD 的基本操作：Transformation 和 Actions<br>3.Spark Mlib 的 FPGrowth 的算法应用</p>
<h3 id="原理-17"><a href="#原理-17" class="headerlink" title="原理"></a>原理</h3><p>通过将数据转换成需要的数据类型，然后将数据使用 Mlib 算法进行模式挖掘。在该实验中的数据格式是 RDD[Array[String]]，当然在 Spark 中还有其他的数据类型，比如说 Vector，LabeledPoint，Matrix 等数据类型。</p>
<p>实验流程图</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748425551.png" alt="img"><br>算法详解<br>关联规则用于表示数据内隐藏的关联性。比如说上面说到的故事。啤酒和尿布湿的关系，买尿布湿的消费者往往也会买啤酒。这就表明了数据之间的某种隐藏联系。但是分开看两者没有任何联系。关联规则算法发展到现在一共有三个算法：FP-Tree 算法、Apriori 算法、Eclat 算法，这三个算法我们主要是讲解 FP-Tree 算法。FP-Tree  是一种不产生候选模式而采用频繁模式增长的方法挖掘频繁模式的算法。此算法只扫描两次，第一次扫描数据是得到频繁项集，第二次扫描是利用支持度过滤掉非频繁项，同时生成 FP 树。然后后面的模式挖掘就是在这棵树上进行。此算法与 Apriori 算法最大不同的有两点：不产生候选集，只遍历两次数据，大大提升了效率。<br>下面我们就来讲讲该算法的实现过程。</p>
<p>假设有这么一批数据：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748431406.png" alt="img"></p>
<p>第一次扫描数据得到频繁项集合，统计所有商品出现的次数：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748437082.png" alt="img"></p>
<p>假设最小支持度为 3，那调整后的频繁项集合就是：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748443662.png" alt="img"></p>
<p>第二次遍历数据构建频繁项集合树：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748450820.png" alt="img"></p>
<p>FPTree 建立的规则就是树的根，为根节点-ROOT，定义为  NULL。然后将数据里面的每一条数据进行插入节点操作，每个节点包含一个名称，和一个次数的属性，例如上图的 a:4，a 就是 Item  中的名称，4 代表的是出现的次数。如果插入新事务时，树中已经包含该节点，这不用创建树节点，直接在节点上的次数加一就行。</p>
</blockquote>
<h3 id="4-1数据准备"><a href="#4-1数据准备" class="headerlink" title="4.1数据准备"></a>4.1数据准备</h3><p>数据文件在一下目录获取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/data/spark/FPGrowth/Groceries.txt</span><br></pre></td></tr></table></figure>

<p>获取数据后，我们看看里面的数据格式。使用 more 命令，查看该数据，如下图所示，这个 csv 文件包含 9835  条数据。下面我们就来介绍该数据文件，首先第一行 items 指的是标题栏是没有用的数据，其中 大括号  中的数据，就是一次性购买的物品，每一行都有一个数字作为他的标签，在这里我们不将他作为行数标记，我们将他认为是用户  ID，我们也可以针对某一用户进行商品推荐。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /root/data/spark/FPGrowth/</span><br><span class="line"># more Groceries.txt</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748483457.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748488859.png" alt="img"></p>
<p>再将数据上传至HDFS</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir /root</span><br><span class="line"># hdfs dfs -put Groceries.txt /root/</span><br><span class="line"># hdfs dfs –ls /root</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748497940.png" alt="img"></p>
<h3 id="4-2启动-spark-shell"><a href="#4-2启动-spark-shell" class="headerlink" title="4.2启动 spark shell"></a>4.2启动 spark shell</h3><p>请在终端中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark-shell</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696748656071.png" alt="img"></p>
<p>进入到 spark 的 REPL 环境，相当于就是 spark 的 Console。</p>
<h3 id="4-3导入数据并转换数据"><a href="#4-3导入数据并转换数据" class="headerlink" title="4.3导入数据并转换数据"></a>4.3导入数据并转换数据</h3><p>下面我们就进入到实验的关键阶段，进行数据的导入，并进行必要的数据格式处理。将数据清洗成我们需要的数据格式。</p>
<h4 id="4-3-1导入数据"><a href="#4-3-1导入数据" class="headerlink" title="4.3.1导入数据"></a>4.3.1导入数据</h4><p>Spark textFile 进行数据的导入，将外部数据导入到 Spark 中来，并将其转换成 RDD。<br>键入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val data = sc.textFile(&#x27;/root/Groceries.txt&#x27;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748513819.png" alt="img"></p>
<p>从图中我们可以看到我们引入了&#x2F;root&#x2F; 下面的 Groceries.txt 文件。在最后我们也了解到了当前的数据格式为 RDD[String]类型的数据。<br>现在我们可以看看当前 data 的数据样式。我们查看 data 的前十行数据。 这里使用到 RDD Action 操作，take(num) 函数，take  的作用就是获取 RDD 中下标 0 到 num-1 的元素。foreach 遍历，并打印。 键入命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.take(10).foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748522719.png" alt="img"></p>
<h4 id="4-3-2去除无用数据"><a href="#4-3-2去除无用数据" class="headerlink" title="4.3.2去除无用数据"></a>4.3.2去除无用数据</h4><p>在之前我们就已经了解到，我们不需要该文件的第一行数据，它只是一个标签行，并没有真正的购物数据。<br>输入如下命令删除第一行数据：<br>RDD filter，filter 函数是应用于 RDD 的每一个元素，会过滤掉不符合条件的元素，返回新的 RDD，其实新的 RDD 的内容就是 filter 里面返回值为 True 的元素。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val dataNoHead = data.filter(line =&gt; !line.contains(&#x27;items&#x27;))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748529908.png" alt="img"></p>
<p>现在我们查看 dataNoHead 的前五行，看是否已经去除了第一行数据。结果如下：<br>输入以下命令：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748537341.png" alt="img"></p>
<p>从图上可以看出，我们已经去除了第一行。</p>
<h4 id="4-3-3获取购物数据"><a href="#4-3-3获取购物数据" class="headerlink" title="4.3.3获取购物数据"></a>4.3.3获取购物数据</h4><p>我们的目的是针对购物数据进行数据关联分析，在此用户 id 是没有用的数据，在这里我们选择将其抛弃，只获取我们需要的购物数据。具体操作如下：使用 rdd 的 map 函数，我们将用户 id 和购物商品分开，然后只取购物信息。<br>输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val dataS = dataNoHead.map(line =&gt; line.split(&#x27;\\&#123;&#x27;))</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748553001.png" alt="img"></p>
<p>从上图可以了解到，dataS 的数据格式为 RDD[Array[String]] 数据，接下来我们看看数据内容。如下图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataS.take(5)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748560784.png" alt="img"></p>
<p>然后我们获取购物数据就简单了，就是 dataS 数据里面的 Array 里面的第二个元素，并去除里面的 } 字符。<br>输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val dataGoods = dataS.map(s =&gt; s(1).replace(&#x27;&#125;\&#x27;&#x27;,&#x27;&#x27;))</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677748999957.png" alt="img"></p>
<p>同理，我们可以看到 dataGoods 的数据格式是 RDD[String]了，我们看看该 rdd 里面的数据样式。</p>
<p>键入命令和上面的一样，我相信通过这几步练习，已经有所了解了吧。效果如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataGoods.take(5).foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677749008215.png" alt="img"></p>
<p>在这里，我们能够看到我们已经完全获取到了我们需要的购物数据。然后我们将这些数据以逗号分割开，用于 FPGrowth 的建模数据。</p>
<p>拆分数据，将其转换为建模数据的格式。需要的格式可以查看 FPGrowth 的 API 查看我们需要传入什么格式的数据，通过了解需要传入  RDD[Array[String]]。将购物数据按逗号拆分，转换成建模数据。并将其 cache 到内存中，方便我们多次使用该数据。</p>
<p>输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val fpData = dataGoods.map(_.split(&#x27;,&#x27;)).cache</span><br></pre></td></tr></table></figure>

<p>同样，我们查看一下该 RDD 里面的内容。输入一下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fpData.take(5).foreach(line =&gt; line.foreach(print))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677749017829.png" alt="img"></p>
<p>这样我们就将数据按照每一行，拆分成单独的商品，以 RDD[Array[String]] 的数据格式存在。到此我们整个的数据准备过程就结束了。</p>
<h3 id="4-4FPGrowth-模型建立"><a href="#4-4FPGrowth-模型建立" class="headerlink" title="4.4FPGrowth 模型建立"></a>4.4FPGrowth 模型建立</h3><p>我们先了解 FP 的三个基本概念。支持度与置信度与提升度。在这里主要介绍支持度和置信度两个概念。</p>
<p>支持度：比如说 A 对 B 的支持度，就是表示 AB 同时出现的事件除以总事件的概率。<br>置信度：比如说 A 对 B 的置信度，就是 AB 同时出现的事件除以 A 的总事件的概率。<br>提升度：同样 A 对 B，表示在包含 A 的条件下同时包含 B 的概率，除以不含 A 的条件下含有 B 的概率。</p>
<p>下面我们举一个列子。比如说有 1000 个顾客，有 400 人买了物品 A，400 人买了物品 B，有 200 人买了 AB 两个商品。那么:<br>A 的支持度为：(400+200)&#x2F;1000<br>AB 的支持度为：200&#x2F;1000<br>A 对 B 的置信度为：200&#x2F;400 (AB&#x2F;A)<br>B 对 A 的置信度为：200&#x2F;400 (AB&#x2F;B)</p>
<p>接下来我们开始建立模型：</p>
<h4 id="4-4-1引入-FPGrowth-包"><a href="#4-4-1引入-FPGrowth-包" class="headerlink" title="4.4.1引入 FPGrowth 包"></a>4.4.1引入 FPGrowth 包</h4><p>输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.fpm.FPGrowth</span><br></pre></td></tr></table></figure>

<p>实例化 FPGrowth 并且设置支持度为 0.05，不满足该支持度的数据将被去除和分区为 3。<br>输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val fpGroup = new FPGrowth().setMinSupport(0.05).setNumPartitions(3)</span><br></pre></td></tr></table></figure>

<p>结果图如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677749040934.png" alt="img"></p>
<h4 id="4-4-2开始创建模型"><a href="#4-4-2开始创建模型" class="headerlink" title="4.4.2开始创建模型"></a>4.4.2开始创建模型</h4><p>使用向前准备好的 fpData 数据，进行 FP 模型的建立。调用 FPGrowth 里面的 run 方法，进行模型的创建。<br>输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val fpModel = fpGroup.run(fpData)</span><br></pre></td></tr></table></figure>

<p>创建模型后最后会有如下输出：<br>获取满足支持度条件的频繁项集。输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val freqItems = fpModel.freqItemsets.collect</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677749060570.png" alt="img"></p>
<p>查找频繁项，看哪些商品关联性高。打印频繁项内容。输入命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">freqItems.foreach(f =&gt; println(&#x27;FrequentItem:&#x27;+f.items.mkString(&#x27;,&#x27;)+&#x27;OccurrenceFrequency:&#x27;+f.freq))</span><br><span class="line"></span><br><span class="line">// FrequentItem：频繁项，OccurrenceFrequency：出现次数</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677749073870.png" alt="img"></p>
<p>从中我们可以看出牛奶和酸奶酪还是比较配的，频繁项次数是 551 次，两个同时出现的次数。当然我只截取了一部分数据，这也是一部分小数据。随着数据的增加就会有更好的效果。</p>
<h4 id="4-4-3进行用户商品推荐"><a href="#4-4-3进行用户商品推荐" class="headerlink" title="4.4.3进行用户商品推荐"></a>4.4.3进行用户商品推荐</h4><p>前面我们看到用户 ID 为 3 的用户只购买了牛奶（whole milk），我们可以向他推荐什么商品呢。定义一个用户 ID 为 3 的变量（排序从 0 开始）。</p>
<p>命令如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val userID = 2</span><br></pre></td></tr></table></figure>

<p>获取用户 3 购买的物品。输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val usrList = fpData.take(3)( userID)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677749083567.png" alt="img"></p>
<p>我们可以看看 userList 的输出。输入以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usrList</span><br></pre></td></tr></table></figure>

<p>我们获取了该用户的商品后，在频繁项中找出该商品出现的次数，用于后面计算商品之间的置信度。定义一个变量用于存储该商品的次数。输入命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var goodsFreq = 0L</span><br></pre></td></tr></table></figure>

<p>查找频繁项集的次数。输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for(goods &lt;- freqItems)&#123;</span><br><span class="line">         if(goods.items.mkString == usrList.mkString)&#123;</span><br><span class="line">        goodsFreq = goods.freq &#125;&#125;</span><br><span class="line">println(&#x27;GoodsNumber:&#x27; + goodsFreq)</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677749094655.png" alt="img"></p>
<p>由于数据量的原因，在这里我们置信度设置为 0.1，当商品的置信度大于 0.1 这个阈值，我们就将其推荐给用户。在推荐过程中需要去除用户已经购买了的商品。</p>
<p>输入如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">for(f &lt;- freqItems)&#123;</span><br><span class="line">      if(f.items.mkString.contains(usrList.mkString) &amp;&amp; f.items.size &gt; usrList.size) &#123;</span><br><span class="line">        val conf:Double = f.freq.toDouble / goodsFreq.toDouble</span><br><span class="line">        if(conf &gt;= 0.1) &#123;</span><br><span class="line">          var item = f.items</span><br><span class="line">          for (i &lt;- 0 until usrList.size) &#123;</span><br><span class="line">            item = item.filter(_ != usrList(i))</span><br><span class="line">          &#125;</span><br><span class="line">          for (str &lt;- item) &#123;</span><br><span class="line">          println(str+&#x27;  ===&#x27;+conf)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>结果如下：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677749102956.png" alt="img"></p>
<p>最后就是其置信度的值。我们看看当 3 号用户买牛奶的时候会给我们推荐什么商品？</p>
<p>Yogurt: 酸乳酪<br>vegettables: 菜蔬<br>rolls&#x2F;buns: 小面包或点心</p>
<p>看到推荐的这些商品，是不是感觉和牛奶很配呢。到此我们的实验就结束了。当然也会存在推荐商品没有的情况，这种情况我们就可以将频繁项集里面的出现次数最高的几件商品推荐给用户。</p>
<h2 id="39-Spark实验：RDD简介与操作"><a href="#39-Spark实验：RDD简介与操作" class="headerlink" title="39.Spark实验：RDD简介与操作"></a>39.Spark实验：RDD简介与操作</h2><blockquote>
<h3 id="目的-18"><a href="#目的-18" class="headerlink" title="目的"></a>目的</h3><p>1.学习 Spark RDD 的简介与相关操作。</p>
<h3 id="要求-18"><a href="#要求-18" class="headerlink" title="要求"></a>要求</h3><p>1.掌握spark的连接初始化操作<br>2.掌握RDD的转化操作、行动操作</p>
<h3 id="原理-18"><a href="#原理-18" class="headerlink" title="原理"></a>原理</h3><p>RDD 即弹性分布式数据集（Resilient Distributed Databases），它是 Spark 对数据的核心抽象，也就是 Spark  对于数据进行处理的基本单位。使用 Spark 对数据进行处理首先需要把数据转换为 RDD，然后在 RDD 上对数据进行操作。RDD  有两种算子，分别是转换和行动。<br>在 Spark 中，对数据处理的操作流程就是：创建 RDD、对 RDD 进行转化操作、然后执行行动操作求值。通过对 RDD 进行操作，隐藏了 Spark 底层各节点通信、协调、容错细节，使得操纵数据更加的轻松方便。</p>
</blockquote>
<h3 id="4-1启动Spark-服务"><a href="#4-1启动Spark-服务" class="headerlink" title="4.1启动Spark 服务"></a>4.1启动Spark 服务</h3><p>进入到spark目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/spark/sbin</span><br></pre></td></tr></table></figure>

<p>启动主节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./start-master.sh</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676280985105.png" alt="img"></p>
<p>打开浏览器访问http:&#x2F;&#x2F;(主节点ip):8080，查看 Spark：</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696748703974.png" alt="img"></p>
<p>URI 是spark:&#x2F;&#x2F;master:7077，请注意你的实验环境可能不同。<br>启动slave节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ./start-slave.sh spark://master:7077</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281000904.png" alt="img"></p>
<p>后续的实验我们将全程采用 Scala 语言，Spark-Shell 交互界面里实践更方便，并且不需要太多依赖，可以更快上手。</p>
<h3 id="4-2连接-Spark"><a href="#4-2连接-Spark" class="headerlink" title="4.2连接 Spark"></a>4.2连接 Spark</h3><p>如果想在外部集成编译环境中编写一个 Spark 应用，你需要在 Spark 上添加一个 Maven 依赖项。在 Maven 的中央仓库可作如下修改：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">groupId = org.apache.spark</span><br><span class="line">artifactId = spark-core_3.32</span><br><span class="line">version = 2.4.4</span><br></pre></td></tr></table></figure>

<p>另外，如果你想要连接到 HDFS 集群，则需要在hadoop-client配置文件中为 HDFS 版本添加依赖项。例如如下修改（一些常见的 HDFS 版本标签在官方的Third Party Distributions 页面可以查看到）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">groupId = org.apache.hadoop</span><br><span class="line">artifactId = hadoop-client</span><br><span class="line">version =</span><br></pre></td></tr></table></figure>

<p>最后，你需要将一些 Spark 中的类引入到你的程序中，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.SparkConf</span><br></pre></td></tr></table></figure>

<h3 id="4-3初始化-Spark"><a href="#4-3初始化-Spark" class="headerlink" title="4.3初始化 Spark"></a>4.3初始化 Spark</h3><p>开发 Spark 程序第一件事就是创建一个SparkConf对象，这个对象包含应用的一些信息，然后创建SparkContext，SparkContext可以让 Spark 知道如何访问集群：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val conf = new SparkConf().setAppName(&quot;aabb&quot;).setMaster(&quot;spark://master:7077&quot;)</span><br><span class="line">new SparkContext(conf)</span><br></pre></td></tr></table></figure>

<p>在每个JVM中，只有一个SparkContext能够被激活。若需要创建新的SparkContext，你必须调用sc.stop()来关闭当前已激活的那个</p>
<h3 id="4-4使用-Spark-Shell"><a href="#4-4使用-Spark-Shell" class="headerlink" title="4.4使用 Spark Shell"></a>4.4使用 Spark Shell</h3><p>Spark shell 中我们不需要这样初始化，因为 Spark Shell 相当于一个 Spark 应用，启动时已经用过spark-shell  –master spark:&#x2F;&#x2F;master:7077来指定集群信息，所以 Spark Shell 启动后已经具备了一个  SparkContext 对象sc，可以在 Spark shell 中输入sc测试下。</p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1696749317297.png" alt="img"><br>如果需要让 Spark Shell 中可以使用某些 jar 模块，可以通过下面的命令来指定：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># spark-shell --master spark://master:7077 --jars code.jar</span><br></pre></td></tr></table></figure>

<p>然后可以通过 maven 坐标来引用一个依赖，就像这样：</p>
<p>spark-shell –master spark:&#x2F;&#x2F;master:7077 –packages “org.example:example:0.1”<br>如果需要查看完整的选项列表，你可以执行命令spark-shell –help。</p>
<h3 id="4-5-RDD-的系列操作"><a href="#4-5-RDD-的系列操作" class="headerlink" title="4.5 RDD 的系列操作"></a>4.5 RDD 的系列操作</h3><p>Spark 上开发的应用程序都是由一个driver programe构成，这个所谓的驱动程序在 Spark  集群通过跑main函数来执行各种并行操作。集群上的所有节点进行并行计算需要共同访问一个分区元素的集合，这就是 RDD（RDD：resilient distributed dataset）弹性分布式数据集。RDD 可以存储在内存或磁盘中，具有一定的容错性，可以在节点宕机重启后恢复。RDD  可以从 HDFS 中的文件创建，也可以从 Scala 或 Python 集合中创建。<br>创建 RDD 有两种方式：一种是调用 SparkContext 的 parallelize() 方法将数据并行化生成 RDD，另外一种方法是从外部存储中的数据集生成 RDD（如 Linux 文件系统，HDFS，HBase 等）。<br>我们首先启动 Spark Shell，后续操作学习在 Spark Shell 上创建 RDD：</p>
<h3 id="4-5-1并行集合"><a href="#4-5-1并行集合" class="headerlink" title="4.5.1并行集合"></a>4.5.1并行集合</h3><p>如果要对已有的集合进行并行化，我们可以先创建一个列表，然后调用上面提到的sc的parallelize方法将该集合并行化。集合中的元素会被复制到一个 RDD 中。并行集合创建后可以进行 RDD 的分布式操作，一个很重要的参数是切片数（slices），表示数据集被切分的份数，Spark  会为每个切片运行一个任务并能够根据集群状况动态调整切片数量。实验中我们使用parallelize方法的参数可以手动设置切片数。<br>在 spark-shell 中操作，依次输入下面的内容：<br>创建 1-5 的列表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val data = Array(1,2,3,4,5)</span><br></pre></td></tr></table></figure>

<p>从列表创建 RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val distData = sc.parallelize(data)</span><br></pre></td></tr></table></figure>

<p>创建切片数为 2 的 RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val distData2 = sc.parallelize(data,2)</span><br></pre></td></tr></table></figure>

<p>对 RDD 进行测试操作<br>对集合中的所有元素进行相加，返回结果为 15</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distData.reduce((a,b) =&gt; a + b)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281076173.png" alt="img"></p>
<p>需要注意的是，由于采用这种方式需要先把所有数据集放在机器的内存中，所以除了开发原型和测试以外，一般不采用这种方式。我们更常用的是下面这一种方式。<br>外部数据集<br>在实际开发中最常用的是从外部存储系统中读取数据创建 RDD。Spark 可以从任何 Hadoop 支持的存储上创建 RDD，比如本地的文件系统、HDFS、Cassandra、HBase 等。同时 Spark 还支持文本文件、SequenceFiles 等。<br>本次实验的数据为&#x2F;etc&#x2F;protocols,并将&#x2F;etc&#x2F;protocols上传至hdfs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hdfs dfs -mkdir /etc/</span><br><span class="line"># hdfs dfs -put /etc/protocols /etc/</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281085364.png" alt="img"></p>
<p>在 spark-shell 中执行下面的操作：<br>从 protocols 文件中创建 RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val distFile = sc.textFile(&quot;/etc/protocols&quot;)</span><br></pre></td></tr></table></figure>

<p>RDD 操作：计算所有行的长度之和，最后结果为 2868</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distFile.map(s =&gt; s.length).reduce((a,b) =&gt; a + b)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281095238.png" alt="img"></p>
<p>Spark RDD 数据源<br>RDD 支持两种操作：<br>1.转换（transformations）：将已存在的数据集 RDD 转换成新的数据集 RDD，例如 map。转换是惰性的，不会立刻计算结果，仅仅记录转换操作应用的目标数据集，当动作需要一个结果时才计算。<br>2.动作（actions） ：在数据集 RDD 上进行计算后返回一个结果值给驱动程序，例如 reduce。<br>默认情况，每个转换过的 RDD 都会在执行动作时重新计算一次，可以通过 persist 或 cache 方法将 RDD 持久化存储到内存中，来让下次使用时查询速度可以更快。同时，RDD 也支持存储到磁盘或者在集群多个节点间复制。<br>在 spark-shell 中输入如下代码：</p>
<p>从 protocols 文件创建RDD</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val distFile = sc.textFile(&quot;/etc/protocols&quot;)</span><br></pre></td></tr></table></figure>

<p>Map 操作，每行的长度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val lineLengths = distFile.map(s =&gt; s.length)</span><br></pre></td></tr></table></figure>

<p>Reduce 操作，获得所有行长度的和，即文件总长度，这里才会执行 map 运算</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val totalLength = lineLengths.reduce((a, b) =&gt; a + b)</span><br></pre></td></tr></table></figure>

<p>可以将转换后的 RDD 保存到集群内存中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lineLengths.persist()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281105771.png" alt="img"></p>
<p>因此，RDD 基本编程步骤可以总结为：<br>1.读取内、外部数据集创建 RDD。<br>2.对于 RDD 进行转化生成新的 RDD ，比如 map()、filter() 等。<br>3.对需要重用的数据执行 persist()&#x2F;cache() 进行缓存。<br>4.执行行动操作获得最终结果，比如 count() 和 first()等。<br>运行如下的命令来统计本地文件 &#x2F;etc&#x2F;protocols 的词频数量，在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val textFile = sc.textFile(&quot;/etc/protocols&quot;)</span><br><span class="line">val counts = textFile.flatMap(line =&gt; line.split(&quot; &quot;)).map(word =&gt; (word,1)).reduceByKey(_ + _)</span><br><span class="line">counts.saveAsTextFile(&quot;/home/hadoop/counts&quot;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281116129.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281121976.png" alt="img"></p>
<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281128114.png" alt="img"></p>
<p>下面我们来逐步讲解上一段代码的作用：<br>首先，使用 SparkContext 的 textFile() 函数从本地读取 &#x2F;etc&#x2F;protocols 文件将其转化为记录着每一行内容的 RDD。<br>其次，使用 .flatMap(line &#x3D;&gt; line.split(“ “)) 把每一行的内容按照空格分隔开，将其转化为记录着每一个单词的 RDD。<br>然后，使用 .map(word &#x3D;&gt; (word,1)) 把记录着每一个单词的 RDD 转化为 (word,1) 这种表示每一个单词出现一次的键值对形式的 RDD。<br>接着，使用 .reduceByKey(_ + _) 把具有相同键的 RDD 的次数进行相加，求出每个单词的词频。<br>最后，使用 saveAsTextFile() 函数把结果存入 &#x2F;home&#x2F;hadoop&#x2F;counts 文件夹中。<br>RDD 简单实例：WordCount<br>RDD 的转化操作是生成新的 RDD 的操作。<br>在 spark-shell 中执行下面的代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val textFile = sc.textFile(&quot;/etc/protocols&quot;)</span><br><span class="line">textFile.first()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281137927.png" alt="img"></p>
<p>转化的 RDD 都是惰性求值的，只有执行行动操作才会被真正的计算。Spark  只是在内部记录下需要转化的操作，真正有必要时才会执行这些操作，所以应该把 RDD 当做记录如何计算数据的指令列表。比如上面的例子中使用  textFile() 函数读取文件的内容，程序并没有真正的读取这个文件，否则加载大量的数据会占用极大的内存，在遇到执行操作 first()  需要获取第一行的数据时去执行读取文件的第一行并返回数据。从上面的例子中可以看出这样会极大的优化程序执行的效率。</p>
<p>一般转化操作分为两类，一类是对所有 RDD 的每一个元素进行转化，另一类是只对具有相同键的所有 RDD 进行转化。后面这一类被称为键值对 RDD，<br>对于所有 RDD 的每一个元素进行转化的常见操作如下：</p>
<h3 id="4-5-2map"><a href="#4-5-2map" class="headerlink" title="4.5.2map()"></a>4.5.2map()</h3><p>参数是函数，函数应用于 RDD 每一个元素，返回值为新的 RDD。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val nums1 = sc.parallelize(List(1,2,3,3))</span><br><span class="line">nums1.map(x =&gt; x*x).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281146470.png" alt="img"></p>
<h3 id="4-5-3flatMap"><a href="#4-5-3flatMap" class="headerlink" title="4.5.3flatMap()"></a>4.5.3flatMap()</h3><p>参数是函数，把每个输入元素生成多个输出元素，函数应用于 RDD 每一个元素，将元素数据进行拆分变为迭代器，返回值为新的 RDD。通常用来切分单词。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.parallelize(List(&quot;hello world&quot;,&quot;hello bigdata&quot;)).collect()</span><br><span class="line">lines.flatMap(line =&gt; line.split(&quot; &quot;))</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281153888.png" alt="img"></p>
<h3 id="4-5-4filter"><a href="#4-5-4filter" class="headerlink" title="4.5.4filter()"></a>4.5.4filter()</h3><p>参数是函数，使用函数过滤掉不符合条件的元素，返回新的 RDD。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.filter(x =&gt; x!=3).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281161470.png" alt="img"></p>
<h3 id="4-5-5distinct"><a href="#4-5-5distinct" class="headerlink" title="4.5.5distinct()"></a>4.5.5distinct()</h3><p>没有参数，把 RDD 里面的元素进行去重。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.distinct().collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281169708.png" alt="img"></p>
<h3 id="4-5-6union"><a href="#4-5-6union" class="headerlink" title="4.5.6union()"></a>4.5.6union()</h3><p>参数是 RDD，生成包含两个 RDD 所有元素的新 RDD。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val nums2 = sc.parallelize(List(1,2,3))</span><br><span class="line">val nums3 = sc.parallelize(List(3,4,5))</span><br><span class="line">nums2.union(nums3).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281177997.png" alt="img"></p>
<h3 id="4-5-7intersection"><a href="#4-5-7intersection" class="headerlink" title="4.5.7intersection()"></a>4.5.7intersection()</h3><p>参数是 RDD，生成包含两个 RDD 共同元素的新 RDD。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums2.intersection(nums3).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281186735.png" alt="img"></p>
<h3 id="4-5-8subtract"><a href="#4-5-8subtract" class="headerlink" title="4.5.8subtract()"></a>4.5.8subtract()</h3><p>参数是 RDD，将原 RDD 里和参数 RDD 里相同的元素去掉生成新的 RDD。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums2.subtract(nums3).collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281194949.png" alt="img"></p>
<h3 id="4-5-9collect"><a href="#4-5-9collect" class="headerlink" title="4.5.9collect()"></a>4.5.9collect()</h3><p>返回 RDD 中的所有元素。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val nums1 = sc.parallelize(List(1,2,2,3))</span><br><span class="line">nums1.collect()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281202754.png" alt="img"></p>
<h3 id="4-5-10count"><a href="#4-5-10count" class="headerlink" title="4.5.10count()"></a>4.5.10count()</h3><p>计算 RDD 中元素的个数。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.count()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281532587.png" alt="img"></p>
<h3 id="4-5-11countByValue"><a href="#4-5-11countByValue" class="headerlink" title="4.5.11countByValue()"></a>4.5.11countByValue()</h3><p>计算各元素在 RDD 中出现的次数。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.countByValue()</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281547745.png" alt="img"></p>
<h3 id="4-5-12take-num"><a href="#4-5-12take-num" class="headerlink" title="4.5.12take(num)"></a>4.5.12take(num)</h3><p>从 RDD 中返回 num 个元素。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.take(2)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281561498.png" alt="img"></p>
<h3 id="4-5-13top-num"><a href="#4-5-13top-num" class="headerlink" title="4.5.13top(num)"></a>4.5.13top(num)</h3><p>从 RDD 中返回最前面的 num 个元素。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.top(3)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281245034.png" alt="img"></p>
<h3 id="4-5-14reduce-func"><a href="#4-5-14reduce-func" class="headerlink" title="4.5.14reduce(func)"></a>4.5.14reduce(func)</h3><p>并行整合 RDD 中所有数据。reduce 将 RDD 中元素两两传递给输入函数求得一个新值，再把新值与 RDD 中的下一个值一起传递给输入函数直到最后一个值为止。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.reduce((x,y) =&gt; x+y)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281252753.png" alt="img"></p>
<h3 id="4-5-15fold-zero-func"><a href="#4-5-15fold-zero-func" class="headerlink" title="4.5.15fold(zero)(func)"></a>4.5.15fold(zero)(func)</h3><p>和 reduce() 一样，但是需要提供初始值。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.fold(5)((x,y) =&gt; x+y)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281261743.png" alt="img"></p>
<h3 id="4-5-16foreach-func"><a href="#4-5-16foreach-func" class="headerlink" title="4.5.16foreach(func)"></a>4.5.16foreach(func)</h3><p>对 RDD 中每个元素使用给定的元素。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.foreach(println)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281270044.png" alt="img"></p>
<h3 id="4-5-17以文件格式存储"><a href="#4-5-17以文件格式存储" class="headerlink" title="4.5.17以文件格式存储"></a>4.5.17以文件格式存储</h3><p>有 3 个方法，分别是：saveAsTextFile(path)、saveAsSequenceFile(path)、saveAsObjectFile(path)</p>
<p>将 RDD 以不同的文件格式(文本文件、Sequence 格式文件、对象文件)存储在本地文件系统或 Hadoop 文件系统中。<br>在 spark-shell 中输入如下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nums1.saveAsTextFile(&quot;/home/hadoop/nums&quot;)</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281289104.png" alt="img"><br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1676281280979.png" alt="img"></p>
<h2 id="40-综合实战：贷款风险评估"><a href="#40-综合实战：贷款风险评估" class="headerlink" title="40.综合实战：贷款风险评估"></a>40.综合实战：贷款风险评估</h2><blockquote>
<h3 id="目的-19"><a href="#目的-19" class="headerlink" title="目的"></a>目的</h3><p>银行贷款员需要分析数据，以便搞清楚那些贷款申请者是“安全的”，银行的“风险”是什么。这就需要构造一个模型或分类器来预测类标号，其预测结果可以为贷款员放贷提供相关依据。本次实验通过提取贷款用户相关特征（年龄、工作、收入等），使用Spark  MLlib构建风险评估模型，使用相关分类算法将用户分为不同的风险等级，此分类结果可作为银行放贷的参考依据。本次实验为方便演示，选用逻辑回归算法将用户风险等级分类两类：高风险、低风险。有能力的同学可以尝试使用其他分类算法实现。</p>
<h3 id="要求-19"><a href="#要求-19" class="headerlink" title="要求"></a>要求</h3><p>1.熟悉Spark程序开发流程：引入jar包-&gt;打包开发-&gt;打包-&gt;提交服务器运行；<br>2.熟悉Spark Mllib中分类算法的使用流程；<br>3.根据教程引导，对原始数据进行分类器模型训练。</p>
<h3 id="原理-19"><a href="#原理-19" class="headerlink" title="原理"></a>原理</h3><p>在使用分类算法进行数据分类时，均须经过学习与分类两个阶段。</p>
<h3 id="3-1学习阶段："><a href="#3-1学习阶段：" class="headerlink" title="3.1学习阶段："></a>3.1学习阶段：</h3><p>学习阶段按以下步骤执行：<br>1选定样本数据，将该数据集划分为训练样本与测试样本两部分（划分比例自定），训练样本与测试样本不能有重叠部分，否则会严重干扰性能评估。<br>2提取样本数据特征，在训练样本上执行选定的分类算法，生成分类器。<br>3在测试数据上执行分类器，生成测试报告。<br>4根据测试报告，将分类结果类别与真实类别相比较，计算相应的评估标准，评估分类器性能。如果性能不佳，则需要返回第二步，调整相关参数，重新执行形成新的分类器，直至性能评估达到预期要求。</p>
<h3 id="3-2分类阶段"><a href="#3-2分类阶段" class="headerlink" title="3.2分类阶段"></a>3.2分类阶段</h3><p>分类阶段按以下步骤执行：<br>1搜集新样本，并对新样本进行特征提取。<br>2使用在学习阶段生成的分类器，对样本数据进行分类。<br>3判别新样本的所属类别。<br>spark-submit使用详解<br>将jar包提交服务器运行时，需要执行spark-submit相关命令才能运行，spark-submit 执行时命令格式如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit [options] &lt;app jar | python file&gt; [app options]</span><br></pre></td></tr></table></figure>

<p>需要传入的参数说明如表3-1所示。<br>表3-1 参数说明</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">    参数名称                                具体含义</span><br><span class="line">--master MASTER_URL           可以是spark://host:port, mesos://host:port, yarn, yarn-cluster,yarn-client, local</span><br><span class="line">--deploy-mode DEPLOY_MODE   Driver程序运行的地方，client或者cluster</span><br><span class="line">--class CLASS_NAME           主类名称，含包名</span><br><span class="line">--name NAME                Application名称</span><br><span class="line">--jars JARS                Driver依赖的第三方jar包</span><br><span class="line">--py-files PY_FILES           用逗号隔开的放置在Python应用程序PYTHONPATH上的.zip, .egg, .py文件列表</span><br><span class="line">--files FILES             用逗号隔开的要放置在每个executor工作目录的文件列表</span><br><span class="line">--properties-file FILE        设置应用程序属性的文件路径，默认是conf/Spark-defaults.conf</span><br><span class="line">--driver-memory MEM           Driver程序使用内存大小</span><br><span class="line">--driver-library-path        Driver程序的库路径</span><br><span class="line">--driver-class-path       Driver程序的类路径</span><br><span class="line">--executor-memory MEM       executor内存大小，默认1G</span><br><span class="line">--driver-cores NUM          Driver程序的使用CPU个数，仅限于Spark Alone模式</span><br><span class="line">--supervise               失败后是否重启Driver，仅限于Spark Alone模式</span><br><span class="line">--total-executor-cores NUM    executor使用的总核数，仅限于Spark Alone、Spark on Mesos模式</span><br><span class="line">--executor-cores NUM       每个executor使用的内核数，默认为1，仅限于Spark on Yarn模式</span><br><span class="line">--queue QUEUE_NAME         提交应用程序给哪个YARN的队列，默认是default队列，仅限于Spark on Yarn模式</span><br><span class="line">--num-executors NUM         启动的executor数量，默认是2个，仅限于Spark on Yarn模式</span><br><span class="line">--archives ARCHIVES         仅限于Spark on Yarn模式</span><br></pre></td></tr></table></figure></blockquote>
<h3 id="4-1-实验数据"><a href="#4-1-实验数据" class="headerlink" title="4.1 实验数据"></a>4.1 实验数据</h3><p>我们的数据在以下路径获取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/root/data/spark/loan/adult.csv</span><br></pre></td></tr></table></figure>

<p>数据内容解释如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">（1）risk-rating:0, 1；</span><br><span class="line">（2）age: continuous；</span><br><span class="line">（3）workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked；</span><br><span class="line">（4）fnlwgt: continuous；</span><br><span class="line">（5）education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool；</span><br><span class="line">（6）education-num: continuous；</span><br><span class="line">（7）marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse；</span><br><span class="line">（8）occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces；</span><br><span class="line">（9）relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried；</span><br><span class="line">（10）race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black；</span><br><span class="line">（11）sex: Female, Male；</span><br><span class="line">（12）capital-gain: continuous；</span><br><span class="line">（13）capital-loss: continuous；</span><br><span class="line">（14）hours-per-week: continuous；</span><br><span class="line">（15）native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&amp;Tobago, Peru, Hong, Holand-Netherlands。</span><br></pre></td></tr></table></figure>

<p>从master上传原始数据文件至HDFS的&#x2F;input目录中</p>
<h3 id="4-2实验操作"><a href="#4-2实验操作" class="headerlink" title="4.2实验操作"></a>4.2实验操作</h3><p>4.2.1IDEA配置：<br>在IntelliJ IDEA中需要导入Spark开发包，Spark&#x2F;lib中的jar包能满足基本的开发需求，开发者可以在菜单：File-&gt;project stucture-&gt;Libraries中设置。如图所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677825185443.png" alt="img"><br>4.2.2代码步骤：<br>获取源数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val path = &quot;hdfs://master:8020/input/adult.csv&quot;</span><br><span class="line">val rawData = sc.textFile(path)</span><br></pre></td></tr></table></figure>

<p>简单的数据清洗。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 取第一列为类标，其余列作为特征值</span><br><span class="line">*/</span><br><span class="line">val data = records.map&#123; point =&gt;val firstdata = point.map(_.replaceAll(&quot; &quot;,&quot;&quot;))</span><br><span class="line">  val replaceData=firstdata.map(_.replaceAll(&quot;,&quot;,&quot; &quot;))</span><br><span class="line">val temp = replaceData(0).split(&quot; &quot;)</span><br><span class="line">  val label=temp(0).toInt</span><br><span class="line">  val feature s = temp.slice(1,temp.size-1)</span><br><span class="line">             .map(_.hashCode)</span><br><span class="line">             .map(x =&gt; x.toDouble)</span><br><span class="line">  LabeledPoint(label,Vectors.dense(features))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>按照一定的比例将数据随机分为训练集和测试集。<br>这里需要程序开发者不断的调试比例以达到预期的准确率，值得注意的是，不当的划分比例导致“欠拟合”或“过拟合”的情况产生。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val splits = data.randomSplit(Array(0.8,0.2),seed = 11L)</span><br><span class="line">val traning = splits(0).cache()</span><br><span class="line">val test = splits(1)</span><br></pre></td></tr></table></figure>

<p>训练分类模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val model = new LogisticRegressionWithLBFGS().setNumClasses(2).run(traning)</span><br></pre></td></tr></table></figure>

<p>预测测试样本的类别。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val predictionAndLabels = test.map&#123;</span><br><span class="line">case LabeledPoint(label,features) =&gt;val prediction = model.predict(features)</span><br><span class="line">    (prediction,label)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>计算并输出准确率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val metrics = new BinaryClassificationMetrics(predictionAndLabels)</span><br><span class="line">val auRoc = metrics.areaUnderROC()</span><br><span class="line">println(&quot;Area under Roc =&quot; + auRoc)</span><br></pre></td></tr></table></figure>

<p>输出权重最大的前10个特征。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val weights = (1 to model.numFeatures) zip model.weights.toArray</span><br><span class="line">println(&quot;Top 5 features:&quot;)</span><br><span class="line">weights.sortBy(-_._2).take(5).foreach&#123;case(k,w) =&gt;println(&quot;Feature &quot; + k + &quot; = &quot; + w)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>保存与加载模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val modelPath = &quot;hdfs://master:8020/output/&quot;</span><br><span class="line">model.save(sc, modelPath)</span><br><span class="line">val sameModel = LogisticRegressionModel.load(sc,modelPath)</span><br></pre></td></tr></table></figure>

<p>4.2.3代码实例</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.classification.LogisticRegressionModel</span><br><span class="line">import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS</span><br><span class="line">import org.apache.spark.mllib.evaluation.&#123;BinaryClassificationMetrics, MulticlassMetrics&#125;</span><br><span class="line">import org.apache.spark.mllib.regression.LabeledPoint</span><br><span class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line">import org.apache.log4j.&#123;Level, Logger&#125;</span><br><span class="line">import org.apache.spark.mllib.linalg.Vectors</span><br><span class="line"></span><br><span class="line">object LRCode &#123;</span><br><span class="line">  def main(args:Array[String]): Unit = &#123;</span><br><span class="line">    val conf = new SparkConf()</span><br><span class="line">                  .setAppName(&quot;Logisitic Test&quot;)</span><br><span class="line">                  .setMaster(&quot;spark://master:7077&quot;)</span><br><span class="line">    val sc = new SparkContext(conf)</span><br><span class="line"></span><br><span class="line">    //屏蔽不必要的日志信息</span><br><span class="line">    Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)</span><br><span class="line">    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)</span><br><span class="line"></span><br><span class="line">    //使用MLUtils对象将hdfs中的数据读取到RDD中</span><br><span class="line">    val path = &quot;hdfs://master:8020/input/adult.csv&quot;</span><br><span class="line">    val rawData = sc.textFile(path)</span><br><span class="line"></span><br><span class="line">    val startTime = System.currentTimeMillis()</span><br><span class="line">    println(&quot;startTime:&quot;+startTime)</span><br><span class="line"></span><br><span class="line">    //通过“\t”即按行对数据内容进行分割</span><br><span class="line">    val records = rawData.map(_.split(&quot;\t&quot;))</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">      * 取第一列为类标，其余列作为特征值</span><br><span class="line">      */</span><br><span class="line">    val data = records.map&#123; point =&gt;      //去除集合中多余的空格</span><br><span class="line">      val firstdata = point.map(_.replaceAll(&quot; &quot;,&quot;&quot;))</span><br><span class="line">      //用空格代替集合中的逗号</span><br><span class="line">      val replaceData=firstdata.map(_.replaceAll(&quot;,&quot;,&quot; &quot;))</span><br><span class="line">      val temp = replaceData(0).split(&quot; &quot;)</span><br><span class="line">      val label=temp(0).toInt</span><br><span class="line">      val features = temp.slice(1,temp.size-1)</span><br><span class="line">        .map(_.hashCode)</span><br><span class="line">        .map(x =&gt; x.toDouble)</span><br><span class="line">      LabeledPoint(label,Vectors.dense(features))</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //按照3:2的比例将数据随机分为训练集和测试集</span><br><span class="line">    val splits = data.randomSplit(Array(0.8,0.2),seed = 11L)</span><br><span class="line">    val traning = splits(0).cache()</span><br><span class="line">    val test = splits(1)</span><br><span class="line"></span><br><span class="line">    //训练二元分类的logistic回归模型</span><br><span class="line">    val model = new LogisticRegressionWithLBFGS().setNumClasses(2).run(traning)</span><br><span class="line"></span><br><span class="line">    //预测测试样本的类别</span><br><span class="line">    val predictionAndLabels = test.map&#123;</span><br><span class="line">      case LabeledPoint(label,features) =&gt; val prediction = model.predict(features)</span><br><span class="line">        (prediction,label)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //输出模型在样本上的准确率</span><br><span class="line">    val metrics = new BinaryClassificationMetrics(predictionAndLabels)</span><br><span class="line">    val auRoc = metrics.areaUnderROC()</span><br><span class="line">    //打印准确率</span><br><span class="line">    println(&quot;Area under Roc =&quot; + auRoc)</span><br><span class="line"></span><br><span class="line">    //计算统计分类耗时</span><br><span class="line">    val endTime = System.currentTimeMillis()</span><br><span class="line">    println(&quot;endtime:&quot;+endTime)</span><br><span class="line">    val timeConsuming = endTime - startTime</span><br><span class="line">    println(&quot;timeConsuming:&quot;+timeConsuming)</span><br><span class="line"></span><br><span class="line">    //输出逻辑回归权重最大的前5个特征</span><br><span class="line">    val weights = (1 to model.numFeatures) zip model.weights.toArray</span><br><span class="line">    println(&quot;Top 5 features:&quot;)</span><br><span class="line">    weights.sortBy(-_._2).take(5).foreach&#123;case(k,w) =&gt;      println(&quot;Feature &quot; + k + &quot; = &quot; + w)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    //保存训练好模型</span><br><span class="line">    val modelPath = &quot;hdfs://master:8020/output/&quot;</span><br><span class="line">    model.save(sc, modelPath)</span><br><span class="line">val sameModel = LogisticRegressionModel.load(sc,modelPath)</span><br><span class="line"></span><br><span class="line">    //关闭程序</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>4.2.4服务器运行<br>编译器打包：<br>1菜单：File-&gt;project stucture (也可以按快捷键ctrl+alt+shift+s)。<br>2在弹窗最左侧选中Artifacts-&gt;左数第二个区域点击’+’,选择jar，然后选择from modules with dependencies，然后会有配置窗口出现，配置完成后，勾选Build On make (make  项目的时候会自动输出jar)-&gt;保存设置。如图所示：<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677825243050.png" alt="img"><br>3然后菜单：Build-&gt;make project。<br>4最后在项目目录下去找输出的jar包。<br>代码运行:<br>a. hdfs中创建文件夹。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># hadoop fs -mkdir /input</span><br><span class="line"># hadoop fs -mkdir /output</span><br></pre></td></tr></table></figure>

<p>b.将数据提交至hdfs。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hdfs dfs -put /root/data/spark/loan/adult.csv /input</span><br></pre></td></tr></table></figure>

<p>c.将jar包提交服务器，执行以下命令。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class LRCode --num-executors 3 --executor-memory 1g --executor-cores 3 Assessment.jar</span><br></pre></td></tr></table></figure>

<p>使用IntelliJ IDEA编写贷款风险评估程序，打包为Assessment.jar，并上传至master服务器上</p>
<blockquote>
<p><img src="/static/upload/resource/exp/ins/a4ff1e04e26344bcb6e504b2c72da4db/image/image.1677825253608.png" alt="image.png"> 由图可知，该分类模型准确率约为71.2%，耗时为23579毫秒，权重最大的前五个特征为第5、6、11、12、13个特征。</p>
</blockquote>
<h2 id="41-综合实战：电影推荐系统"><a href="#41-综合实战：电影推荐系统" class="headerlink" title="41.综合实战：电影推荐系统"></a>41.综合实战：电影推荐系统</h2><blockquote>
<h3 id="目的-20"><a href="#目的-20" class="headerlink" title="目的"></a>目的</h3><p>1了解常用的基于矩阵分解的协同过滤推荐算法的基本原理<br>2掌握Spark MLlib中对基于模型的协同过滤算法的封装函数的使用<br>3对Spark中机器学习模块内容加深理解，根据电影推荐系统得出十部最佳电影 </p>
<h3 id="要求-20"><a href="#要求-20" class="headerlink" title="要求"></a>要求</h3><p>1根据提供的电影评分数据，利用Spark进行训练，得到一个最佳推荐模型；<br>2用实际数据和平均值这两方面评价该模型的准确度；<br>3根据我的评分数据向我推荐10部电影。</p>
<h3 id="原理-20"><a href="#原理-20" class="headerlink" title="原理"></a>原理</h3><p>3.1原理概述<br>协同过滤算法按照数据使用，可以分为：<br>（1）基于用户（UserCF）；<br>（2）基于商品（ItemCF）；<br>（3）基于模型（ModelCF）。<br>按照模型，又可以分为：<br>（1）最近邻模型：基于距离的协同过滤算法；<br>（2）Latent Factor Mode（SVD）：基于矩阵分解的模型；<br>（3）Graph：图模型，社会网络图模型。<br>本次实验，使用的协同过滤算法是基于矩阵分解的模型，就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐。<br>ALS是alternating least  squares的缩写，意为交替最小二乘法，该方法常用于基于矩阵分解的推荐系统中。对于一个R（观众对电影的一个评价矩阵）可以分解为U（观众的特征矩阵）和V（电影的特征矩阵），在这个矩阵分解的过程中，评分缺失项得到了填充，也就是说我们可以基于这个填充的评分来给用户最适合的商品推荐了。</p>
<p>MLlib支持基于模型的协同过滤算法，其中user和product对应图中的user和movie，user和product之间有一些隐藏因子。MLlib使用ALS(alternating least squares)来学习得到这些<br>潜在因子。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677815039468.png" alt="img"><br>原始矩阵R可能是非常稀疏的，但乘机UV是稠密的，即使该矩阵存在非零元素，非零元素的数量也非常少。因此模型只是对R的一种近似。原始矩阵R中大量元素是缺失的（元素值为0），算法为这些缺失元素生成（补全)了一个值，从这个角度讲，我们可以把算法称为模型。根据这个补全的矩阵，我们就可以从知道user也就知道了movies，或者知道movie也就知道了users，这就是以下实验推荐算法的基本原理。</p>
</blockquote>
<h3 id="4-1数据集准备"><a href="#4-1数据集准备" class="headerlink" title="4.1数据集准备"></a>4.1数据集准备</h3><p>构建模型的第一步是了解数据，对数据进行解析或转换，以便在Spark中作分析。Spark MLlib的ALS算法要求用户和产品的ID必须都是数值型，并且是32位非负整数，以下准备的数据集完全符合Spark  MLlib的ALS算法要求，不必进行转换，可直接使用。<br>在本地目录&#x2F;root&#x2F;data&#x2F;spark&#x2F;movie_recommend&#x2F;movie下有本次实验数据集，文件列表如图所示。<br><img src="/./assets/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/image.1677815046718.png" alt="img"><br>各文件数据格式如下（详细见README文件）：<br>（1）用户数据（users.dat）<br>用户ID::性别::年龄::职业编号::邮编。<br>6031::F::18::0::45123<br>6032::M::45::7::55108<br>6033::M::50::13::78232<br>6034::M::25::14::94117<br>6035::F::25::1::78734<br>6036::F::25::15::32603<br>6037::F::45::1::76006<br>6038::F::56::1::14706<br>6039::F::45::0::01060<br>6040::M::25::6::11106<br>（2）电影数据（movies.dat）<br>电影ID::电影名称::电影种类。<br>3943::Bamboozled (2000)::Comedy<br>3944::Bootmen (2000)::Comedy|Drama<br>3945::Digimon: The Movie (2000)::Adventure|Animation|Children’s<br>3946::Get Carter (2000)::Action|Drama|Thriller<br>3947::Get Carter (1971)::Thriller<br>3948::Meet the Parents (2000)::Comedy<br>3949::Requiem for a Dream (2000)::Drama<br>3950::Tigerland (2000)::Drama<br>3951::Two Family House (2000)::Drama<br>3952::Contender, The (2000)::Drama|Thriller<br>（3）评分数据（ratings.dat）<br>用户ID::电影ID::评分::时间。<br>6040::2022::5::956716207<br>6040::2028::5::956704519<br>6040::1080::4::957717322<br>6040::1089::4::956704996<br>6040::1090::3::956715518<br>6040::1091::1::956716541<br>6040::1094::5::956704887<br>6040::562::5::956704746<br>6040::1096::4::956715648<br>6040::1097::4::956715569<br>（4）我的评分数据（test.dat）<br>用户ID::电影ID::评分::时间。<br>0::780::4::1409495135<br>0::590::3::1409495135<br>0::1210::4::1409495135<br>0::648::5::1409495135<br>0::344::3::1409495135<br>0::165::4::1409495135<br>0::153::5::1409495135<br>0::597::4::1409495135<br>0::1580::5::1409495135<br>将以上数据文件上传到HDFS文件系统：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/hadoop/bin</span><br><span class="line"># hdfs dfs -copyFromLocal /root/data/spark/movie_recommend/movie /</span><br></pre></td></tr></table></figure>

<h3 id="4-2代码实现"><a href="#4-2代码实现" class="headerlink" title="4.2代码实现"></a>4.2代码实现</h3><p>为防止shell端INFO日志刷屏，影响查看打印信息，修改打印日志级别，进入Spark安装的conf目录下，将log4j.properties.template文件复制一份，命名log4j.properties文件，然后将文件如下配置项：<br>log4j.rootCategory&#x3D;WARN, console<br>进入Spark安装目录下bin目录，启动spark-shell：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cd /usr/cstor/spark/bin</span><br><span class="line"># ./spark-shell --master spark://master:7077</span><br></pre></td></tr></table></figure>

<p>具体代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">/** 导入Spark机器学习推荐算法相关包 **/</span><br><span class="line">  import org.apache.spark.mllib.recommendation.&#123;ALS, Rating, MatrixFactorizationModel&#125;</span><br><span class="line">  import org.apache.spark.rdd.RDD</span><br><span class="line"></span><br><span class="line">  /** 定义函数，校验集预测数据和实际数据之间的均方根误差，后面会调用此函数 **/</span><br><span class="line">  def computeRmse(model:MatrixFactorizationModel,data:RDD[Rating],n:Long):Double = &#123;</span><br><span class="line">    val predictions:RDD[Rating] = model.predict((data.map(x =&amp;gt; (x.user,x.product))))</span><br><span class="line">    val predictionsAndRatings = predictions.map&#123; x =&amp;gt;((x.user,x.product),x.rating)&#125;</span><br><span class="line">      .join(data.map(x =&amp;gt; ((x.user,x.product),x.rating))).values</span><br><span class="line">    math.sqrt(predictionsAndRatings.map( x =&amp;gt; (x._1 - x._2) * (x._1 - x._2)).reduce(_+_)/n)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  /** 加载数据 **/</span><br><span class="line">  //1、我的评分数据(test.dat),转成Rating格式，即用户id，电影id，评分</span><br><span class="line">    val myRatingsRDD = sc.textFile(&quot;/movie/test.dat&quot;).map &#123;</span><br><span class="line">      line =&amp;gt;        val fields = line.split(&quot;::&quot;)</span><br><span class="line">        // format: Rating(userId, movieId, rating)</span><br><span class="line">        Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble)</span><br><span class="line">    &#125;</span><br><span class="line">  //2、样本评分数据(ratings.dat)，其中最后一列Timestamp取除10的余数作为key，Rating为值，</span><br><span class="line"> 即(Int，Rating)，以备后续数据切分</span><br><span class="line">    val ratings = sc.textFile(&quot;/movie/ratings.dat&quot;).map &#123;</span><br><span class="line">      line =&amp;gt;        val fields = line.split(&quot;::&quot;)</span><br><span class="line">        // format: (timestamp % 10, Rating(userId, movieId, rating))</span><br><span class="line">        (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))</span><br><span class="line">    &#125;</span><br><span class="line">    //3、电影数据(movies.dat)(电影ID-&amp;gt;电影标题)</span><br><span class="line">    val movies = sc.textFile(&quot;/movie/movies.dat&quot;).map &#123;</span><br><span class="line">      line =&amp;gt;        val fields = line.split(&quot;::&quot;)</span><br><span class="line">        // format: (movieId, movieName)</span><br><span class="line">        (fields(0).toInt, fields(1))</span><br><span class="line">    &#125;.collect().toMap</span><br><span class="line"></span><br><span class="line">    /** 统计所有用户数量和电影数量以及用户对电影的评分数目 **/</span><br><span class="line">    val numRatings = ratings.count()</span><br><span class="line">    val numUsers = ratings.map(_._2.user).distinct().count()</span><br><span class="line">    val numMovies = ratings.map(_._2.product).distinct().count()</span><br><span class="line">    println(&quot;total number of rating data: &quot; + numRatings)</span><br><span class="line">    println(&quot;number of users participating in the score: &quot; + numUsers)</span><br><span class="line">    println(&quot;number of participating movie data: &quot; + numMovies)</span><br><span class="line"></span><br><span class="line">/** 将样本评分表以key值切分成3个部分，分别用于训练（60%，并加入我的评分数据）、</span><br><span class="line"> 校验（20%）以及测试（20%） **/</span><br><span class="line">    //定义分区数，即数据并行度</span><br><span class="line">     val numPartitions = 4</span><br><span class="line">    //因为以下数据在计算过程中要多次应用到，所以cache到内存</span><br><span class="line">    //训练数据集，包含我的评分数据</span><br><span class="line">    val training = ratings.filter(x =&amp;gt; x._1 &amp;lt; 6).values.union(myRatingsRDD).repartition(numPartitions).persist()</span><br><span class="line">    //验证数据集</span><br><span class="line">    val validation = ratings.filter(x =&amp;gt; x._1 &amp;gt;= 6 &amp;&amp; x._1 &amp;lt; 8).values.repartition(numPartitions).persist()</span><br><span class="line">    //测试数据集</span><br><span class="line">    val test = ratings.filter(x =&amp;gt; x._1 &amp;gt;= 8).values.persist()</span><br><span class="line">    //统计各数据集数量</span><br><span class="line">    val numTraining = training.count()</span><br><span class="line">    val numValidation = validation.count()</span><br><span class="line">    val numTest = test.count()</span><br><span class="line">    println(&quot;the number of scoring data for training) (including my score data): &quot; + numTraining)</span><br><span class="line">    println(&quot;number of rating data as validation: &quot; + numValidation)</span><br><span class="line">    println(&quot;number of rating data as a test: &quot; + numTest)</span><br><span class="line"></span><br><span class="line">/** 训练不同参数下的模型，获取最佳模型 **/</span><br><span class="line">//设置训练参数及最佳模型初始化值</span><br><span class="line">//模型的潜在因素的个数，即U和V矩阵的列数，也叫矩阵的阶</span><br><span class="line">val ranks = List(8, 12)</span><br><span class="line">//标准的过拟合参数</span><br><span class="line">val lambdas = List(0.1, 10.0)</span><br><span class="line">//矩阵分解迭代次数，次数越多花费时间越长，分解的结果也可能会更好</span><br><span class="line">    val numIters = List(10, 20)</span><br><span class="line">    var bestModel: Option[MatrixFactorizationModel] = None</span><br><span class="line">    var bestValidationRmse = Double.MaxValue</span><br><span class="line">    var bestRank = 0</span><br><span class="line">    var bestLambda = -1.0</span><br><span class="line">    var bestNumIter = -1</span><br><span class="line">    //根据设定的训练参数对训练数据集进行训练</span><br><span class="line">for (rank &amp;lt;- ranks; lambda &amp;lt;- lambdas; numIter &amp;lt;- numIters) &#123;</span><br><span class="line">  //计算模型</span><br><span class="line">      val model = ALS.train(training, rank, numIter, lambda)</span><br><span class="line">      //计算针对校验集的预测数据和实际数据之间的均方根误差</span><br><span class="line">      val validationRmse = computeRmse(model, validation, numValidation)</span><br><span class="line">      println(&quot;Root mean square: &quot; + validationRmse + &quot; Parameter: --rank = &quot;</span><br><span class="line">        + rank + &quot; --lambda = &quot; + lambda + &quot; --numIter = &quot; + numIter + &quot;.&quot;)</span><br><span class="line"></span><br><span class="line">      //均方根误差最小的为最佳模型</span><br><span class="line">      if (validationRmse &amp;lt; bestValidationRmse) &#123;</span><br><span class="line">        bestModel = Some(model)</span><br><span class="line">        bestValidationRmse = validationRmse</span><br><span class="line">        bestRank = rank</span><br><span class="line">        bestLambda = lambda</span><br><span class="line">        bestNumIter = numIter</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    /** 用训练的最佳模型预测评分并评估模型准确度 **/</span><br><span class="line">    //训练完成后，用最佳模型预测测试集的评分，并计算和实际评分之间的均方根误差（RMSE）</span><br><span class="line">    val testRmse = computeRmse(bestModel.get, test, numTest)</span><br><span class="line">    println(&quot;Optimal model parameters --rank = &quot; + bestRank + &quot; --lambda = &quot; + bestLambda</span><br><span class="line">                     + &quot; --numIter = &quot; + bestNumIter +</span><br><span class="line">          &quot; \nThe root mean square between the predicted data and the real data under the optimal model: &quot; + testRmse + &quot;.&quot;)</span><br><span class="line"></span><br><span class="line">    //创建一个用均值预测的评分，并与最好的模型进行比较，这个mean（）方法在DoubleRDDFunctions中，求平均值</span><br><span class="line">    val meanRating = training.union(validation).map(_.rating).mean</span><br><span class="line">    val baselineRmse = math.sqrt(test.map(x =&amp;gt; (meanRating - x.rating) * (meanRating - x.rating))</span><br><span class="line">    .reduce(_ + _) / numTest)</span><br><span class="line">    println(&quot;Root mean square between mean prediction data and real data: &quot; + baselineRmse + &quot;.&quot;)</span><br><span class="line">    val improvement = (baselineRmse - testRmse) / baselineRmse * 100</span><br><span class="line">    println(&quot;The accuracy of the prediction data of the best model with respect to the mean prediction data: &quot; + &quot;%1.2f&quot;.format(improvement) + &quot;%.&quot;)</span><br><span class="line"></span><br><span class="line">    //向我推荐十部最感兴趣的电影</span><br><span class="line">    val recommendations = bestModel.get.recommendProducts(0,10)</span><br><span class="line">    //打印推荐结果</span><br><span class="line">    var i = 1</span><br><span class="line">    println(&quot;10 films recommended to me:&quot;)</span><br><span class="line">    recommendations.foreach &#123; r =&amp;gt;      println(&quot;%2d&quot;.format(i) + &quot;: &quot; + movies(r.product))</span><br><span class="line">      i += 1</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>代码执行过程中打印日志信息如下： 所有数据数量统计图: <img src="/static/upload/resource/exp/ins/f3c8891d703445d3bfc3133a3a6842f4/image/image.1677815060096.png" alt="image.png"> 评分数据切分的各数据集统计图: <img src="/static/upload/resource/exp/ins/f3c8891d703445d3bfc3133a3a6842f4/image/image.1677815066194.png" alt="image.png"> 训练时的参数及对应的误差图: <img src="/static/upload/resource/exp/ins/f3c8891d703445d3bfc3133a3a6842f4/image/image.1677815071466.png" alt="image.png"> 最佳模型的参数及对应的误差图: <img src="/static/upload/resource/exp/ins/f3c8891d703445d3bfc3133a3a6842f4/image/image.1677815078879.png" alt="image.png"> 均值预测的误差图: <img src="/static/upload/resource/exp/ins/f3c8891d703445d3bfc3133a3a6842f4/image/image.1677815084791.png" alt="image.png"> 最佳模型预测相比均值预测比较图: <img src="/static/upload/resource/exp/ins/f3c8891d703445d3bfc3133a3a6842f4/image/image.1677815094002.png" alt="image.png"> 最佳模型下向我推荐的电影图: <img src="/static/upload/resource/exp/ins/f3c8891d703445d3bfc3133a3a6842f4/image/image.1677815100965.png" alt="image.png"></p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">tong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/">http://example.com/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://example.com" target="_blank">Blog</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/./img/touxiang.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/./img/WeChat_pay.jpg" target="_blank"><img class="post-qr-code-img" src="/./img/WeChat_pay.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/./img/Alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/./img/Alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/" title="Hadoop应用与开发1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Hadoop应用与开发1</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/" title="Hadoop应用与开发3"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Hadoop应用与开发3</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发342.Flink实验：Flink简介与配置 目的1.了解Flink特性、功能模块、编程模型、构架模型并学会Flink各种模式的环境部署。 要求本次试验后，要求学生能：1.深入了解Flink的特性、功能模块、编程模型、构架模型；2.掌握Flink各种模式的环境部署 原理3.1Flink介绍Flink起源于一个名为Stratosphere的研究项目，目的是建立下一代大数据分析平台，于2014年4月16日成为Apache孵化器项目。 Apache Flink是一个面向数据流处理和批量数据处理的可分布式的开源计算框架，它基于同一个Flink流式执行模型（streaming execution...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/" title="Hadoop应用与开发3"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-16</div><div class="info-item-2">Hadoop应用与开发3</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发342.Flink实验：Flink简介与配置 目的1.了解Flink特性、功能模块、编程模型、构架模型并学会Flink各种模式的环境部署。 要求本次试验后，要求学生能：1.深入了解Flink的特性、功能模块、编程模型、构架模型；2.掌握Flink各种模式的环境部署 原理3.1Flink介绍Flink起源于一个名为Stratosphere的研究项目，目的是建立下一代大数据分析平台，于2014年4月16日成为Apache孵化器项目。 Apache Flink是一个面向数据流处理和批量数据处理的可分布式的开源计算框架，它基于同一个Flink流式执行模型（streaming execution...</div></div></div></a><a class="pagination-related" href="/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/" title="Hadoop应用与开发1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-20</div><div class="info-item-2">Hadoop应用与开发1</div></div><div class="info-2"><div class="info-item-1">...</div></div></div></a><a class="pagination-related" href="/2025/06/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%914/" title="Hadoop应用与开发4"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">Hadoop应用与开发4</div></div><div class="info-2"><div class="info-item-1">Hadoop应用与开发459.Hadoop实验：HDFS基础与部署 目的1.理解大数据生态圈各组件特性2.理解HDFS存在的优势3.理解HDFS体系架构4.学会在环境中部署HDFS学会HDFS基本命令 要求1.要求实验结束时，能够构建出HDFS集群2.要求能够在构建出的HDFS集群上使用基本的HDFS命令3.要求能够大致了解Hadoop生态圈中各个组件 原理3.1 Hadoop 生态：Hadoop是一个提供高可靠，可扩展（横向）的分布式计算的开源软件平台。1.Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构。2.主要解决，海量数据的存储和海量数据的分析计算问题。3.我们现在讲HADOOP 通常是指一个更广泛的概念——HADOOP...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/./img/touxiang.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">tong</div><div class="author-info-description">这里是描述</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">7</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">现在没有公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912"><span class="toc-text">Hadoop应用与开发2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#21-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASparkSQL%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98"><span class="toc-text">21.Spark实验：SparkSQL入门实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#22-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ARDD%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-text">22.Spark实验：RDD高级应用与持久化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-1"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-1"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-1"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%91Spark%E4%BC%A0%E9%80%92%E5%87%BD%E6%95%B0"><span class="toc-text">4.1向Spark传递函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E7%90%86%E8%A7%A3%E9%97%AD%E5%8C%85"><span class="toc-text">4.2理解闭包</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F"><span class="toc-text">4.3共享变量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E9%94%AE%E5%80%BC%E5%AF%B9RDD"><span class="toc-text">4.4键值对RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E8%BD%AC%E5%8C%96%E6%93%8D%E4%BD%9C"><span class="toc-text">4.5转化操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6%E8%A1%8C%E5%8A%A8%E6%93%8D%E4%BD%9C"><span class="toc-text">4.6行动操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8BRDD%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BD%AC%E6%8D%A2"><span class="toc-text">4.7不同类型RDD之间的转换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8RDD%E7%9A%84%E7%BC%93%E5%AD%98%E3%80%81%E6%8C%81%E4%B9%85%E5%8C%96"><span class="toc-text">4.8RDD的缓存、持久化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-9RDD-%E7%BC%93%E5%AD%98"><span class="toc-text">4.9RDD 缓存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-10RDD"><span class="toc-text">4.10RDD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#23-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASparkStreaming"><span class="toc-text">23.Spark实验：SparkStreaming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-2"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-2"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-2"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1Spark-Streaming%E6%9E%B6%E6%9E%84"><span class="toc-text">3.1Spark Streaming架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2Spark-Streaming%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2Spark Streaming编程模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Spark-Streaming%E5%85%B8%E5%9E%8B%E6%A1%88%E4%BE%8B"><span class="toc-text">3.3 Spark Streaming典型案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E7%BC%96%E5%86%99Spark-steaming%E4%BB%A3%E7%A0%81"><span class="toc-text">4.1编写Spark-steaming代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#24-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASparkWordCount"><span class="toc-text">24.Spark实验：SparkWordCount</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-3"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-3"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-3"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Scala%E6%98%AF%E5%85%BC%E5%AE%B9%E7%9A%84"><span class="toc-text">3.1 Scala是兼容的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2Scala%E6%98%AF%E7%AE%80%E6%B4%81%E7%9A%84"><span class="toc-text">3.2Scala是简洁的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3Scala%E6%98%AF%E9%AB%98%E7%BA%A7%E7%9A%84"><span class="toc-text">3.3Scala是高级的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4Scala%E6%98%AF%E9%9D%99%E6%80%81%E7%B1%BB%E5%9E%8B%E7%9A%84"><span class="toc-text">3.4Scala是静态类型的</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E7%BC%96%E5%86%99WordCount"><span class="toc-text">4.1编写WordCount</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#25-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASpark%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98"><span class="toc-text">25.Spark实验：Spark机器学习入门实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-4"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-4"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-4"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1"><span class="toc-text">4.1数据统计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E4%BD%BF%E7%94%A8Spark-ml"><span class="toc-text">4.2使用Spark.ml</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95"><span class="toc-text">4.3朴素贝叶斯算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E4%BA%8C%E5%88%86-K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95"><span class="toc-text">4.4二分 K 均值算法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#26-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASpark%E7%BB%BC%E4%BE%8B"><span class="toc-text">26.Spark实验：Spark综例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-5"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-5"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-5"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1Scala"><span class="toc-text">3.1Scala</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2Spark-Shell"><span class="toc-text">3.2Spark Shell</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%90%AF%E5%8A%A8Spark-Shell"><span class="toc-text">4.1 启动Spark Shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E7%BC%96%E5%86%99%E5%B9%B6%E6%89%A7%E8%A1%8CScala%E4%BB%A3%E7%A0%81"><span class="toc-text">4.2编写并执行Scala代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E6%9F%A5%E7%9C%8B%E6%89%A7%E8%A1%8C%E7%BB%93%E6%9E%9C"><span class="toc-text">4.3查看执行结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#27-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASpark%E5%AE%9E%E7%8E%B0%E6%B5%81%E9%87%8F%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90"><span class="toc-text">27.Spark实验：Spark实现流量日志分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-6"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-6"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-6"><span class="toc-text">原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1%E6%97%A5%E5%BF%97%E6%A0%BC%E5%BC%8F%E7%AE%80%E4%BB%8B"><span class="toc-text">3.1日志格式简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2%E6%97%A5%E5%BF%97%E5%90%84%E5%AD%97%E6%AE%B5%E8%AF%B4%E6%98%8E"><span class="toc-text">3.2日志各字段说明</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE"><span class="toc-text">4.1 准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%88%9B%E5%BB%BA-maven-%E9%A1%B9%E7%9B%AE"><span class="toc-text">4.2 创建 maven 项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81"><span class="toc-text">4.3 编写代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E8%BF%90%E8%A1%8C%E4%BB%A3%E7%A0%81"><span class="toc-text">4.4 运行代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#28-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASpark%E5%AE%9E%E7%8E%B0%E9%BB%91%E5%90%8D%E5%8D%95%E5%AE%9E%E6%97%B6%E8%BF%87%E6%BB%A4"><span class="toc-text">28.Spark实验：Spark实现黑名单实时过滤</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-7"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-7"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-7"><span class="toc-text">原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1%E5%BC%B9%E6%80%A7%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86-RDD"><span class="toc-text">3.1弹性分布式数据集(RDD)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-transformation-%E6%93%8D%E4%BD%9C"><span class="toc-text">3.2 transformation 操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-action-%E6%93%8D%E4%BD%9C"><span class="toc-text">3.3 action 操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1%E7%9B%91%E5%90%AC%E7%AB%AF%E5%8F%A3"><span class="toc-text">4.1监听端口</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2%E5%90%AF%E5%8A%A8spark-shell"><span class="toc-text">4.2启动spark-shell</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3%E8%B0%83%E7%94%A8%E5%B7%A6%E5%A4%96%E8%BF%9E%E6%8E%A5"><span class="toc-text">4.3调用左外连接</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4%E6%89%93%E5%8D%B0%E7%99%BD%E5%90%8D%E5%8D%95"><span class="toc-text">4.4打印白名单</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#29-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASpark%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97%E5%95%86%E5%93%81%E5%85%B3%E6%B3%A8%E5%BA%A6"><span class="toc-text">29.Spark实验：Spark流式计算商品关注度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-8"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-8"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-8"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E5%9B%BE%EF%BC%9A"><span class="toc-text">4.1项目结构图：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2Java-%E5%B7%A5%E7%A8%8B%E5%88%9B%E5%BB%BA"><span class="toc-text">4.2Java 工程创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3Spark-assembly-%E5%B7%A5%E5%85%B7%E5%8C%85%E7%9A%84%E5%BC%95%E5%85%A5"><span class="toc-text">4.3Spark-assembly 工具包的引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">4.4代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-StreamingGoods-java-%E7%B1%BB%E7%9A%84%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0"><span class="toc-text">4.4.1 StreamingGoods.java 类的具体实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-text">4.5 完整代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6%E7%A8%8B%E5%BA%8F%E8%BF%90%E8%A1%8C%E8%AF%B4%E6%98%8E"><span class="toc-text">4.6程序运行说明</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#30-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%86%B3%E7%AD%96%E6%A0%91%E9%A2%84%E6%B5%8B%E6%A3%AE%E6%9E%97%E6%A4%8D%E8%A2%AB"><span class="toc-text">30.Spark实验：决策树预测森林植被</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-9"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-9"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-9"><span class="toc-text">原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-text">3.1 决策树算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-text">3.2 数据集介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-%E5%AE%9E%E9%AA%8C%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="toc-text">3.3 实验流程图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="toc-text">3.4 算法详解</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-text">4.1 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%90%AF%E5%8A%A8-Spark-Shell"><span class="toc-text">4.2 启动 Spark Shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3 导入数据并转换数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.1 导入数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2"><span class="toc-text">4.3.2 数据转换</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-DecisionTreeModel-%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B"><span class="toc-text">4.4 DecisionTreeModel 模型建立</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-%E5%BC%95%E5%85%A5-mllib-tree-%E5%8C%85"><span class="toc-text">4.4.1 引入 mllib.tree._ 包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-2-%E5%88%9B%E5%BB%BA-DecisionTreeModel-%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.4.2 创建 DecisionTreeModel 模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-3-%E5%A2%9E%E5%8A%A0%E5%8F%82%E6%95%B0%E5%86%8D%E6%AC%A1%E6%B5%8B%E8%AF%95%E5%87%86%E7%A1%AE%E5%BA%A6"><span class="toc-text">4.4.3 增加参数再次测试准确度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-4-%E5%87%86%E7%A1%AE%E5%BA%A6%E5%AF%B9%E6%AF%94"><span class="toc-text">4.4.4 准确度对比</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-5-%E5%AF%B9%E6%B5%8B%E8%AF%95%E9%9B%86%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="toc-text">4.4.5 对测试集进行预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#31-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%87%BA%E7%A7%9F%E8%BD%A6%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-text">31.Spark实验：出租车数据分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-10"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-10"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-10"><span class="toc-text">原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-K-Means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="toc-text">3.1 K-Means 聚类算法简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-K-Means-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-text">3.2 K-Means 算法实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-text">3.3数据解析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8-Spark-Shell"><span class="toc-text">4.1启动 Spark Shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2 导入数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1%E5%8A%A0%E8%BD%BD%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E5%8C%85"><span class="toc-text">4.2.1加载实验所需的包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-%E5%AE%9A%E4%B9%89%E5%AD%97%E6%AE%B5%E6%A0%BC%E5%BC%8F"><span class="toc-text">4.2.2 定义字段格式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2.3 读取数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-4-%E6%A3%80%E6%9F%A5%E5%B7%B2%E5%AF%BC%E5%85%A5%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2.4 检查已导入的数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%AF%B9%E5%87%BA%E7%A7%9F%E8%BD%A6%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E8%81%9A%E7%B1%BB"><span class="toc-text">4.3对出租车数据进行聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1%E5%AE%9A%E4%B9%89%E7%89%B9%E5%BE%81%E6%95%B0%E7%BB%84"><span class="toc-text">4.3.1定义特征数组</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2%E8%BF%9B%E8%A1%8C-K-Means-%E8%AE%A1%E7%AE%97"><span class="toc-text">4.3.2进行 K-Means 计算</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3-%E5%AF%B9%E6%B5%8B%E8%AF%95%E9%9B%86%E8%BF%9B%E8%A1%8C%E8%81%9A%E7%B1%BB"><span class="toc-text">4.3.3 对测试集进行聚类</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E5%88%86%E6%9E%90%E8%81%9A%E7%B1%BB%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="toc-text">4.4分析聚类的预测结果</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1%E6%AF%8F%E5%A4%A9%E5%93%AA%E4%B8%AA%E6%97%B6%E6%AE%B5%E7%9A%84%E5%87%BA%E7%A7%9F%E8%BD%A6%E6%9C%80%E7%B9%81%E5%BF%99%EF%BC%9F"><span class="toc-text">4.4.1每天哪个时段的出租车最繁忙？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-2-%E6%AF%8F%E5%A4%A9%E5%93%AA%E4%B8%AA%E5%8C%BA%E5%9F%9F%E7%9A%84%E5%87%BA%E7%A7%9F%E8%BD%A6%E6%9C%80%E7%B9%81%E5%BF%99%EF%BC%9F"><span class="toc-text">4.4.2 每天哪个区域的出租车最繁忙？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#32-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9AK%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E8%AF%84%E4%BC%B0%E8%B6%B3%E7%90%83%E6%AF%94%E8%B5%9B"><span class="toc-text">32.Spark实验：K均值聚类评估足球比赛</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-11"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-11"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-11"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E9%97%AE%E9%A2%98%E5%BC%95%E5%85%A5"><span class="toc-text">3.1问题引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3"><span class="toc-text">3.2 算法思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E6%AD%A5%E9%AA%A4"><span class="toc-text">3.3 算法实现步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D"><span class="toc-text">4.1数据集介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%90%AF%E5%8A%A8-spark-shell"><span class="toc-text">4.2 启动 spark shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3 导入数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E4%BD%BF%E7%94%A8import-%E5%91%BD%E4%BB%A4%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96"><span class="toc-text">4.4使用import 命令导入依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5%E6%89%93%E5%8D%B0%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%AF%B9%E5%BA%94%E7%9A%84%E5%AD%90%E9%9B%86"><span class="toc-text">4.5打印数据及对应的子集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#33-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9A%E8%87%AA%E5%AE%9A%E4%B9%89UDF%E5%88%86%E6%9E%90Uber%E6%95%B0%E6%8D%AE"><span class="toc-text">33.Spark实验：自定义UDF分析Uber数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-12"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-12"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-12"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%90%AF%E5%8A%A8-spark-shell"><span class="toc-text">4.1 启动 spark shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2 导入数据并转换数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E8%87%AA%E5%AE%9A%E4%B9%89-UDF-%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3 自定义 UDF 分析数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%9F%A5%E8%AF%A2%E5%90%84%E5%8C%BA%E5%9F%9F%E7%BC%96%E5%8F%B7%E7%9A%84%E6%97%85%E6%B8%B8%E6%AC%A1%E6%95%B0"><span class="toc-text">4.4 查询各区域编号的旅游次数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E6%9F%A5%E8%AF%A2%E5%90%84%E5%8C%BA%E5%9F%9F%E7%BC%96%E5%8F%B7%E7%9A%84%E6%9C%BA%E5%8A%A8%E8%BD%A6%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-text">4.5 查询各区域编号的机动车的使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#34-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%88%86%E6%9E%90%E9%93%B6%E8%A1%8C%E8%90%A5%E9%94%80%E6%95%B0%E6%8D%AE"><span class="toc-text">34.Spark实验：分析银行营销数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-13"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-13"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-13"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-Logistic-Regression"><span class="toc-text">3.1 逻辑回归(Logistic Regression)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">3.2 逻辑回归与线性回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">3.3 特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-1-StringIndex"><span class="toc-text">3.3.1 StringIndex</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-OneHotEncoder"><span class="toc-text">3.3.2 OneHotEncoder</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%8E%B7%E5%8F%96%E9%93%B6%E8%A1%8C%E8%90%A5%E9%94%80%E6%95%B0%E6%8D%AE"><span class="toc-text">4.1 获取银行营销数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E7%BC%96%E5%86%99%E7%A8%8B%E5%BA%8F%E6%9F%A5%E7%9C%8B%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84"><span class="toc-text">4.2编写程序查看数据结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%A6%82%E8%A6%81%E5%88%86%E6%9E%90%E6%95%B0%E6%8D%AE%E5%AD%97%E6%AE%B5%E5%86%85%E5%AE%B9"><span class="toc-text">4.3 概要分析数据字段内容</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-%E5%9C%A8-main%E6%96%B9%E6%B3%95%E4%B8%AD%E7%BB%A7%E7%BB%AD%E7%BC%96%E5%86%99%E5%A6%82%E4%B8%8B%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="toc-text">4.3.1 在 main方法中继续编写如下代码：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F%EF%BC%8C%E6%9F%A5%E7%9C%8B%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C%EF%BC%9A"><span class="toc-text">4.3.2 运行程序，查看运行结果：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">4.4 特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-%E6%96%B0%E5%BB%BA%E4%B8%80%E4%B8%AAScala-Object"><span class="toc-text">4.4.1 新建一个Scala Object</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-3%E7%BB%A7%E7%BB%AD%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="toc-text">4.4.3继续编写代码：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%BB%BA%E7%AB%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B"><span class="toc-text">4.5 建立逻辑回归模型进行预测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-1-%E7%BB%A7%E7%BB%AD%E7%BC%96%E5%86%99%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="toc-text">4.5.1 继续编写代码：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-2-%E5%86%8D%E6%AC%A1%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F%EF%BC%8C%E6%9F%A5%E7%9C%8B%E8%BF%90%E8%A1%8C%E7%BB%93%E6%9E%9C%EF%BC%9A"><span class="toc-text">4.5.2 再次运行程序，查看运行结果：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#35-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9AD3-js%E5%88%86%E6%9E%90%E8%88%AA%E7%8F%AD%E5%A4%A7%E6%95%B0%E6%8D%AE"><span class="toc-text">35.Spark实验：D3.js分析航班大数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-14"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-14"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-14"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%87%86%E5%A4%87"><span class="toc-text">4.1数据集简介及准备</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AE%80%E4%BB%8B"><span class="toc-text">4.1.1数据集简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-%E8%8E%B7%E5%8F%96%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">4.1.2 获取数据集</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="toc-text">4.2 数据清洗</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%90%AF%E5%8A%A8-Spark-Shell"><span class="toc-text">4.3 启动 Spark Shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%8F%8A%E5%A4%84%E7%90%86%E6%A0%BC%E5%BC%8F"><span class="toc-text">4.4导入数据及处理格式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1%E6%AF%8F%E5%A4%A9%E8%88%AA%E7%8F%AD%E6%9C%80%E7%B9%81%E5%BF%99%E7%9A%84%E6%97%B6%E9%97%B4%E6%AE%B5%E6%98%AF%E5%93%AA%E4%BA%9B"><span class="toc-text">4.4.1每天航班最繁忙的时间段是哪些</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-2-%E9%A3%9E%E5%93%AA%E6%9C%80%E5%87%86%E6%97%B6"><span class="toc-text">4.4.2 飞哪最准时</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-3-%E5%87%BA%E5%8F%91%E5%BB%B6%E8%AF%AF%E7%9A%84%E9%87%8D%E7%81%BE%E5%8C%BA%E9%83%BD%E6%9C%89%E5%93%AA%E4%BA%9B"><span class="toc-text">4.4.3 出发延误的重灾区都有哪些</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E8%88%AA%E7%8F%AD%E5%BB%B6%E8%AF%AF%E6%97%B6%E9%97%B4%E9%A2%84%E6%B5%8B"><span class="toc-text">4.5 航班延误时间预测</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-1-%E5%BC%95%E5%85%A5%E7%9B%B8%E5%85%B3%E7%9A%84%E5%8C%85"><span class="toc-text">4.5.1 引入相关的包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-2-DataFrame-%E8%BD%AC%E6%8D%A2%E4%B8%BA-RDD"><span class="toc-text">4.5.2 DataFrame 转换为 RDD</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-3-%E6%8F%90%E5%8F%96%E7%89%B9%E5%BE%81"><span class="toc-text">4.5.3 提取特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-5-%E5%88%9B%E5%BB%BA%E6%A0%87%E8%AE%B0%E7%82%B9"><span class="toc-text">4.5.5 创建标记点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-6-%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.5.6 训练模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-5-7-%E6%B5%8B%E8%AF%95%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.5.7 测试模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#36-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9A%E5%AE%9E%E7%8E%B0%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="toc-text">36.Spark实验：实现电影推荐系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-15"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-15"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-15"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0"><span class="toc-text">3.1 机器学习概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%9F%BA%E4%BA%8E-Spark-MLlib-%E5%B9%B3%E5%8F%B0%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="toc-text">3.2 基于 Spark MLlib 平台的协同过滤算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-1"><span class="toc-text">4.1 数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%95%B4%E4%BD%93%E6%80%9D%E8%B7%AF"><span class="toc-text">4.2 整体思路</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">4.3 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-%E9%A6%96%E5%85%88%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96"><span class="toc-text">4.3.1 首先导入依赖</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-%E5%AE%9A%E4%B9%89-addRatings"><span class="toc-text">4.3.2 定义 addRatings</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3-%E5%8A%A0%E8%BD%BD%E6%A0%B7%E6%9C%AC%E8%AF%84%E5%88%86%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.3 加载样本评分数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-4-%E5%8A%A0%E8%BD%BD-movies-dat"><span class="toc-text">4.3.4 加载 movies.dat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-5-%E7%BB%9F%E8%AE%A1%E7%94%A8%E6%88%B7%E6%95%B0%E9%87%8F%E5%92%8C%E7%94%B5%E5%BD%B1%E6%95%B0%E9%87%8F%E4%BB%A5%E5%8F%8A%E7%94%A8%E6%88%B7%E5%AF%B9%E7%94%B5%E5%BD%B1%E7%9A%84%E8%AF%84%E5%88%86%E6%95%B0%E7%9B%AE%E3%80%82"><span class="toc-text">4.3.5 统计用户数量和电影数量以及用户对电影的评分数目。</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-6-Key%E5%80%BC%E5%88%87%E5%88%86"><span class="toc-text">4.3.6 Key值切分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-7-%E8%8E%B7%E5%8F%96%E6%9C%80%E4%BD%B3%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.3.7 获取最佳模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-8-%E5%AE%9A%E4%B9%89-compute-%E5%87%BD%E6%95%B0"><span class="toc-text">4.3.8 定义 compute 函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-9-%E8%AE%A1%E7%AE%97%E6%9C%80%E4%BD%B3%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.3.9 计算最佳模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-10-%E8%AE%A1%E7%AE%97%E5%92%8C%E5%AE%9E%E9%99%85%E8%AF%84%E5%88%86%E4%B9%8B%E9%97%B4%E7%9A%84%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF"><span class="toc-text">4.3.10 计算和实际评分之间的均方根误</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-11-%E8%AE%A1%E7%AE%97%E6%9C%80%E4%BD%B3%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%8E%9F%E5%A7%8B%E5%9F%BA%E7%A1%80%E7%9A%84%E7%9B%B8%E6%AF%94%E5%85%B6%E6%8F%90%E5%8D%87%E5%BA%A6"><span class="toc-text">4.3.11 计算最佳模型与原始基础的相比其提升度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-12-%E6%8E%A8%E8%8D%90%E5%89%8D-15-%E9%83%A8%E6%9C%80%E6%84%9F%E5%85%B4%E8%B6%A3%E7%9A%84%E7%94%B5%E5%BD%B1%E7%BB%99%E6%88%91%EF%BC%8C%E6%B3%A8%E6%84%8F%E8%A6%81%E5%89%94%E9%99%A4%E7%94%A8%E6%88%B7-%E6%88%91-%E5%B7%B2%E7%BB%8F%E8%AF%84%E5%88%86%E7%9A%84%E7%94%B5%E5%BD%B1"><span class="toc-text">4.3.12 推荐前 15 部最感兴趣的电影给我，注意要剔除用户(我)已经评分的电影</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#37-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASpark%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-text">37.Spark实验：Spark简介与安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-16"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-16"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-16"><span class="toc-text">原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1Spark-%E7%AE%80%E4%BB%8B"><span class="toc-text">3.1Spark 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2Spark-%E7%9A%84%E7%89%B9%E6%80%A7"><span class="toc-text">3.2Spark 的特性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3Spark-%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-text">3.3Spark 的优势</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4%E5%93%AA%E4%BA%9B%E5%85%AC%E5%8F%B8%E5%9C%A8%E4%BD%BF%E7%94%A8-Spark"><span class="toc-text">3.4哪些公司在使用 Spark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5Spark-%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F-BDAS"><span class="toc-text">3.5Spark 生态系统 BDAS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E9%83%A8%E7%BD%B2-Spark"><span class="toc-text">4.1部署 Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E4%BD%BF%E7%94%A8Spark-Shell"><span class="toc-text">4.2使用Spark Shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3Pyspark"><span class="toc-text">4.3Pyspark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%90%AF%E5%8A%A8-Spark-%E6%9C%8D%E5%8A%A1"><span class="toc-text">4.4.启动 Spark 服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5Spark-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%EF%BC%9A"><span class="toc-text">4.5Spark 集群部署：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#38-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ASparkFPGrowth%E5%85%B3%E8%81%94%E5%AD%A6%E4%B9%A0"><span class="toc-text">38.Spark实验：SparkFPGrowth关联学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-17"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-17"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-17"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-text">4.1数据准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%90%AF%E5%8A%A8-spark-shell"><span class="toc-text">4.2启动 spark shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%BD%AC%E6%8D%A2%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3导入数据并转换数据</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.1导入数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2%E5%8E%BB%E9%99%A4%E6%97%A0%E7%94%A8%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.2去除无用数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3%E8%8E%B7%E5%8F%96%E8%B4%AD%E7%89%A9%E6%95%B0%E6%8D%AE"><span class="toc-text">4.3.3获取购物数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4FPGrowth-%E6%A8%A1%E5%9E%8B%E5%BB%BA%E7%AB%8B"><span class="toc-text">4.4FPGrowth 模型建立</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1%E5%BC%95%E5%85%A5-FPGrowth-%E5%8C%85"><span class="toc-text">4.4.1引入 FPGrowth 包</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-2%E5%BC%80%E5%A7%8B%E5%88%9B%E5%BB%BA%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.4.2开始创建模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-3%E8%BF%9B%E8%A1%8C%E7%94%A8%E6%88%B7%E5%95%86%E5%93%81%E6%8E%A8%E8%8D%90"><span class="toc-text">4.4.3进行用户商品推荐</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#39-Spark%E5%AE%9E%E9%AA%8C%EF%BC%9ARDD%E7%AE%80%E4%BB%8B%E4%B8%8E%E6%93%8D%E4%BD%9C"><span class="toc-text">39.Spark实验：RDD简介与操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-18"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-18"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-18"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E5%90%AF%E5%8A%A8Spark-%E6%9C%8D%E5%8A%A1"><span class="toc-text">4.1启动Spark 服务</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E8%BF%9E%E6%8E%A5-Spark"><span class="toc-text">4.2连接 Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3%E5%88%9D%E5%A7%8B%E5%8C%96-Spark"><span class="toc-text">4.3初始化 Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4%E4%BD%BF%E7%94%A8-Spark-Shell"><span class="toc-text">4.4使用 Spark Shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-RDD-%E7%9A%84%E7%B3%BB%E5%88%97%E6%93%8D%E4%BD%9C"><span class="toc-text">4.5 RDD 的系列操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-1%E5%B9%B6%E8%A1%8C%E9%9B%86%E5%90%88"><span class="toc-text">4.5.1并行集合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-2map"><span class="toc-text">4.5.2map()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-3flatMap"><span class="toc-text">4.5.3flatMap()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-4filter"><span class="toc-text">4.5.4filter()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-5distinct"><span class="toc-text">4.5.5distinct()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-6union"><span class="toc-text">4.5.6union()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-7intersection"><span class="toc-text">4.5.7intersection()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-8subtract"><span class="toc-text">4.5.8subtract()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-9collect"><span class="toc-text">4.5.9collect()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-10count"><span class="toc-text">4.5.10count()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-11countByValue"><span class="toc-text">4.5.11countByValue()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-12take-num"><span class="toc-text">4.5.12take(num)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-13top-num"><span class="toc-text">4.5.13top(num)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-14reduce-func"><span class="toc-text">4.5.14reduce(func)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-15fold-zero-func"><span class="toc-text">4.5.15fold(zero)(func)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-16foreach-func"><span class="toc-text">4.5.16foreach(func)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-17%E4%BB%A5%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%E5%AD%98%E5%82%A8"><span class="toc-text">4.5.17以文件格式存储</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#40-%E7%BB%BC%E5%90%88%E5%AE%9E%E6%88%98%EF%BC%9A%E8%B4%B7%E6%AC%BE%E9%A3%8E%E9%99%A9%E8%AF%84%E4%BC%B0"><span class="toc-text">40.综合实战：贷款风险评估</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-19"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-19"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-19"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E5%AD%A6%E4%B9%A0%E9%98%B6%E6%AE%B5%EF%BC%9A"><span class="toc-text">3.1学习阶段：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2%E5%88%86%E7%B1%BB%E9%98%B6%E6%AE%B5"><span class="toc-text">3.2分类阶段</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE"><span class="toc-text">4.1 实验数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E5%AE%9E%E9%AA%8C%E6%93%8D%E4%BD%9C"><span class="toc-text">4.2实验操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#41-%E7%BB%BC%E5%90%88%E5%AE%9E%E6%88%98%EF%BC%9A%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="toc-text">41.综合实战：电影推荐系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84-20"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%81%E6%B1%82-20"><span class="toc-text">要求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E7%90%86-20"><span class="toc-text">原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1%E6%95%B0%E6%8D%AE%E9%9B%86%E5%87%86%E5%A4%87"><span class="toc-text">4.1数据集准备</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">4.2代码实现</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%914/" title="Hadoop应用与开发4">Hadoop应用与开发4</a><time datetime="2025-06-16T04:00:00.000Z" title="发表于 2025-06-16 12:00:00">2025-06-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/16/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%913/" title="Hadoop应用与开发3">Hadoop应用与开发3</a><time datetime="2025-05-16T04:00:00.000Z" title="发表于 2025-05-16 12:00:00">2025-05-16</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/11/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%912/" title="Hadoop应用与开发2">Hadoop应用与开发2</a><time datetime="2025-05-11T04:00:00.000Z" title="发表于 2025-05-11 12:00:00">2025-05-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/20/Hadoop/Hadoop%E5%BA%94%E7%94%A8%E4%B8%8E%E5%BC%80%E5%8F%91/" title="Hadoop应用与开发1">Hadoop应用与开发1</a><time datetime="2025-04-20T04:00:00.000Z" title="发表于 2025-04-20 12:00:00">2025-04-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/12/Java/mooc%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E2%80%94%E2%80%94Java/" title="mooc面向对象程序设计——Java">mooc面向对象程序设计——Java</a><time datetime="2025-03-12T10:22:00.000Z" title="发表于 2025-03-12 18:22:00">2025-03-12</time></div></div></div></div></div></div></main><footer id="footer" style="background: linear-gradient(20deg, #ffd6e0, #f5f5f5, #c1f0c1);"><div id="footer-wrap"><div class="copyright">&copy;2025 By tong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>